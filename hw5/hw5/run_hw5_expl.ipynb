{"cells":[{"cell_type":"markdown","metadata":{"id":"gUl_qfOR8JV6"},"source":["##Setup\n","\n","You will need to make a copy of this notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**."]},{"cell_type":"code","execution_count":11,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1582,"status":"ok","timestamp":1669251161414,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"iizPcHAp8LnA","outputId":"dfe19675-e81a-41e9-f560-513483d855c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["#@title mount your Google Drive\n","#@markdown Your work will be stored in a folder called `cs285_f2022` by default to prevent Colab instance timeouts from deleting your edits.\n","\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":12,"metadata":{"cellView":"form","executionInfo":{"elapsed":342,"status":"ok","timestamp":1669251161754,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"nAb10wnb8N0m","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cd187e9d-c5be-451e-f989-11ce6c7ab3b6"},"outputs":[{"output_type":"stream","name":"stderr","text":["<>:3: DeprecationWarning: invalid escape sequence \\ \n"]}],"source":["#@title set up mount symlink\n","\n","DRIVE_PATH = '/content/gdrive/My\\ Drive/cs285_f2022'\n","DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","  %mkdir $DRIVE_PATH\n","\n","## the space in `My Drive` causes some issues,\n","## make a symlink to avoid this\n","SYM_PATH = '/content/cs285_f2022'\n","if not os.path.exists(SYM_PATH):\n","  !ln -s $DRIVE_PATH $SYM_PATH"]},{"cell_type":"code","execution_count":13,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9377,"status":"ok","timestamp":1669251171129,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"gtS9-WsD8QVr","outputId":"f81f4ccc-318d-4ffd-bd54-6b6dfe7174c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n","Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Fetched 261 kB in 4s (65.0 kB/s)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","7 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"]}],"source":["#@title apt install requirements\n","\n","#@markdown Run each section with Shift+Enter\n","\n","#@markdown Double-click on section headers to show code.\n","\n","!apt update \n","!apt install -y --no-install-recommends \\\n","        build-essential \\\n","        curl \\\n","        git \\\n","        gnupg2 \\\n","        make \\\n","        cmake \\\n","        ffmpeg \\\n","        swig \\\n","        libz-dev \\\n","        unzip \\\n","        zlib1g-dev \\\n","        libglfw3 \\\n","        libglfw3-dev \\\n","        libxrandr2 \\\n","        libxinerama-dev \\\n","        libxi6 \\\n","        libxcursor-dev \\\n","        libgl1-mesa-dev \\\n","        libgl1-mesa-glx \\\n","        libglew-dev \\\n","        libosmesa6-dev \\\n","        lsb-release \\\n","        ack-grep \\\n","        patchelf \\\n","        wget \\\n","        xpra \\\n","        xserver-xorg-dev \\\n","        xvfb \\\n","        python-opengl \\\n","        ffmpeg > /dev/null 2>&1"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8776,"status":"ok","timestamp":1669251179881,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"-XcwBiBN8-Fg","outputId":"0327f220-700e-43d7-fcc5-b5b51c3567d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/cs285_f2022\n","/content/gdrive/My Drive/cs285_f2022/homework_fall2022/hw5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mujoco==2.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_colab.txt (line 1)) (2.2.0)\n","Requirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements_colab.txt (line 2)) (0.25.2)\n","Requirement already satisfied: tensorboardX==2.5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements_colab.txt (line 3)) (2.5.1)\n","Requirement already satisfied: moviepy==1.0.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements_colab.txt (line 4)) (1.0.3)\n","Requirement already satisfied: pyvirtualdisplay==3.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements_colab.txt (line 5)) (3.0)\n","Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements_colab.txt (line 6)) (1.12.1+cu113)\n","Requirement already satisfied: opencv-python==4.6.0.66 in /usr/local/lib/python3.7/dist-packages (from -r requirements_colab.txt (line 7)) (4.6.0.66)\n","Requirement already satisfied: swig==4.0.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements_colab.txt (line 8)) (4.0.2)\n","Requirement already satisfied: box2d-py==2.3.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements_colab.txt (line 9)) (2.3.8)\n","Requirement already satisfied: networkx==2.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements_colab.txt (line 10)) (2.5)\n","Requirement already satisfied: ipdb==0.13.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements_colab.txt (line 11)) (0.13.3)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->-r requirements_colab.txt (line 1)) (1.3.0)\n","Requirement already satisfied: glfw in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->-r requirements_colab.txt (line 1)) (2.5.5)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->-r requirements_colab.txt (line 1)) (3.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mujoco==2.2.0->-r requirements_colab.txt (line 1)) (1.21.6)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.25.2->-r requirements_colab.txt (line 2)) (1.5.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.25.2->-r requirements_colab.txt (line 2)) (0.0.8)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.25.2->-r requirements_colab.txt (line 2)) (4.13.0)\n","Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.5.1->-r requirements_colab.txt (line 3)) (3.19.6)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.7/dist-packages (from moviepy==1.0.3->-r requirements_colab.txt (line 4)) (2.9.0)\n","Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from moviepy==1.0.3->-r requirements_colab.txt (line 4)) (0.4.7)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.7/dist-packages (from moviepy==1.0.3->-r requirements_colab.txt (line 4)) (0.1.10)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy==1.0.3->-r requirements_colab.txt (line 4)) (4.64.1)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy==1.0.3->-r requirements_colab.txt (line 4)) (4.4.2)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from moviepy==1.0.3->-r requirements_colab.txt (line 4)) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->-r requirements_colab.txt (line 6)) (4.1.1)\n","Requirement already satisfied: ipython>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from ipdb==0.13.3->-r requirements_colab.txt (line 11)) (7.9.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb==0.13.3->-r requirements_colab.txt (line 11)) (57.4.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.5->moviepy==1.0.3->-r requirements_colab.txt (line 4)) (7.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym==0.25.2->-r requirements_colab.txt (line 2)) (3.10.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.1.0->ipdb==0.13.3->-r requirements_colab.txt (line 11)) (5.1.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.1.0->ipdb==0.13.3->-r requirements_colab.txt (line 11)) (2.6.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=5.1.0->ipdb==0.13.3->-r requirements_colab.txt (line 11)) (4.8.0)\n","Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.1.0->ipdb==0.13.3->-r requirements_colab.txt (line 11)) (0.18.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.1.0->ipdb==0.13.3->-r requirements_colab.txt (line 11)) (0.7.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=5.1.0->ipdb==0.13.3->-r requirements_colab.txt (line 11)) (0.2.0)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.1.0->ipdb==0.13.3->-r requirements_colab.txt (line 11)) (2.0.10)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=5.1.0->ipdb==0.13.3->-r requirements_colab.txt (line 11)) (0.8.3)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.1.0->ipdb==0.13.3->-r requirements_colab.txt (line 11)) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.1.0->ipdb==0.13.3->-r requirements_colab.txt (line 11)) (0.2.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3->-r requirements_colab.txt (line 4)) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3->-r requirements_colab.txt (line 4)) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3->-r requirements_colab.txt (line 4)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3->-r requirements_colab.txt (line 4)) (2022.9.24)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython>=5.1.0->ipdb==0.13.3->-r requirements_colab.txt (line 11)) (0.7.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5\n","Installing collected packages: cs285\n","  Attempting uninstall: cs285\n","    Found existing installation: cs285 0.1.0\n","    Can't uninstall 'cs285'. No files were found to uninstall.\n","  Running setup.py develop for cs285\n","Successfully installed cs285-0.1.0\n"]}],"source":["#@title clone homework repo\n","#@markdown Note that this is the same codebase from homework 1,\n","#@markdown so you may need to move your old `homework_fall2022`\n","#@markdown folder in order to clone the repo again.\n","\n","#@markdown **Don't delete your old work though!**\n","#@markdown You will need it for this assignment.\n","\n","%cd $SYM_PATH\n","# !git clone https://github.com/berkeleydeeprlcourse/homework_fall2022.git\n","%cd homework_fall2022/hw5\n","%pip install -r requirements_colab.txt\n","%pip install -e ."]},{"cell_type":"code","execution_count":15,"metadata":{"cellView":"form","executionInfo":{"elapsed":289,"status":"ok","timestamp":1669251180165,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"g5xIOIpW8_jC"},"outputs":[],"source":["#@title set up virtual display\n","\n","from pyvirtualdisplay import Display\n","\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","\n","# For later\n","from cs285.infrastructure.colab_utils import (\n","    wrap_env,\n","    show_video\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":709},"executionInfo":{"elapsed":2619,"status":"ok","timestamp":1669251182781,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"2rsWAWaK9BVp","outputId":"d4074ed5-9436-40b5-b871-990032a1bd05"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n","/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n","/usr/local/lib/python3.7/dist-packages/gym/wrappers/record_video.py:79: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n","  f\"Overwriting existing videos at {self.video_folder} folder \"\n","/usr/local/lib/python3.7/dist-packages/gym/wrappers/monitoring/video_recorder.py:79: DeprecationWarning: \u001b[33mWARN: Recording ability for environment Ant-v4 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n","  f\"Recording ability for environment {env.spec.id} initialized with `render_mode=None` is marked \"\n","/usr/local/lib/python3.7/dist-packages/gym/core.py:44: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  \"The argument mode in render method is deprecated; \"\n","/usr/local/lib/python3.7/dist-packages/gym/core.py:44: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  \"The argument mode in render method is deprecated; \"\n"]},{"output_type":"stream","name":"stdout","text":["Loading video...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n","             </video>"]},"metadata":{}}],"source":["#@title test virtual display\n","\n","#@markdown If you see a video of a four-legged ant fumbling about, setup is complete!\n","\n","import gym\n","import matplotlib\n","matplotlib.use('Agg')\n","\n","env = wrap_env(gym.make(\"Ant-v4\"))\n","\n","observation = env.reset()\n","for i in range(10):\n","    env.render(mode='rgb_array')\n","    obs, rew, term, _ = env.step(env.action_space.sample() ) \n","    if term:\n","      break;\n","            \n","env.close()\n","print('Loading video...')\n","show_video()"]},{"cell_type":"markdown","metadata":{"id":"QizpiHDh9Fwk"},"source":["## Editing Code\n","\n","To edit code, click the folder icon on the left menu. Navigate to the corresponding file (`cs285_f2022/...`). Double click a file to open an editor. There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window). We sync your edits to Google Drive so that you won't lose your work in the event of an instance timeout, but you will need to re-mount your Google Drive and re-install packages with every new instance."]},{"cell_type":"markdown","metadata":{"id":"Nii6qk2C9Ipk"},"source":["## Run Exploration or Exploitation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4t7FUeEG9Dkf"},"outputs":[],"source":["import os\n","import time\n","\n","from cs285.infrastructure.rl_trainer import RL_Trainer\n","from cs285.agents.explore_or_exploit_agent import ExplorationOrExploitationAgent\n","from cs285.infrastructure.dqn_utils import get_env_kwargs, PiecewiseSchedule, ConstantSchedule\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"xV8QEMwcOUbq"},"source":["## 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2038694,"status":"ok","timestamp":1668943788759,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"YVNIX4Pl9_jK","outputId":"bbc095b5-b7b5-4617-b861-b534b1e72cac"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env1_rnd_PointmassEasy-v0_20-11-2022_10-55-53 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env1_rnd_PointmassEasy-v0_20-11-2022_10-55-53\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.003664\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0036640167236328125\n","Eval_AverageReturn : -49.380950927734375\n","Eval_StdReturn : 1.7036707401275635\n","Eval_MaxReturn : -43.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 49.523809523809526\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -50.000000\n","best mean reward -inf\n","running time 12.986113\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -50.0\n","TimeSinceStart : 12.986112594604492\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -49.875000\n","best mean reward -inf\n","running time 25.901188\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -49.875\n","TimeSinceStart : 25.901188373565674\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -46.484375\n","best mean reward -inf\n","running time 63.840096\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -46.484375\n","TimeSinceStart : 63.84009552001953\n","Exploitation Critic Loss : 0.2906196415424347\n","Exploration Critic Loss : 0.3005392551422119\n","Exploration Model Loss : 0.4432642161846161\n","Exploitation Data q-values : -3.4848718643188477\n","Exploitation OOD q-values : -1.8957469463348389\n","Exploitation CQL Loss : 1.5891249179840088\n","Eval_AverageReturn : -46.772727966308594\n","Eval_StdReturn : 6.4732441902160645\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 47.04545454545455\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -38.349998\n","best mean reward -38.349998\n","running time 106.526286\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -38.349998474121094\n","Train_BestReturn : -38.349998474121094\n","TimeSinceStart : 106.5262861251831\n","Exploitation Critic Loss : 0.5463612079620361\n","Exploration Critic Loss : 0.3399231731891632\n","Exploration Model Loss : 0.002589411335065961\n","Exploitation Data q-values : -5.339717864990234\n","Exploitation OOD q-values : -3.7499961853027344\n","Exploitation CQL Loss : 1.589721441268921\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -27.400000\n","best mean reward -27.400000\n","running time 151.205999\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -27.399999618530273\n","Train_BestReturn : -27.399999618530273\n","TimeSinceStart : 151.2059986591339\n","Exploitation Critic Loss : 0.5441322326660156\n","Exploration Critic Loss : 1.2508265972137451\n","Exploration Model Loss : 0.0035868948325514793\n","Exploitation Data q-values : -6.293154716491699\n","Exploitation OOD q-values : -4.742375373840332\n","Exploitation CQL Loss : 1.550779104232788\n","Eval_AverageReturn : -49.238094329833984\n","Eval_StdReturn : 3.407341957092285\n","Eval_MaxReturn : -34.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 49.285714285714285\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -29.139999\n","best mean reward -27.400000\n","running time 192.843862\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -29.139999389648438\n","Train_BestReturn : -27.399999618530273\n","TimeSinceStart : 192.84386205673218\n","Exploitation Critic Loss : 0.5539728403091431\n","Exploration Critic Loss : 1.1147923469543457\n","Exploration Model Loss : 0.003969117999076843\n","Exploitation Data q-values : -7.201663970947266\n","Exploitation OOD q-values : -5.689156532287598\n","Exploitation CQL Loss : 1.5125070810317993\n","Eval_AverageReturn : -34.03333282470703\n","Eval_StdReturn : 13.287797927856445\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 34.7\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -32.889999\n","best mean reward -27.400000\n","running time 232.216482\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -32.88999938964844\n","Train_BestReturn : -27.399999618530273\n","TimeSinceStart : 232.2164821624756\n","Exploitation Critic Loss : 0.5384310483932495\n","Exploration Critic Loss : 1.3233516216278076\n","Exploration Model Loss : 0.0016269332263618708\n","Exploitation Data q-values : -7.534512042999268\n","Exploitation OOD q-values : -5.996037483215332\n","Exploitation CQL Loss : 1.5384740829467773\n","Eval_AverageReturn : -13.385714530944824\n","Eval_StdReturn : 2.3981711864471436\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -22.0\n","Eval_AverageEpLen : 14.385714285714286\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -38.939999\n","best mean reward -27.400000\n","running time 282.828945\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -38.939998626708984\n","Train_BestReturn : -27.399999618530273\n","TimeSinceStart : 282.8289451599121\n","Exploitation Critic Loss : 0.22938397526741028\n","Exploration Critic Loss : 0.6635044813156128\n","Exploration Model Loss : 0.0011171402875334024\n","Exploitation Data q-values : -7.332638263702393\n","Exploitation OOD q-values : -5.807844638824463\n","Exploitation CQL Loss : 1.5247936248779297\n","Eval_AverageReturn : -13.45714282989502\n","Eval_StdReturn : 2.221233367919922\n","Eval_MaxReturn : -8.0\n","Eval_MinReturn : -22.0\n","Eval_AverageEpLen : 14.457142857142857\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -45.520000\n","best mean reward -27.400000\n","running time 333.625870\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -45.52000045776367\n","Train_BestReturn : -27.399999618530273\n","TimeSinceStart : 333.6258704662323\n","Exploitation Critic Loss : 0.5239816904067993\n","Exploration Critic Loss : 0.9227311611175537\n","Exploration Model Loss : 0.0006852922379039228\n","Exploitation Data q-values : -6.7470502853393555\n","Exploitation OOD q-values : -5.166874885559082\n","Exploitation CQL Loss : 1.5801761150360107\n","Eval_AverageReturn : -13.79411792755127\n","Eval_StdReturn : 2.65993595123291\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -20.0\n","Eval_AverageEpLen : 14.794117647058824\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -49.320000\n","best mean reward -27.400000\n","running time 384.750001\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -49.31999969482422\n","Train_BestReturn : -27.399999618530273\n","TimeSinceStart : 384.7500011920929\n","Exploitation Critic Loss : 0.22102557122707367\n","Exploration Critic Loss : 0.6566701531410217\n","Exploration Model Loss : 0.00036888450267724693\n","Exploitation Data q-values : -6.944716453552246\n","Exploitation OOD q-values : -5.391330242156982\n","Exploitation CQL Loss : 1.5533864498138428\n","Eval_AverageReturn : -13.838234901428223\n","Eval_StdReturn : 2.0976898670196533\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -21.0\n","Eval_AverageEpLen : 14.838235294117647\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -31.830000\n","best mean reward -27.400000\n","running time 439.505764\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -31.829999923706055\n","Train_BestReturn : -27.399999618530273\n","TimeSinceStart : 439.50576400756836\n","Exploitation Critic Loss : 0.31802046298980713\n","Exploration Critic Loss : 0.681807816028595\n","Exploration Model Loss : 0.0002248249511467293\n","Exploitation Data q-values : -7.006218910217285\n","Exploitation OOD q-values : -5.509255409240723\n","Exploitation CQL Loss : 1.4969637393951416\n","Eval_AverageReturn : -13.239436149597168\n","Eval_StdReturn : 2.3881442546844482\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -24.0\n","Eval_AverageEpLen : 14.23943661971831\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -17.709999\n","best mean reward -17.709999\n","running time 494.062299\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -17.709999084472656\n","Train_BestReturn : -17.709999084472656\n","TimeSinceStart : 494.0622992515564\n","Exploitation Critic Loss : 0.8038687705993652\n","Exploration Critic Loss : 0.8498033285140991\n","Exploration Model Loss : 0.00015254371101036668\n","Exploitation Data q-values : -6.964190483093262\n","Exploitation OOD q-values : -5.484527587890625\n","Exploitation CQL Loss : 1.4796631336212158\n","Eval_AverageReturn : -13.79411792755127\n","Eval_StdReturn : 2.1730895042419434\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.794117647058824\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -18.090000\n","best mean reward -17.709999\n","running time 551.551704\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -18.09000015258789\n","Train_BestReturn : -17.709999084472656\n","TimeSinceStart : 551.5517036914825\n","Exploitation Critic Loss : 0.32472988963127136\n","Exploration Critic Loss : 0.6169930696487427\n","Exploration Model Loss : 9.390068589709699e-05\n","Exploitation Data q-values : -6.857577323913574\n","Exploitation OOD q-values : -5.389599800109863\n","Exploitation CQL Loss : 1.4679776430130005\n","Eval_AverageReturn : -14.523077011108398\n","Eval_StdReturn : 2.818115472793579\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -23.0\n","Eval_AverageEpLen : 15.523076923076923\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -17.120001\n","best mean reward -17.120001\n","running time 606.419847\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -17.1200008392334\n","Train_BestReturn : -17.1200008392334\n","TimeSinceStart : 606.419846534729\n","Exploitation Critic Loss : 0.44651922583580017\n","Exploration Critic Loss : 0.742560863494873\n","Exploration Model Loss : 5.783362212241627e-05\n","Exploitation Data q-values : -6.927245616912842\n","Exploitation OOD q-values : -5.483450889587402\n","Exploitation CQL Loss : 1.4437949657440186\n","Eval_AverageReturn : -13.52173900604248\n","Eval_StdReturn : 2.236772298812866\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -22.0\n","Eval_AverageEpLen : 14.521739130434783\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -17.799999\n","best mean reward -17.120001\n","running time 661.902130\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -17.799999237060547\n","Train_BestReturn : -17.1200008392334\n","TimeSinceStart : 661.9021298885345\n","Exploitation Critic Loss : 0.4273756742477417\n","Exploration Critic Loss : 0.9044876098632812\n","Exploration Model Loss : 5.135910032549873e-05\n","Exploitation Data q-values : -6.328625679016113\n","Exploitation OOD q-values : -4.860363006591797\n","Exploitation CQL Loss : 1.468262791633606\n","Eval_AverageReturn : -13.95522403717041\n","Eval_StdReturn : 2.4943687915802\n","Eval_MaxReturn : -8.0\n","Eval_MinReturn : -20.0\n","Eval_AverageEpLen : 14.955223880597014\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -17.440001\n","best mean reward -17.120001\n","running time 722.998280\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -17.440000534057617\n","Train_BestReturn : -17.1200008392334\n","TimeSinceStart : 722.9982802867889\n","Exploitation Critic Loss : 0.11732903122901917\n","Exploration Critic Loss : 0.8070164918899536\n","Exploration Model Loss : 3.082227340200916e-05\n","Exploitation Data q-values : -6.654362678527832\n","Exploitation OOD q-values : -5.232680320739746\n","Exploitation CQL Loss : 1.4216827154159546\n","Eval_AverageReturn : -13.897058486938477\n","Eval_StdReturn : 2.1971888542175293\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.897058823529411\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -17.709999\n","best mean reward -17.120001\n","running time 785.300818\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -17.709999084472656\n","Train_BestReturn : -17.1200008392334\n","TimeSinceStart : 785.3008179664612\n","Exploitation Critic Loss : 0.21835389733314514\n","Exploration Critic Loss : 0.9760346412658691\n","Exploration Model Loss : 2.6486739443498664e-05\n","Exploitation Data q-values : -6.658801078796387\n","Exploitation OOD q-values : -5.219610691070557\n","Exploitation CQL Loss : 1.43919038772583\n","Eval_AverageReturn : -13.735294342041016\n","Eval_StdReturn : 2.240126371383667\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -21.0\n","Eval_AverageEpLen : 14.735294117647058\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -18.190001\n","best mean reward -17.120001\n","running time 851.476500\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -18.190000534057617\n","Train_BestReturn : -17.1200008392334\n","TimeSinceStart : 851.4765000343323\n","Exploitation Critic Loss : 0.6626585721969604\n","Exploration Critic Loss : 0.8873242139816284\n","Exploration Model Loss : 9.158565262623597e-06\n","Exploitation Data q-values : -6.787062644958496\n","Exploitation OOD q-values : -5.348207473754883\n","Exploitation CQL Loss : 1.4388552904129028\n","Eval_AverageReturn : -13.808823585510254\n","Eval_StdReturn : 1.9573193788528442\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -22.0\n","Eval_AverageEpLen : 14.808823529411764\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -17.660000\n","best mean reward -17.120001\n","running time 920.658348\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -17.65999984741211\n","Train_BestReturn : -17.1200008392334\n","TimeSinceStart : 920.6583476066589\n","Exploitation Critic Loss : 0.34721702337265015\n","Exploration Critic Loss : 1.0166956186294556\n","Exploration Model Loss : 0.0001435961021343246\n","Exploitation Data q-values : -6.329216003417969\n","Exploitation OOD q-values : -4.924323081970215\n","Exploitation CQL Loss : 1.4048925638198853\n","Eval_AverageReturn : -13.823529243469238\n","Eval_StdReturn : 1.7982112169265747\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -18.0\n","Eval_AverageEpLen : 14.823529411764707\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -17.490000\n","best mean reward -17.120001\n","running time 987.747631\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -17.489999771118164\n","Train_BestReturn : -17.1200008392334\n","TimeSinceStart : 987.7476313114166\n","Exploitation Critic Loss : 0.12852129340171814\n","Exploration Critic Loss : 0.6243680715560913\n","Exploration Model Loss : 0.00015181177877821028\n","Exploitation Data q-values : -6.45207405090332\n","Exploitation OOD q-values : -5.077369213104248\n","Exploitation CQL Loss : 1.3747050762176514\n","Eval_AverageReturn : -13.594202995300293\n","Eval_StdReturn : 2.155672311782837\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -21.0\n","Eval_AverageEpLen : 14.594202898550725\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -17.750000\n","best mean reward -17.120001\n","running time 1056.797676\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -17.75\n","Train_BestReturn : -17.1200008392334\n","TimeSinceStart : 1056.7976758480072\n","Exploitation Critic Loss : 0.7234694361686707\n","Exploration Critic Loss : 1.1232547760009766\n","Exploration Model Loss : 0.0004532134917099029\n","Exploitation Data q-values : -6.507102012634277\n","Exploitation OOD q-values : -5.1189446449279785\n","Exploitation CQL Loss : 1.3881574869155884\n","Eval_AverageReturn : -13.428571701049805\n","Eval_StdReturn : 2.03940749168396\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.428571428571429\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -18.120001\n","best mean reward -17.120001\n","running time 1124.659487\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -18.1200008392334\n","Train_BestReturn : -17.1200008392334\n","TimeSinceStart : 1124.6594870090485\n","Exploitation Critic Loss : 0.39866819977760315\n","Exploration Critic Loss : 1.0044682025909424\n","Exploration Model Loss : 0.0003999205073341727\n","Exploitation Data q-values : -6.546947002410889\n","Exploitation OOD q-values : -5.197585105895996\n","Exploitation CQL Loss : 1.3493623733520508\n","Eval_AverageReturn : -14.08955192565918\n","Eval_StdReturn : 1.77655827999115\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -18.0\n","Eval_AverageEpLen : 15.08955223880597\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -17.840000\n","best mean reward -17.120001\n","running time 1190.350801\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -17.84000015258789\n","Train_BestReturn : -17.1200008392334\n","TimeSinceStart : 1190.3508005142212\n","Exploitation Critic Loss : 0.8471090793609619\n","Exploration Critic Loss : 1.085805892944336\n","Exploration Model Loss : 0.00014392858429346234\n","Exploitation Data q-values : -6.673779487609863\n","Exploitation OOD q-values : -5.2685346603393555\n","Exploitation CQL Loss : 1.4052445888519287\n","Eval_AverageReturn : -13.735294342041016\n","Eval_StdReturn : 2.2137115001678467\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.735294117647058\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -17.770000\n","best mean reward -17.120001\n","running time 1258.417745\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -17.770000457763672\n","Train_BestReturn : -17.1200008392334\n","TimeSinceStart : 1258.4177448749542\n","Exploitation Critic Loss : 0.6000874638557434\n","Exploration Critic Loss : 0.912136435508728\n","Exploration Model Loss : 0.00032472374732606113\n","Exploitation Data q-values : -6.459410667419434\n","Exploitation OOD q-values : -5.080865859985352\n","Exploitation CQL Loss : 1.378544569015503\n","Eval_AverageReturn : -13.623188018798828\n","Eval_StdReturn : 2.1875898838043213\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -21.0\n","Eval_AverageEpLen : 14.623188405797102\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -17.860001\n","best mean reward -17.120001\n","running time 1323.727206\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -17.860000610351562\n","Train_BestReturn : -17.1200008392334\n","TimeSinceStart : 1323.7272064685822\n","Exploitation Critic Loss : 0.7136402130126953\n","Exploration Critic Loss : 1.04142427444458\n","Exploration Model Loss : 4.32420420111157e-05\n","Exploitation Data q-values : -6.532709121704102\n","Exploitation OOD q-values : -5.148740768432617\n","Exploitation CQL Loss : 1.383968710899353\n","Eval_AverageReturn : -13.852941513061523\n","Eval_StdReturn : 1.9194415807724\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -18.0\n","Eval_AverageEpLen : 14.852941176470589\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -17.709999\n","best mean reward -17.120001\n","running time 1390.767831\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -17.709999084472656\n","Train_BestReturn : -17.1200008392334\n","TimeSinceStart : 1390.7678308486938\n","Exploitation Critic Loss : 0.1573803573846817\n","Exploration Critic Loss : 0.8622461557388306\n","Exploration Model Loss : 7.441185880452394e-05\n","Exploitation Data q-values : -6.248083114624023\n","Exploitation OOD q-values : -4.92178201675415\n","Exploitation CQL Loss : 1.326301097869873\n","Eval_AverageReturn : -13.309859275817871\n","Eval_StdReturn : 2.133512258529663\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.309859154929578\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -17.049999\n","best mean reward -17.049999\n","running time 1457.814793\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -17.049999237060547\n","Train_BestReturn : -17.049999237060547\n","TimeSinceStart : 1457.8147931098938\n","Exploitation Critic Loss : 0.2882544994354248\n","Exploration Critic Loss : 0.8224051594734192\n","Exploration Model Loss : 8.023098052944988e-05\n","Exploitation Data q-values : -6.233609676361084\n","Exploitation OOD q-values : -4.918219566345215\n","Exploitation CQL Loss : 1.3153904676437378\n","Eval_AverageReturn : -14.029850959777832\n","Eval_StdReturn : 2.1715264320373535\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -20.0\n","Eval_AverageEpLen : 15.029850746268657\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -16.740000\n","best mean reward -16.740000\n","running time 1521.167445\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -16.739999771118164\n","Train_BestReturn : -16.739999771118164\n","TimeSinceStart : 1521.1674449443817\n","Exploitation Critic Loss : 0.29629480838775635\n","Exploration Critic Loss : 0.708171546459198\n","Exploration Model Loss : 1.714419704512693e-05\n","Exploitation Data q-values : -6.552577018737793\n","Exploitation OOD q-values : -5.218945026397705\n","Exploitation CQL Loss : 1.333632469177246\n","Eval_AverageReturn : -13.720588684082031\n","Eval_StdReturn : 2.3753983974456787\n","Eval_MaxReturn : -8.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.720588235294118\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -17.020000\n","best mean reward -16.740000\n","running time 1592.304349\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -17.020000457763672\n","Train_BestReturn : -16.739999771118164\n","TimeSinceStart : 1592.304348707199\n","Exploitation Critic Loss : 0.8916544914245605\n","Exploration Critic Loss : 0.7438116073608398\n","Exploration Model Loss : 0.00018273535533808172\n","Exploitation Data q-values : -6.511995315551758\n","Exploitation OOD q-values : -5.136850357055664\n","Exploitation CQL Loss : 1.3751448392868042\n","Eval_AverageReturn : -13.666666984558105\n","Eval_StdReturn : 2.164714813232422\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -20.0\n","Eval_AverageEpLen : 14.666666666666666\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -17.350000\n","best mean reward -16.740000\n","running time 1657.542985\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -17.350000381469727\n","Train_BestReturn : -16.739999771118164\n","TimeSinceStart : 1657.5429849624634\n","Exploitation Critic Loss : 0.27603089809417725\n","Exploration Critic Loss : 0.7153299450874329\n","Exploration Model Loss : 0.00012459824210964143\n","Exploitation Data q-values : -6.044225215911865\n","Exploitation OOD q-values : -4.738876819610596\n","Exploitation CQL Loss : 1.3053483963012695\n","Eval_AverageReturn : -13.79411792755127\n","Eval_StdReturn : 2.440822124481201\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -21.0\n","Eval_AverageEpLen : 14.794117647058824\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -16.870001\n","best mean reward -16.740000\n","running time 1724.626768\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -16.8700008392334\n","Train_BestReturn : -16.739999771118164\n","TimeSinceStart : 1724.6267681121826\n","Exploitation Critic Loss : 0.27254718542099\n","Exploration Critic Loss : 0.6715388298034668\n","Exploration Model Loss : 0.00034974244772456586\n","Exploitation Data q-values : -6.03519344329834\n","Exploitation OOD q-values : -4.760163307189941\n","Exploitation CQL Loss : 1.2750297784805298\n","Eval_AverageReturn : -14.029850959777832\n","Eval_StdReturn : 2.298410654067993\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -21.0\n","Eval_AverageEpLen : 15.029850746268657\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -16.900000\n","best mean reward -16.740000\n","running time 1794.710989\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -16.899999618530273\n","Train_BestReturn : -16.739999771118164\n","TimeSinceStart : 1794.7109887599945\n","Exploitation Critic Loss : 0.15203185379505157\n","Exploration Critic Loss : 0.49043649435043335\n","Exploration Model Loss : 3.748051676666364e-05\n","Exploitation Data q-values : -6.028411388397217\n","Exploitation OOD q-values : -4.714294910430908\n","Exploitation CQL Loss : 1.3141165971755981\n","Eval_AverageReturn : -13.385714530944824\n","Eval_StdReturn : 2.508800745010376\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -24.0\n","Eval_AverageEpLen : 14.385714285714286\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -17.110001\n","best mean reward -16.740000\n","running time 1861.850583\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -17.110000610351562\n","Train_BestReturn : -16.739999771118164\n","TimeSinceStart : 1861.85058259964\n","Exploitation Critic Loss : 0.4591659605503082\n","Exploration Critic Loss : 0.6279867887496948\n","Exploration Model Loss : 3.0474313916784013e-07\n","Exploitation Data q-values : -6.127134323120117\n","Exploitation OOD q-values : -4.8687744140625\n","Exploitation CQL Loss : 1.2583599090576172\n","Eval_AverageReturn : -13.882352828979492\n","Eval_StdReturn : 2.09712290763855\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -18.0\n","Eval_AverageEpLen : 14.882352941176471\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -16.750000\n","best mean reward -16.740000\n","running time 1927.417096\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -16.75\n","Train_BestReturn : -16.739999771118164\n","TimeSinceStart : 1927.4170956611633\n","Exploitation Critic Loss : 0.4209402799606323\n","Exploration Critic Loss : 0.5847496390342712\n","Exploration Model Loss : 0.00012639997294172645\n","Exploitation Data q-values : -6.261742115020752\n","Exploitation OOD q-values : -4.999356269836426\n","Exploitation CQL Loss : 1.262385606765747\n","Eval_AverageReturn : -13.253520965576172\n","Eval_StdReturn : 2.1475062370300293\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.253521126760564\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -17.370001\n","best mean reward -16.740000\n","running time 1988.401944\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -17.3700008392334\n","Train_BestReturn : -16.739999771118164\n","TimeSinceStart : 1988.40194439888\n","Exploitation Critic Loss : 0.6385753154754639\n","Exploration Critic Loss : 0.6342586278915405\n","Exploration Model Loss : 0.00017170450882986188\n","Exploitation Data q-values : -6.382686614990234\n","Exploitation OOD q-values : -5.097038269042969\n","Exploitation CQL Loss : 1.285648226737976\n","Eval_AverageReturn : -13.882352828979492\n","Eval_StdReturn : 2.470588207244873\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -22.0\n","Eval_AverageEpLen : 14.882352941176471\n","Buffer size : 35001\n","Done logging...\n","\n","\n","Traceback (most recent call last):\n","  File \"cs285/scripts/run_hw5_expl.py\", line 136, in <module>\n","  File \"cs285/scripts/run_hw5_expl.py\", line 132, in main\n","    trainer.run_training_loop()\n","  File \"cs285/scripts/run_hw5_expl.py\", line 39, in run_training_loop\n","    eval_policy = self.rl_trainer.agent.actor,\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py\", line 201, in run_training_loop\n","    all_logs = self.train_agent()\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py\", line 260, in train_agent\n","    train_log = self.agent.train(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/agents/explore_or_exploit_agent.py\", line 64, in train\n","    expl_bonus = self.exploration_model.forward_np(ob_no)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/exploration/rnd_model.py\", line 58, in forward_np\n","    error = self(ob_no)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/exploration/rnd_model.py\", line 51, in forward\n","    fh_out = self.f_hat(ob_no)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 139, in forward\n","    input = module(input)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","KeyboardInterrupt\n"]}],"source":["!python cs285/scripts/run_hw5_expl.py --env_name PointmassEasy-v0 --use_rnd --unsupervised_exploration --exp_name q1_env1_rnd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1501838,"status":"ok","timestamp":1668945443315,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"PXsiv5nbz01f","outputId":"3ebbcf38-2a37-4171-83fa-c7c29a945c8a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env1_random_PointmassEasy-v0_20-11-2022_11-32-23 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env1_random_PointmassEasy-v0_20-11-2022_11-32-23\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002595\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.002595186233520508\n","Eval_AverageReturn : -49.380950927734375\n","Eval_StdReturn : 1.7036707401275635\n","Eval_MaxReturn : -43.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 49.523809523809526\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -50.000000\n","best mean reward -inf\n","running time 13.290227\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -50.0\n","TimeSinceStart : 13.290226936340332\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -50.000000\n","best mean reward -inf\n","running time 26.726448\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -50.0\n","TimeSinceStart : 26.726447820663452\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -50.000000\n","best mean reward -inf\n","running time 39.518960\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -50.0\n","TimeSinceStart : 39.51896047592163\n","Eval_AverageReturn : -49.0476188659668\n","Eval_StdReturn : 3.184429168701172\n","Eval_MaxReturn : -36.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 49.142857142857146\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -49.862499\n","best mean reward -inf\n","running time 52.761091\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -49.86249923706055\n","TimeSinceStart : 52.761091470718384\n","Eval_AverageReturn : -49.238094329833984\n","Eval_StdReturn : 3.407341957092285\n","Eval_MaxReturn : -34.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 49.285714285714285\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -49.889999\n","best mean reward -inf\n","running time 65.838807\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -49.88999938964844\n","TimeSinceStart : 65.83880662918091\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -49.889999\n","best mean reward -49.889999\n","running time 78.915398\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -49.88999938964844\n","Train_BestReturn : -49.88999938964844\n","TimeSinceStart : 78.91539764404297\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -49.889999\n","best mean reward -49.889999\n","running time 92.681044\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -49.88999938964844\n","Train_BestReturn : -49.88999938964844\n","TimeSinceStart : 92.68104362487793\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -49.889999\n","best mean reward -49.889999\n","running time 107.345544\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -49.88999938964844\n","Train_BestReturn : -49.88999938964844\n","TimeSinceStart : 107.34554433822632\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -50.000000\n","best mean reward -49.889999\n","running time 121.825691\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -50.0\n","Train_BestReturn : -49.88999938964844\n","TimeSinceStart : 121.82569098472595\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -50.000000\n","best mean reward -49.889999\n","running time 134.642344\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -50.0\n","Train_BestReturn : -49.88999938964844\n","TimeSinceStart : 134.64234399795532\n","Eval_AverageReturn : -49.380950927734375\n","Eval_StdReturn : 1.9142619371414185\n","Eval_MaxReturn : -43.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 49.476190476190474\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -49.779999\n","best mean reward -49.779999\n","running time 172.543754\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -49.779998779296875\n","Train_BestReturn : -49.779998779296875\n","TimeSinceStart : 172.54375410079956\n","Exploitation Critic Loss : 0.1093858927488327\n","Exploration Critic Loss : 0.0657159611582756\n","Exploration Model Loss : 0.4709306061267853\n","Exploitation Data q-values : -3.4804792404174805\n","Exploitation OOD q-values : -1.8726212978363037\n","Exploitation CQL Loss : 1.6078580617904663\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -48.430000\n","best mean reward -48.430000\n","running time 210.032223\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -48.43000030517578\n","Train_BestReturn : -48.43000030517578\n","TimeSinceStart : 210.03222274780273\n","Exploitation Critic Loss : 0.4523255228996277\n","Exploration Critic Loss : 0.20758242905139923\n","Exploration Model Loss : 0.0020963652059435844\n","Exploitation Data q-values : -5.520727157592773\n","Exploitation OOD q-values : -3.9141273498535156\n","Exploitation CQL Loss : 1.6065996885299683\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -42.430000\n","best mean reward -42.430000\n","running time 249.420736\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -42.43000030517578\n","Train_BestReturn : -42.43000030517578\n","TimeSinceStart : 249.4207363128662\n","Exploitation Critic Loss : 0.7007908225059509\n","Exploration Critic Loss : 0.2679281234741211\n","Exploration Model Loss : 0.022297346964478493\n","Exploitation Data q-values : -7.0852251052856445\n","Exploitation OOD q-values : -5.487412452697754\n","Exploitation CQL Loss : 1.5978126525878906\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -33.990002\n","best mean reward -33.990002\n","running time 292.777178\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -33.9900016784668\n","Train_BestReturn : -33.9900016784668\n","TimeSinceStart : 292.77717757225037\n","Exploitation Critic Loss : 0.8986884951591492\n","Exploration Critic Loss : 1.006129503250122\n","Exploration Model Loss : 0.0021349077578634024\n","Exploitation Data q-values : -8.484315872192383\n","Exploitation OOD q-values : -6.9122395515441895\n","Exploitation CQL Loss : 1.5720758438110352\n","Eval_AverageReturn : -14.272727012634277\n","Eval_StdReturn : 3.1553008556365967\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -29.0\n","Eval_AverageEpLen : 15.272727272727273\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -24.070000\n","best mean reward -24.070000\n","running time 346.556139\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -24.06999969482422\n","Train_BestReturn : -24.06999969482422\n","TimeSinceStart : 346.55613899230957\n","Exploitation Critic Loss : 0.7374988794326782\n","Exploration Critic Loss : 1.0922868251800537\n","Exploration Model Loss : 0.0028839579317718744\n","Exploitation Data q-values : -9.059423446655273\n","Exploitation OOD q-values : -7.488138198852539\n","Exploitation CQL Loss : 1.5712854862213135\n","Eval_AverageReturn : -14.920635223388672\n","Eval_StdReturn : 4.2325334548950195\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -33.0\n","Eval_AverageEpLen : 15.920634920634921\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -18.530001\n","best mean reward -18.530001\n","running time 402.526463\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -18.530000686645508\n","Train_BestReturn : -18.530000686645508\n","TimeSinceStart : 402.52646255493164\n","Exploitation Critic Loss : 1.1057313680648804\n","Exploration Critic Loss : 1.1885451078414917\n","Exploration Model Loss : 0.001055603614076972\n","Exploitation Data q-values : -9.03094482421875\n","Exploitation OOD q-values : -7.489818572998047\n","Exploitation CQL Loss : 1.5411263704299927\n","Eval_AverageReturn : -14.104477882385254\n","Eval_StdReturn : 2.3634932041168213\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -21.0\n","Eval_AverageEpLen : 15.104477611940299\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -18.040001\n","best mean reward -18.040001\n","running time 456.127704\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -18.040000915527344\n","Train_BestReturn : -18.040000915527344\n","TimeSinceStart : 456.12770414352417\n","Exploitation Critic Loss : 2.1379590034484863\n","Exploration Critic Loss : 2.277109146118164\n","Exploration Model Loss : 0.0009845069143921137\n","Exploitation Data q-values : -8.48397159576416\n","Exploitation OOD q-values : -6.959909439086914\n","Exploitation CQL Loss : 1.524062156677246\n","Eval_AverageReturn : -13.764705657958984\n","Eval_StdReturn : 2.2499518394470215\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.764705882352942\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -18.430000\n","best mean reward -18.040001\n","running time 513.649767\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -18.43000030517578\n","Train_BestReturn : -18.040000915527344\n","TimeSinceStart : 513.6497671604156\n","Exploitation Critic Loss : 1.3515485525131226\n","Exploration Critic Loss : 2.9830331802368164\n","Exploration Model Loss : 0.0003131515404675156\n","Exploitation Data q-values : -8.223215103149414\n","Exploitation OOD q-values : -6.71605110168457\n","Exploitation CQL Loss : 1.5071642398834229\n","Eval_AverageReturn : -13.65217399597168\n","Eval_StdReturn : 1.8945083618164062\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -18.0\n","Eval_AverageEpLen : 14.652173913043478\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -17.700001\n","best mean reward -17.700001\n","running time 569.344279\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -17.700000762939453\n","Train_BestReturn : -17.700000762939453\n","TimeSinceStart : 569.3442785739899\n","Exploitation Critic Loss : 0.23134353756904602\n","Exploration Critic Loss : 2.1646060943603516\n","Exploration Model Loss : 0.0003539598546922207\n","Exploitation Data q-values : -7.871458053588867\n","Exploitation OOD q-values : -6.36176872253418\n","Exploitation CQL Loss : 1.5096896886825562\n","Eval_AverageReturn : -13.75\n","Eval_StdReturn : 2.251633405685425\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.75\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -17.510000\n","best mean reward -17.510000\n","running time 624.546507\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -17.510000228881836\n","Train_BestReturn : -17.510000228881836\n","TimeSinceStart : 624.5465068817139\n","Exploitation Critic Loss : 0.374869167804718\n","Exploration Critic Loss : 1.6601688861846924\n","Exploration Model Loss : 0.00017758837202563882\n","Exploitation Data q-values : -8.044598579406738\n","Exploitation OOD q-values : -6.541016578674316\n","Exploitation CQL Loss : 1.503581166267395\n","Eval_AverageReturn : -13.779411315917969\n","Eval_StdReturn : 2.4665145874023438\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -20.0\n","Eval_AverageEpLen : 14.779411764705882\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -17.530001\n","best mean reward -17.510000\n","running time 682.320026\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -17.530000686645508\n","Train_BestReturn : -17.510000228881836\n","TimeSinceStart : 682.3200256824493\n","Exploitation Critic Loss : 0.11994659900665283\n","Exploration Critic Loss : 0.7175034880638123\n","Exploration Model Loss : 0.00011172707309015095\n","Exploitation Data q-values : -8.190019607543945\n","Exploitation OOD q-values : -6.704826354980469\n","Exploitation CQL Loss : 1.4851926565170288\n","Eval_AverageReturn : -13.764705657958984\n","Eval_StdReturn : 2.051668405532837\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -18.0\n","Eval_AverageEpLen : 14.764705882352942\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -17.459999\n","best mean reward -17.459999\n","running time 737.508391\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -17.459999084472656\n","Train_BestReturn : -17.459999084472656\n","TimeSinceStart : 737.5083909034729\n","Exploitation Critic Loss : 0.24659503996372223\n","Exploration Critic Loss : 1.3880516290664673\n","Exploration Model Loss : 6.868787022540346e-05\n","Exploitation Data q-values : -8.104984283447266\n","Exploitation OOD q-values : -6.6527323722839355\n","Exploitation CQL Loss : 1.4522507190704346\n","Eval_AverageReturn : -14.029850959777832\n","Eval_StdReturn : 2.115826368331909\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -20.0\n","Eval_AverageEpLen : 15.029850746268657\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -16.860001\n","best mean reward -16.860001\n","running time 792.883232\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -16.860000610351562\n","Train_BestReturn : -16.860000610351562\n","TimeSinceStart : 792.8832316398621\n","Exploitation Critic Loss : 1.1247529983520508\n","Exploration Critic Loss : 2.0804059505462646\n","Exploration Model Loss : 5.499952385434881e-05\n","Exploitation Data q-values : -7.959571838378906\n","Exploitation OOD q-values : -6.468064308166504\n","Exploitation CQL Loss : 1.4915074110031128\n","Eval_AverageReturn : -13.579710006713867\n","Eval_StdReturn : 2.052963972091675\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.579710144927537\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -17.610001\n","best mean reward -16.860001\n","running time 850.126662\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -17.610000610351562\n","Train_BestReturn : -16.860000610351562\n","TimeSinceStart : 850.1266622543335\n","Exploitation Critic Loss : 0.8204604387283325\n","Exploration Critic Loss : 1.7700811624526978\n","Exploration Model Loss : 3.1723189749754965e-05\n","Exploitation Data q-values : -7.792095184326172\n","Exploitation OOD q-values : -6.313633441925049\n","Exploitation CQL Loss : 1.4784619808197021\n","Eval_AverageReturn : -13.623188018798828\n","Eval_StdReturn : 1.9861946105957031\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -20.0\n","Eval_AverageEpLen : 14.623188405797102\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -18.110001\n","best mean reward -16.860001\n","running time 905.214511\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -18.110000610351562\n","Train_BestReturn : -16.860000610351562\n","TimeSinceStart : 905.2145111560822\n","Exploitation Critic Loss : 0.9080479145050049\n","Exploration Critic Loss : 2.499518632888794\n","Exploration Model Loss : 1.5981862816261128e-05\n","Exploitation Data q-values : -7.5613555908203125\n","Exploitation OOD q-values : -6.1129655838012695\n","Exploitation CQL Loss : 1.4483896493911743\n","Eval_AverageReturn : -13.594202995300293\n","Eval_StdReturn : 2.1011996269226074\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.594202898550725\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -18.049999\n","best mean reward -16.860001\n","running time 960.647967\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -18.049999237060547\n","Train_BestReturn : -16.860000610351562\n","TimeSinceStart : 960.6479668617249\n","Exploitation Critic Loss : 0.41384434700012207\n","Exploration Critic Loss : 1.4453285932540894\n","Exploration Model Loss : 1.3459015463013202e-05\n","Exploitation Data q-values : -7.416295528411865\n","Exploitation OOD q-values : -5.992802619934082\n","Exploitation CQL Loss : 1.4234925508499146\n","Eval_AverageReturn : -13.492753982543945\n","Eval_StdReturn : 1.8385816812515259\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -17.0\n","Eval_AverageEpLen : 14.492753623188406\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -17.490000\n","best mean reward -16.860001\n","running time 1020.563564\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -17.489999771118164\n","Train_BestReturn : -16.860000610351562\n","TimeSinceStart : 1020.5635643005371\n","Exploitation Critic Loss : 0.423926442861557\n","Exploration Critic Loss : 1.7908852100372314\n","Exploration Model Loss : 0.0001344334305031225\n","Exploitation Data q-values : -7.151708126068115\n","Exploitation OOD q-values : -5.743356704711914\n","Exploitation CQL Loss : 1.4083518981933594\n","Eval_AverageReturn : -14.44615364074707\n","Eval_StdReturn : 2.2602298259735107\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -20.0\n","Eval_AverageEpLen : 15.446153846153846\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -16.860001\n","best mean reward -16.860001\n","running time 1075.856828\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -16.860000610351562\n","Train_BestReturn : -16.860000610351562\n","TimeSinceStart : 1075.8568277359009\n","Exploitation Critic Loss : 1.2084674835205078\n","Exploration Critic Loss : 2.4329261779785156\n","Exploration Model Loss : 0.0002040092513198033\n","Exploitation Data q-values : -6.924524307250977\n","Exploitation OOD q-values : -5.517613887786865\n","Exploitation CQL Loss : 1.4069103002548218\n","Eval_AverageReturn : -13.428571701049805\n","Eval_StdReturn : 2.4352848529815674\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -20.0\n","Eval_AverageEpLen : 14.428571428571429\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -17.770000\n","best mean reward -16.860001\n","running time 1131.527981\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -17.770000457763672\n","Train_BestReturn : -16.860000610351562\n","TimeSinceStart : 1131.527981042862\n","Exploitation Critic Loss : 0.130792498588562\n","Exploration Critic Loss : 1.49303138256073\n","Exploration Model Loss : 2.3318501916946843e-05\n","Exploitation Data q-values : -6.687298774719238\n","Exploitation OOD q-values : -5.296689033508301\n","Exploitation CQL Loss : 1.3906102180480957\n","Eval_AverageReturn : -13.75\n","Eval_StdReturn : 2.178605794906616\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.75\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -18.139999\n","best mean reward -16.860001\n","running time 1185.790146\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -18.139999389648438\n","Train_BestReturn : -16.860000610351562\n","TimeSinceStart : 1185.790146112442\n","Exploitation Critic Loss : 0.7305740714073181\n","Exploration Critic Loss : 1.8341013193130493\n","Exploration Model Loss : 0.00020713679259642959\n","Exploitation Data q-values : -7.060691833496094\n","Exploitation OOD q-values : -5.70072078704834\n","Exploitation CQL Loss : 1.3599705696105957\n","Eval_AverageReturn : -13.565217018127441\n","Eval_StdReturn : 2.2423994541168213\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -21.0\n","Eval_AverageEpLen : 14.565217391304348\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -16.959999\n","best mean reward -16.860001\n","running time 1244.793443\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -16.959999084472656\n","Train_BestReturn : -16.860000610351562\n","TimeSinceStart : 1244.7934432029724\n","Exploitation Critic Loss : 0.13005517423152924\n","Exploration Critic Loss : 1.1516270637512207\n","Exploration Model Loss : 5.435610000859015e-05\n","Exploitation Data q-values : -7.454874038696289\n","Exploitation OOD q-values : -6.050875186920166\n","Exploitation CQL Loss : 1.403998851776123\n","Eval_AverageReturn : -13.720588684082031\n","Eval_StdReturn : 2.3442392349243164\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.720588235294118\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -16.540001\n","best mean reward -16.540001\n","running time 1299.587510\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -16.540000915527344\n","Train_BestReturn : -16.540000915527344\n","TimeSinceStart : 1299.5875103473663\n","Exploitation Critic Loss : 0.8098036050796509\n","Exploration Critic Loss : 1.4471882581710815\n","Exploration Model Loss : 4.6624772949144244e-05\n","Exploitation Data q-values : -7.148638725280762\n","Exploitation OOD q-values : -5.829753875732422\n","Exploitation CQL Loss : 1.3188846111297607\n","Eval_AverageReturn : -14.014925003051758\n","Eval_StdReturn : 3.1361756324768066\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -24.0\n","Eval_AverageEpLen : 15.014925373134329\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -16.879999\n","best mean reward -16.540001\n","running time 1354.501772\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -16.8799991607666\n","Train_BestReturn : -16.540000915527344\n","TimeSinceStart : 1354.5017721652985\n","Exploitation Critic Loss : 0.4781460762023926\n","Exploration Critic Loss : 1.1861512660980225\n","Exploration Model Loss : 7.062013992253924e-06\n","Exploitation Data q-values : -6.838964462280273\n","Exploitation OOD q-values : -5.528761863708496\n","Exploitation CQL Loss : 1.3102024793624878\n","Eval_AverageReturn : -13.79411792755127\n","Eval_StdReturn : 1.9293310642242432\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 14.794117647058824\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -17.459999\n","best mean reward -16.540001\n","running time 1420.476669\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -17.459999084472656\n","Train_BestReturn : -16.540000915527344\n","TimeSinceStart : 1420.4766693115234\n","Exploitation Critic Loss : 0.25390923023223877\n","Exploration Critic Loss : 0.9204875826835632\n","Exploration Model Loss : 0.0001370651152683422\n","Exploitation Data q-values : -7.2208967208862305\n","Exploitation OOD q-values : -5.854758262634277\n","Exploitation CQL Loss : 1.366137981414795\n","Eval_AverageReturn : -12.958333015441895\n","Eval_StdReturn : 2.009957790374756\n","Eval_MaxReturn : -9.0\n","Eval_MinReturn : -19.0\n","Eval_AverageEpLen : 13.958333333333334\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -17.639999\n","best mean reward -16.540001\n","running time 1477.974845\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -17.639999389648438\n","Train_BestReturn : -16.540000915527344\n","TimeSinceStart : 1477.9748454093933\n","Exploitation Critic Loss : 0.16124947369098663\n","Exploration Critic Loss : 1.0793410539627075\n","Exploration Model Loss : 3.8037673220969737e-05\n","Exploitation Data q-values : -6.916814804077148\n","Exploitation OOD q-values : -5.526871681213379\n","Exploitation CQL Loss : 1.38994300365448\n","Eval_AverageReturn : -13.867647171020508\n","Eval_StdReturn : 2.2420079708099365\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -20.0\n","Eval_AverageEpLen : 14.867647058823529\n","Buffer size : 35001\n","Done logging...\n","\n","\n","Traceback (most recent call last):\n","  File \"cs285/scripts/run_hw5_expl.py\", line 136, in <module>\n","    main()\n","  File \"cs285/scripts/run_hw5_expl.py\", line 132, in main\n","    trainer.run_training_loop()\n","  File \"cs285/scripts/run_hw5_expl.py\", line 39, in run_training_loop\n","    eval_policy = self.rl_trainer.agent.actor,\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py\", line 201, in run_training_loop\n","    all_logs = self.train_agent()\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py\", line 260, in train_agent\n","    train_log = self.agent.train(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/agents/explore_or_exploit_agent.py\", line 64, in train\n","    expl_bonus = self.exploration_model.forward_np(ob_no)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/exploration/rnd_model.py\", line 58, in forward_np\n","    error = self(ob_no)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/exploration/rnd_model.py\", line 50, in forward\n","    f_out = self.f(ob_no).detach()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 139, in forward\n","    input = module(input)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","KeyboardInterrupt\n"]}],"source":["!python cs285/scripts/run_hw5_expl.py --env_name PointmassEasy-v0 --unsupervised_exploration --exp_name q1_env1_random"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oMVtBJAA5HU-","outputId":"a6cd1069-e2d7-42ae-cefb-064fdce08df0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env2_rnd_PointmassMedium-v0_20-11-2022_11-57-46 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env2_rnd_PointmassMedium-v0_20-11-2022_11-57-46\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002371\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.002371072769165039\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 6.737004\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 6.737003564834595\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -142.857147\n","best mean reward -inf\n","running time 13.880406\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -142.85714721679688\n","TimeSinceStart : 13.88040566444397\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -145.000000\n","best mean reward -inf\n","running time 48.836203\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -145.0\n","TimeSinceStart : 48.83620262145996\n","Exploitation Critic Loss : 0.028650199994444847\n","Exploration Critic Loss : 0.14596101641654968\n","Exploration Model Loss : 0.7785481214523315\n","Exploitation Data q-values : -3.6900699138641357\n","Exploitation OOD q-values : -2.086857795715332\n","Exploitation CQL Loss : 1.6032122373580933\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -146.296295\n","best mean reward -inf\n","running time 81.283620\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -146.29629516601562\n","TimeSinceStart : 81.28361988067627\n","Exploitation Critic Loss : 0.08629842847585678\n","Exploration Critic Loss : 0.15161746740341187\n","Exploration Model Loss : 2.579090118408203\n","Exploitation Data q-values : -5.809206008911133\n","Exploitation OOD q-values : -4.2027177810668945\n","Exploitation CQL Loss : 1.6064882278442383\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -147.058823\n","best mean reward -inf\n","running time 113.024955\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -147.05882263183594\n","TimeSinceStart : 113.02495455741882\n","Exploitation Critic Loss : 0.1879282146692276\n","Exploration Critic Loss : 0.3892146944999695\n","Exploration Model Loss : 0.0012047404889017344\n","Exploitation Data q-values : -7.706748962402344\n","Exploitation OOD q-values : -6.093378067016602\n","Exploitation CQL Loss : 1.613370656967163\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -147.500000\n","best mean reward -inf\n","running time 144.355827\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -147.5\n","TimeSinceStart : 144.3558268547058\n","Exploitation Critic Loss : 0.8969836831092834\n","Exploration Critic Loss : 0.40772122144699097\n","Exploration Model Loss : 0.5792716145515442\n","Exploitation Data q-values : -9.404224395751953\n","Exploitation OOD q-values : -7.806252956390381\n","Exploitation CQL Loss : 1.5979716777801514\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -147.872345\n","best mean reward -inf\n","running time 176.360685\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -147.87234497070312\n","TimeSinceStart : 176.36068487167358\n","Exploitation Critic Loss : 0.6524736285209656\n","Exploration Critic Loss : 0.36806175112724304\n","Exploration Model Loss : 0.0007461195928044617\n","Exploitation Data q-values : -10.702378273010254\n","Exploitation OOD q-values : -9.090409278869629\n","Exploitation CQL Loss : 1.6119695901870728\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -148.148148\n","best mean reward -inf\n","running time 208.098949\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -148.1481475830078\n","TimeSinceStart : 208.09894919395447\n","Exploitation Critic Loss : 0.9669771194458008\n","Exploration Critic Loss : 0.42409247159957886\n","Exploration Model Loss : 0.000917373807169497\n","Exploitation Data q-values : -11.542743682861328\n","Exploitation OOD q-values : -9.883642196655273\n","Exploitation CQL Loss : 1.6591010093688965\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -148.333328\n","best mean reward -inf\n","running time 240.795769\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -148.3333282470703\n","TimeSinceStart : 240.79576921463013\n","Exploitation Critic Loss : 1.9568134546279907\n","Exploration Critic Loss : 0.5592878460884094\n","Exploration Model Loss : 0.000559790525585413\n","Exploitation Data q-values : -12.216754913330078\n","Exploitation OOD q-values : -10.510358810424805\n","Exploitation CQL Loss : 1.7063971757888794\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -148.507462\n","best mean reward -inf\n","running time 272.232760\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -148.50746154785156\n","TimeSinceStart : 272.23275995254517\n","Exploitation Critic Loss : 1.8113263845443726\n","Exploration Critic Loss : 0.6622753739356995\n","Exploration Model Loss : 0.00044897268526256084\n","Exploitation Data q-values : -11.308709144592285\n","Exploitation OOD q-values : -9.427555084228516\n","Exploitation CQL Loss : 1.881154179573059\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -142.688309\n","best mean reward -inf\n","running time 303.964163\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -142.6883087158203\n","TimeSinceStart : 303.96416330337524\n","Exploitation Critic Loss : 1.455650806427002\n","Exploration Critic Loss : 0.5951944589614868\n","Exploration Model Loss : 0.0006983899511396885\n","Exploitation Data q-values : -9.40643310546875\n","Exploitation OOD q-values : -7.70358943939209\n","Exploitation CQL Loss : 1.7028439044952393\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -142.879517\n","best mean reward -inf\n","running time 334.909672\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -142.8795166015625\n","TimeSinceStart : 334.90967178344727\n","Exploitation Critic Loss : 2.099766254425049\n","Exploration Critic Loss : 0.9049463272094727\n","Exploration Model Loss : 0.00032267923234030604\n","Exploitation Data q-values : -8.316848754882812\n","Exploitation OOD q-values : -6.728472709655762\n","Exploitation CQL Loss : 1.5883760452270508\n","Eval_AverageReturn : -138.625\n","Eval_StdReturn : 30.095420837402344\n","Eval_MaxReturn : -59.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 138.75\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -139.559143\n","best mean reward -inf\n","running time 366.985223\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -139.55914306640625\n","TimeSinceStart : 366.9852228164673\n","Exploitation Critic Loss : 0.2983272671699524\n","Exploration Critic Loss : 0.9207563996315002\n","Exploration Model Loss : 0.00026555999647825956\n","Exploitation Data q-values : -7.945807933807373\n","Exploitation OOD q-values : -6.256938934326172\n","Exploitation CQL Loss : 1.6888684034347534\n","Eval_AverageReturn : -139.625\n","Eval_StdReturn : 18.546815872192383\n","Eval_MaxReturn : -93.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 140.125\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -137.320007\n","best mean reward -137.320007\n","running time 402.157658\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -137.32000732421875\n","Train_BestReturn : -137.32000732421875\n","TimeSinceStart : 402.1576578617096\n","Exploitation Critic Loss : 0.5937110781669617\n","Exploration Critic Loss : 0.5962761044502258\n","Exploration Model Loss : 5.87442955293227e-05\n","Exploitation Data q-values : -7.822494983673096\n","Exploitation OOD q-values : -6.153209686279297\n","Exploitation CQL Loss : 1.6692858934402466\n","Eval_AverageReturn : -75.13333129882812\n","Eval_StdReturn : 28.936405181884766\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 76.06666666666666\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -126.160004\n","best mean reward -126.160004\n","running time 437.829406\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -126.16000366210938\n","Train_BestReturn : -126.16000366210938\n","TimeSinceStart : 437.82940554618835\n","Exploitation Critic Loss : 0.6456972360610962\n","Exploration Critic Loss : 0.8223813772201538\n","Exploration Model Loss : 6.066823334549554e-05\n","Exploitation Data q-values : -8.236869812011719\n","Exploitation OOD q-values : -6.566412925720215\n","Exploitation CQL Loss : 1.6704561710357666\n","Eval_AverageReturn : -74.86666870117188\n","Eval_StdReturn : 38.449737548828125\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 75.8\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -119.820000\n","best mean reward -119.820000\n","running time 471.447359\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -119.81999969482422\n","Train_BestReturn : -119.81999969482422\n","TimeSinceStart : 471.44735860824585\n","Exploitation Critic Loss : 0.23611225187778473\n","Exploration Critic Loss : 0.7729529142379761\n","Exploration Model Loss : 2.3662765670451336e-05\n","Exploitation Data q-values : -8.962451934814453\n","Exploitation OOD q-values : -7.339117050170898\n","Exploitation CQL Loss : 1.6233350038528442\n","Eval_AverageReturn : -45.0\n","Eval_StdReturn : 15.802762031555176\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -83.0\n","Eval_AverageEpLen : 46.0\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -105.830002\n","best mean reward -105.830002\n","running time 505.067903\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -105.83000183105469\n","Train_BestReturn : -105.83000183105469\n","TimeSinceStart : 505.06790256500244\n","Exploitation Critic Loss : 0.7766472101211548\n","Exploration Critic Loss : 0.9359756708145142\n","Exploration Model Loss : 2.6331903427490033e-05\n","Exploitation Data q-values : -9.295197486877441\n","Exploitation OOD q-values : -7.657195091247559\n","Exploitation CQL Loss : 1.6380020380020142\n","Eval_AverageReturn : -41.625\n","Eval_StdReturn : 16.89874839782715\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -108.0\n","Eval_AverageEpLen : 42.625\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -91.519997\n","best mean reward -91.519997\n","running time 540.606407\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -91.5199966430664\n","Train_BestReturn : -91.5199966430664\n","TimeSinceStart : 540.6064071655273\n","Exploitation Critic Loss : 0.5107736587524414\n","Exploration Critic Loss : 0.7834168672561646\n","Exploration Model Loss : 1.7767550161806867e-05\n","Exploitation Data q-values : -10.053742408752441\n","Exploitation OOD q-values : -8.45578384399414\n","Exploitation CQL Loss : 1.5979589223861694\n","Eval_AverageReturn : -68.33333587646484\n","Eval_StdReturn : 30.43608856201172\n","Eval_MaxReturn : -26.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 69.26666666666667\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -68.919998\n","best mean reward -68.919998\n","running time 577.978178\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -68.91999816894531\n","Train_BestReturn : -68.91999816894531\n","TimeSinceStart : 577.9781777858734\n","Exploitation Critic Loss : 0.6239668130874634\n","Exploration Critic Loss : 0.9966042637825012\n","Exploration Model Loss : 2.7165868232259527e-05\n","Exploitation Data q-values : -10.482397079467773\n","Exploitation OOD q-values : -8.868064880371094\n","Exploitation CQL Loss : 1.614332675933838\n","Eval_AverageReturn : -25.05128288269043\n","Eval_StdReturn : 6.492396831512451\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -39.0\n","Eval_AverageEpLen : 26.05128205128205\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -54.869999\n","best mean reward -54.869999\n","running time 617.090335\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -54.869998931884766\n","Train_BestReturn : -54.869998931884766\n","TimeSinceStart : 617.090334892273\n","Exploitation Critic Loss : 0.18082039058208466\n","Exploration Critic Loss : 1.2983522415161133\n","Exploration Model Loss : 8.936151425587013e-05\n","Exploitation Data q-values : -10.707147598266602\n","Exploitation OOD q-values : -9.083162307739258\n","Exploitation CQL Loss : 1.6239862442016602\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -46.660000\n","best mean reward -46.660000\n","running time 652.112478\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -46.65999984741211\n","Train_BestReturn : -46.65999984741211\n","TimeSinceStart : 652.112478017807\n","Exploitation Critic Loss : 0.33263325691223145\n","Exploration Critic Loss : 0.912754237651825\n","Exploration Model Loss : 0.00011139258276671171\n","Exploitation Data q-values : -10.610881805419922\n","Exploitation OOD q-values : -9.031061172485352\n","Exploitation CQL Loss : 1.5798203945159912\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -48.119999\n","best mean reward -46.660000\n","running time 685.150292\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -48.119998931884766\n","Train_BestReturn : -46.65999984741211\n","TimeSinceStart : 685.1502916812897\n","Exploitation Critic Loss : 0.22599084675312042\n","Exploration Critic Loss : 1.4492981433868408\n","Exploration Model Loss : 3.002650373673532e-05\n","Exploitation Data q-values : -11.103775024414062\n","Exploitation OOD q-values : -9.55012035369873\n","Exploitation CQL Loss : 1.5536541938781738\n","Eval_AverageReturn : -49.04999923706055\n","Eval_StdReturn : 21.392698287963867\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -94.0\n","Eval_AverageEpLen : 50.05\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -44.250000\n","best mean reward -44.250000\n","running time 721.883243\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -44.25\n","Train_BestReturn : -44.25\n","TimeSinceStart : 721.8832433223724\n","Exploitation Critic Loss : 1.6614480018615723\n","Exploration Critic Loss : 2.4297807216644287\n","Exploration Model Loss : 3.7525467632804066e-05\n","Exploitation Data q-values : -10.670309066772461\n","Exploitation OOD q-values : -9.138601303100586\n","Exploitation CQL Loss : 1.5317084789276123\n","Eval_AverageReturn : -27.0\n","Eval_StdReturn : 7.619419574737549\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -48.0\n","Eval_AverageEpLen : 28.0\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -41.810001\n","best mean reward -41.810001\n","running time 761.502229\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -41.810001373291016\n","Train_BestReturn : -41.810001373291016\n","TimeSinceStart : 761.5022292137146\n","Exploitation Critic Loss : 0.9105240702629089\n","Exploration Critic Loss : 1.1640002727508545\n","Exploration Model Loss : 0.00010736868716776371\n","Exploitation Data q-values : -10.893855094909668\n","Exploitation OOD q-values : -9.355588912963867\n","Exploitation CQL Loss : 1.5382652282714844\n","Eval_AverageReturn : -25.3157901763916\n","Eval_StdReturn : 7.280490398406982\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -46.0\n","Eval_AverageEpLen : 26.31578947368421\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -36.189999\n","best mean reward -36.189999\n","running time 803.047643\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -36.189998626708984\n","Train_BestReturn : -36.189998626708984\n","TimeSinceStart : 803.0476429462433\n","Exploitation Critic Loss : 0.18955111503601074\n","Exploration Critic Loss : 1.0737965106964111\n","Exploration Model Loss : 3.4470380342099816e-05\n","Exploitation Data q-values : -11.132508277893066\n","Exploitation OOD q-values : -9.546751022338867\n","Exploitation CQL Loss : 1.5857571363449097\n","Eval_AverageReturn : -21.44444465637207\n","Eval_StdReturn : 4.369365692138672\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -40.0\n","Eval_AverageEpLen : 22.444444444444443\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -29.780001\n","best mean reward -29.780001\n","running time 845.099365\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -29.780000686645508\n","Train_BestReturn : -29.780000686645508\n","TimeSinceStart : 845.099365234375\n","Exploitation Critic Loss : 0.30257633328437805\n","Exploration Critic Loss : 0.8935999870300293\n","Exploration Model Loss : 0.00023051943571772426\n","Exploitation Data q-values : -11.248809814453125\n","Exploitation OOD q-values : -9.7378511428833\n","Exploitation CQL Loss : 1.5109587907791138\n","Eval_AverageReturn : -22.604650497436523\n","Eval_StdReturn : 4.493610858917236\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -38.0\n","Eval_AverageEpLen : 23.6046511627907\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -28.770000\n","best mean reward -28.770000\n","running time 887.214664\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -28.770000457763672\n","Train_BestReturn : -28.770000457763672\n","TimeSinceStart : 887.2146635055542\n","Exploitation Critic Loss : 1.469273567199707\n","Exploration Critic Loss : 2.199775457382202\n","Exploration Model Loss : 6.603905058000237e-05\n","Exploitation Data q-values : -10.212514877319336\n","Exploitation OOD q-values : -8.691804885864258\n","Exploitation CQL Loss : 1.52070951461792\n","Eval_AverageReturn : -20.782608032226562\n","Eval_StdReturn : 3.4572739601135254\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -31.0\n","Eval_AverageEpLen : 21.782608695652176\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -27.590000\n","best mean reward -27.590000\n","running time 930.115518\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -27.59000015258789\n","Train_BestReturn : -27.59000015258789\n","TimeSinceStart : 930.1155178546906\n","Exploitation Critic Loss : 0.1379731446504593\n","Exploration Critic Loss : 1.2956119775772095\n","Exploration Model Loss : 8.731120033189654e-05\n","Exploitation Data q-values : -10.418219566345215\n","Exploitation OOD q-values : -8.893512725830078\n","Exploitation CQL Loss : 1.5247063636779785\n","Eval_AverageReturn : -21.422222137451172\n","Eval_StdReturn : 3.0146965980529785\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -29.0\n","Eval_AverageEpLen : 22.42222222222222\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -27.780001\n","best mean reward -27.590000\n","running time 971.343457\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -27.780000686645508\n","Train_BestReturn : -27.59000015258789\n","TimeSinceStart : 971.3434572219849\n","Exploitation Critic Loss : 1.114953637123108\n","Exploration Critic Loss : 1.3576202392578125\n","Exploration Model Loss : 1.7008904251269996e-05\n","Exploitation Data q-values : -10.349510192871094\n","Exploitation OOD q-values : -8.84532356262207\n","Exploitation CQL Loss : 1.5041861534118652\n","Eval_AverageReturn : -21.733333587646484\n","Eval_StdReturn : 3.4473819732666016\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -30.0\n","Eval_AverageEpLen : 22.733333333333334\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -27.889999\n","best mean reward -27.590000\n","running time 1012.877542\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -27.889999389648438\n","Train_BestReturn : -27.59000015258789\n","TimeSinceStart : 1012.8775415420532\n","Exploitation Critic Loss : 0.14498946070671082\n","Exploration Critic Loss : 1.0968518257141113\n","Exploration Model Loss : 8.865794370649382e-05\n","Exploitation Data q-values : -9.509593963623047\n","Exploitation OOD q-values : -8.016281127929688\n","Exploitation CQL Loss : 1.493312954902649\n","Eval_AverageReturn : -21.46666717529297\n","Eval_StdReturn : 3.3239870071411133\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -29.0\n","Eval_AverageEpLen : 22.466666666666665\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -27.969999\n","best mean reward -27.590000\n","running time 1053.851174\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -27.969999313354492\n","Train_BestReturn : -27.59000015258789\n","TimeSinceStart : 1053.851173877716\n","Exploitation Critic Loss : 0.3634911775588989\n","Exploration Critic Loss : 0.8920602798461914\n","Exploration Model Loss : 0.0003490775707177818\n","Exploitation Data q-values : -9.85221004486084\n","Exploitation OOD q-values : -8.312217712402344\n","Exploitation CQL Loss : 1.539991855621338\n","Eval_AverageReturn : -21.19565200805664\n","Eval_StdReturn : 3.6092848777770996\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -31.0\n","Eval_AverageEpLen : 22.195652173913043\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -27.950001\n","best mean reward -27.590000\n","running time 1095.371335\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -27.950000762939453\n","Train_BestReturn : -27.59000015258789\n","TimeSinceStart : 1095.3713347911835\n","Exploitation Critic Loss : 1.4131102561950684\n","Exploration Critic Loss : 1.0843126773834229\n","Exploration Model Loss : 6.1335140344453976e-06\n","Exploitation Data q-values : -9.46591854095459\n","Exploitation OOD q-values : -7.98114013671875\n","Exploitation CQL Loss : 1.4847787618637085\n","Eval_AverageReturn : -20.404254913330078\n","Eval_StdReturn : 3.5530004501342773\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -28.0\n","Eval_AverageEpLen : 21.404255319148938\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -27.809999\n","best mean reward -27.590000\n","running time 1138.250049\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -27.809999465942383\n","Train_BestReturn : -27.59000015258789\n","TimeSinceStart : 1138.2500486373901\n","Exploitation Critic Loss : 0.36518049240112305\n","Exploration Critic Loss : 0.8250148296356201\n","Exploration Model Loss : 0.0007781145977787673\n","Exploitation Data q-values : -9.767967224121094\n","Exploitation OOD q-values : -8.288204193115234\n","Exploitation CQL Loss : 1.479762077331543\n","Eval_AverageReturn : -21.863636016845703\n","Eval_StdReturn : 4.070910453796387\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -35.0\n","Eval_AverageEpLen : 22.863636363636363\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -27.860001\n","best mean reward -27.590000\n","running time 1179.785500\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -27.860000610351562\n","Train_BestReturn : -27.59000015258789\n","TimeSinceStart : 1179.7855002880096\n","Exploitation Critic Loss : 0.21648713946342468\n","Exploration Critic Loss : 0.8069995045661926\n","Exploration Model Loss : 3.338618398629478e-06\n","Exploitation Data q-values : -9.417272567749023\n","Exploitation OOD q-values : -7.910225868225098\n","Exploitation CQL Loss : 1.5070466995239258\n","Eval_AverageReturn : -23.560976028442383\n","Eval_StdReturn : 4.428558826446533\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -34.0\n","Eval_AverageEpLen : 24.5609756097561\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -27.120001\n","best mean reward -27.120001\n","running time 1220.697682\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -27.1200008392334\n","Train_BestReturn : -27.1200008392334\n","TimeSinceStart : 1220.6976823806763\n","Exploitation Critic Loss : 0.16929493844509125\n","Exploration Critic Loss : 0.5627030730247498\n","Exploration Model Loss : 6.479302101070061e-05\n","Exploitation Data q-values : -9.663553237915039\n","Exploitation OOD q-values : -8.212394714355469\n","Exploitation CQL Loss : 1.4511581659317017\n","Eval_AverageReturn : -20.84782600402832\n","Eval_StdReturn : 3.6053545475006104\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -28.0\n","Eval_AverageEpLen : 21.847826086956523\n","Buffer size : 35001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -26.790001\n","best mean reward -26.790001\n","running time 1264.411859\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -26.790000915527344\n","Train_BestReturn : -26.790000915527344\n","TimeSinceStart : 1264.411859035492\n","Exploitation Critic Loss : 0.7767767906188965\n","Exploration Critic Loss : 0.5348732471466064\n","Exploration Model Loss : 2.8270899292692775e-06\n","Exploitation Data q-values : -9.630369186401367\n","Exploitation OOD q-values : -8.19709300994873\n","Exploitation CQL Loss : 1.4332765340805054\n","Eval_AverageReturn : -21.622222900390625\n","Eval_StdReturn : 3.4402122497558594\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -30.0\n","Eval_AverageEpLen : 22.622222222222224\n","Buffer size : 36001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -25.760000\n","best mean reward -25.760000\n","running time 1306.760669\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -25.760000228881836\n","Train_BestReturn : -25.760000228881836\n","TimeSinceStart : 1306.760668516159\n","Exploitation Critic Loss : 0.6319035887718201\n","Exploration Critic Loss : 0.6807205677032471\n","Exploration Model Loss : 7.193782494141487e-07\n","Exploitation Data q-values : -9.83948802947998\n","Exploitation OOD q-values : -8.359718322753906\n","Exploitation CQL Loss : 1.4797688722610474\n","Eval_AverageReturn : -21.10869598388672\n","Eval_StdReturn : 3.5888006687164307\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -30.0\n","Eval_AverageEpLen : 22.108695652173914\n","Buffer size : 37001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -25.680000\n","best mean reward -25.680000\n","running time 1349.454822\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -25.68000030517578\n","Train_BestReturn : -25.68000030517578\n","TimeSinceStart : 1349.4548223018646\n","Exploitation Critic Loss : 0.15718722343444824\n","Exploration Critic Loss : 0.4533199965953827\n","Exploration Model Loss : 4.980481662641978e-07\n","Exploitation Data q-values : -9.310298919677734\n","Exploitation OOD q-values : -7.856701374053955\n","Exploitation CQL Loss : 1.4535975456237793\n","Eval_AverageReturn : -20.70212745666504\n","Eval_StdReturn : 3.4512710571289062\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -31.0\n","Eval_AverageEpLen : 21.70212765957447\n","Buffer size : 38001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -25.920000\n","best mean reward -25.680000\n","running time 1393.004653\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -25.920000076293945\n","Train_BestReturn : -25.68000030517578\n","TimeSinceStart : 1393.0046527385712\n","Exploitation Critic Loss : 0.18841753900051117\n","Exploration Critic Loss : 0.5182654857635498\n","Exploration Model Loss : 5.151280129211955e-05\n","Exploitation Data q-values : -9.534770965576172\n","Exploitation OOD q-values : -8.105016708374023\n","Exploitation CQL Loss : 1.4297536611557007\n","Eval_AverageReturn : -21.399999618530273\n","Eval_StdReturn : 3.7380921840667725\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -30.0\n","Eval_AverageEpLen : 22.4\n","Buffer size : 39001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -26.340000\n","best mean reward -25.680000\n","running time 1435.823926\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -26.34000015258789\n","Train_BestReturn : -25.68000030517578\n","TimeSinceStart : 1435.8239257335663\n","Exploitation Critic Loss : 0.19728994369506836\n","Exploration Critic Loss : 0.5585559606552124\n","Exploration Model Loss : 4.135119525017217e-05\n","Exploitation Data q-values : -9.304405212402344\n","Exploitation OOD q-values : -7.849055290222168\n","Exploitation CQL Loss : 1.4553508758544922\n","Eval_AverageReturn : -21.33333396911621\n","Eval_StdReturn : 2.943920373916626\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -29.0\n","Eval_AverageEpLen : 22.333333333333332\n","Buffer size : 40001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -27.129999\n","best mean reward -25.680000\n","running time 1478.312603\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -27.1299991607666\n","Train_BestReturn : -25.68000030517578\n","TimeSinceStart : 1478.3126032352448\n","Exploitation Critic Loss : 1.3861366510391235\n","Exploration Critic Loss : 0.5331010818481445\n","Exploration Model Loss : 7.684726733714342e-05\n","Exploitation Data q-values : -9.03841495513916\n","Exploitation OOD q-values : -7.611815929412842\n","Exploitation CQL Loss : 1.4265999794006348\n","Eval_AverageReturn : -21.799999237060547\n","Eval_StdReturn : 4.2353010177612305\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -35.0\n","Eval_AverageEpLen : 22.8\n","Buffer size : 41001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -27.410000\n","best mean reward -25.680000\n","running time 1521.260945\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -27.40999984741211\n","Train_BestReturn : -25.68000030517578\n","TimeSinceStart : 1521.2609446048737\n","Exploitation Critic Loss : 0.11549649387598038\n","Exploration Critic Loss : 0.42465928196907043\n","Exploration Model Loss : 8.47044702823041e-06\n","Exploitation Data q-values : -9.508423805236816\n","Exploitation OOD q-values : -8.042678833007812\n","Exploitation CQL Loss : 1.4657459259033203\n","Eval_AverageReturn : -22.0\n","Eval_StdReturn : 3.7719056606292725\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -32.0\n","Eval_AverageEpLen : 23.0\n","Buffer size : 42001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -27.030001\n","best mean reward -25.680000\n","running time 1563.743855\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -27.030000686645508\n","Train_BestReturn : -25.68000030517578\n","TimeSinceStart : 1563.743854522705\n","Exploitation Critic Loss : 0.3082725405693054\n","Exploration Critic Loss : 0.452948659658432\n","Exploration Model Loss : 0.00033822000841610134\n","Exploitation Data q-values : -9.084571838378906\n","Exploitation OOD q-values : -7.648393154144287\n","Exploitation CQL Loss : 1.4361786842346191\n","Eval_AverageReturn : -21.488889694213867\n","Eval_StdReturn : 3.9022626876831055\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -31.0\n","Eval_AverageEpLen : 22.488888888888887\n","Buffer size : 43001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -27.090000\n","best mean reward -25.680000\n","running time 1606.370844\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -27.09000015258789\n","Train_BestReturn : -25.68000030517578\n","TimeSinceStart : 1606.370843887329\n","Exploitation Critic Loss : 0.17183859646320343\n","Exploration Critic Loss : 0.48916733264923096\n","Exploration Model Loss : 3.225046327770542e-07\n","Exploitation Data q-values : -9.001755714416504\n","Exploitation OOD q-values : -7.571810722351074\n","Exploitation CQL Loss : 1.4299452304840088\n","Eval_AverageReturn : -21.35555648803711\n","Eval_StdReturn : 3.0489020347595215\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -27.0\n","Eval_AverageEpLen : 22.355555555555554\n","Buffer size : 44001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -27.490000\n","best mean reward -25.680000\n","running time 1648.813898\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -27.489999771118164\n","Train_BestReturn : -25.68000030517578\n","TimeSinceStart : 1648.8138978481293\n","Exploitation Critic Loss : 0.19688311219215393\n","Exploration Critic Loss : 0.41542986035346985\n","Exploration Model Loss : 4.589423042489216e-05\n","Exploitation Data q-values : -9.355205535888672\n","Exploitation OOD q-values : -7.9308977127075195\n","Exploitation CQL Loss : 1.424307942390442\n","Eval_AverageReturn : -20.978260040283203\n","Eval_StdReturn : 3.0608973503112793\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -29.0\n","Eval_AverageEpLen : 21.97826086956522\n","Buffer size : 45001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -27.580000\n","best mean reward -25.680000\n","running time 1691.224026\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -27.579999923706055\n","Train_BestReturn : -25.68000030517578\n","TimeSinceStart : 1691.2240262031555\n","Exploitation Critic Loss : 0.19569502770900726\n","Exploration Critic Loss : 0.48437803983688354\n","Exploration Model Loss : 3.1755098461871967e-07\n","Exploitation Data q-values : -8.966625213623047\n","Exploitation OOD q-values : -7.557399749755859\n","Exploitation CQL Loss : 1.4092258214950562\n","Eval_AverageReturn : -20.934782028198242\n","Eval_StdReturn : 3.4603476524353027\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -32.0\n","Eval_AverageEpLen : 21.934782608695652\n","Buffer size : 46001\n","Done logging...\n","\n","\n","Traceback (most recent call last):\n","  File \"cs285/scripts/run_hw5_expl.py\", line 136, in <module>\n","  File \"cs285/scripts/run_hw5_expl.py\", line 132, in main\n","    trainer.run_training_loop()\n","  File \"cs285/scripts/run_hw5_expl.py\", line 39, in run_training_loop\n","    eval_policy = self.rl_trainer.agent.actor,\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py\", line 201, in run_training_loop\n","    all_logs = self.train_agent()\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py\", line 260, in train_agent\n","    train_log = self.agent.train(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/agents/explore_or_exploit_agent.py\", line 64, in train\n","    expl_bonus = self.exploration_model.forward_np(ob_no)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/exploration/rnd_model.py\", line 58, in forward_np\n","    error = self(ob_no)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/exploration/rnd_model.py\", line 50, in forward\n","    f_out = self.f(ob_no).detach()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 139, in forward\n","    input = module(input)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\", line 354, in forward\n","    return torch.tanh(input)\n","KeyboardInterrupt\n","\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env2_random_PointmassMedium-v0_20-11-2022_12-26-18 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q1_env2_random_PointmassMedium-v0_20-11-2022_12-26-18\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002739\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0027391910552978516\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 6.276040\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 6.276040315628052\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 13.292614\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 13.292613506317139\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 20.153027\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 20.153027296066284\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -148.230774\n","best mean reward -inf\n","running time 26.632909\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -148.23077392578125\n","TimeSinceStart : 26.632909297943115\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -148.606064\n","best mean reward -inf\n","running time 33.322494\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -148.60606384277344\n","TimeSinceStart : 33.3224937915802\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -146.219513\n","best mean reward -inf\n","running time 40.185181\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -146.21951293945312\n","TimeSinceStart : 40.18518137931824\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -146.106384\n","best mean reward -inf\n","running time 46.918635\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -146.10638427734375\n","TimeSinceStart : 46.91863512992859\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -146.611115\n","best mean reward -inf\n","running time 53.816173\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -146.61111450195312\n","TimeSinceStart : 53.81617331504822\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -147.000000\n","best mean reward -inf\n","running time 60.803052\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -147.0\n","TimeSinceStart : 60.80305218696594\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -147.268661\n","best mean reward -inf\n","running time 67.792619\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -147.26866149902344\n","TimeSinceStart : 67.79261875152588\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -147.527023\n","best mean reward -inf\n","running time 99.497300\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -147.5270233154297\n","TimeSinceStart : 99.49729990959167\n","Exploitation Critic Loss : 0.05243997275829315\n","Exploration Critic Loss : 0.08451860398054123\n","Exploration Model Loss : 0.28140920400619507\n","Exploitation Data q-values : -3.5979185104370117\n","Exploitation OOD q-values : -1.9887874126434326\n","Exploitation CQL Loss : 1.6091312170028687\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -146.728394\n","best mean reward -inf\n","running time 131.944789\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -146.7283935546875\n","TimeSinceStart : 131.94478940963745\n","Exploitation Critic Loss : 0.09874469041824341\n","Exploration Critic Loss : 0.21645134687423706\n","Exploration Model Loss : 0.0011493858182802796\n","Exploitation Data q-values : -5.952985763549805\n","Exploitation OOD q-values : -4.343306541442871\n","Exploitation CQL Loss : 1.6096794605255127\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -146.988632\n","best mean reward -inf\n","running time 163.819560\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -146.98863220214844\n","TimeSinceStart : 163.81955981254578\n","Exploitation Critic Loss : 0.5188944339752197\n","Exploration Critic Loss : 0.11229409277439117\n","Exploration Model Loss : 0.0016167759895324707\n","Exploitation Data q-values : -7.664224624633789\n","Exploitation OOD q-values : -6.055233001708984\n","Exploitation CQL Loss : 1.6089907884597778\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -145.926315\n","best mean reward -inf\n","running time 195.437006\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -145.9263153076172\n","TimeSinceStart : 195.43700647354126\n","Exploitation Critic Loss : 0.7301267981529236\n","Exploration Critic Loss : 0.4044293761253357\n","Exploration Model Loss : 0.0007833096315152943\n","Exploitation Data q-values : -9.626808166503906\n","Exploitation OOD q-values : -8.013072967529297\n","Exploitation CQL Loss : 1.6137361526489258\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -140.250000\n","best mean reward -140.250000\n","running time 227.114986\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -140.25\n","Train_BestReturn : -140.25\n","TimeSinceStart : 227.114985704422\n","Exploitation Critic Loss : 0.35863083600997925\n","Exploration Critic Loss : 0.3674768805503845\n","Exploration Model Loss : 0.000929771747905761\n","Exploitation Data q-values : -10.914470672607422\n","Exploitation OOD q-values : -9.300518035888672\n","Exploitation CQL Loss : 1.6139527559280396\n","Eval_AverageReturn : -99.30000305175781\n","Eval_StdReturn : 35.93619155883789\n","Eval_MaxReturn : -55.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -136.830002\n","best mean reward -136.830002\n","running time 259.501186\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -136.8300018310547\n","Train_BestReturn : -136.8300018310547\n","TimeSinceStart : 259.5011863708496\n","Exploitation Critic Loss : 0.4965499937534332\n","Exploration Critic Loss : 0.5113328695297241\n","Exploration Model Loss : 0.003166692331433296\n","Exploitation Data q-values : -11.813167572021484\n","Exploitation OOD q-values : -10.20198917388916\n","Exploitation CQL Loss : 1.6111782789230347\n","Eval_AverageReturn : -63.125\n","Eval_StdReturn : 22.39384651184082\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -97.0\n","Eval_AverageEpLen : 64.125\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -126.419998\n","best mean reward -126.419998\n","running time 293.198038\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -126.41999816894531\n","Train_BestReturn : -126.41999816894531\n","TimeSinceStart : 293.19803762435913\n","Exploitation Critic Loss : 1.4682046175003052\n","Exploration Critic Loss : 1.4988139867782593\n","Exploration Model Loss : 0.0013338974677026272\n","Exploitation Data q-values : -12.1115083694458\n","Exploitation OOD q-values : -10.524344444274902\n","Exploitation CQL Loss : 1.5871635675430298\n","Eval_AverageReturn : -39.08000183105469\n","Eval_StdReturn : 10.97240161895752\n","Eval_MaxReturn : -19.0\n","Eval_MinReturn : -62.0\n","Eval_AverageEpLen : 40.08\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -121.889999\n","best mean reward -121.889999\n","running time 325.884355\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -121.88999938964844\n","Train_BestReturn : -121.88999938964844\n","TimeSinceStart : 325.88435530662537\n","Exploitation Critic Loss : 0.7642820477485657\n","Exploration Critic Loss : 1.5200495719909668\n","Exploration Model Loss : 0.0002868402225431055\n","Exploitation Data q-values : -12.889793395996094\n","Exploitation OOD q-values : -11.293326377868652\n","Exploitation CQL Loss : 1.5964668989181519\n","Eval_AverageReturn : -51.94736862182617\n","Eval_StdReturn : 13.566497802734375\n","Eval_MaxReturn : -26.0\n","Eval_MinReturn : -80.0\n","Eval_AverageEpLen : 52.94736842105263\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -109.809998\n","best mean reward -109.809998\n","running time 358.704932\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -109.80999755859375\n","Train_BestReturn : -109.80999755859375\n","TimeSinceStart : 358.70493245124817\n","Exploitation Critic Loss : 1.47806978225708\n","Exploration Critic Loss : 1.335007667541504\n","Exploration Model Loss : 0.0005957679823040962\n","Exploitation Data q-values : -13.081754684448242\n","Exploitation OOD q-values : -11.490692138671875\n","Exploitation CQL Loss : 1.5910632610321045\n","Eval_AverageReturn : -34.75862121582031\n","Eval_StdReturn : 10.377878189086914\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -60.0\n","Eval_AverageEpLen : 35.758620689655174\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -89.760002\n","best mean reward -89.760002\n","running time 394.839312\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -89.76000213623047\n","Train_BestReturn : -89.76000213623047\n","TimeSinceStart : 394.83931183815\n","Exploitation Critic Loss : 0.17269529402256012\n","Exploration Critic Loss : 3.095874309539795\n","Exploration Model Loss : 0.00029082869878038764\n","Exploitation Data q-values : -12.935160636901855\n","Exploitation OOD q-values : -11.383394241333008\n","Exploitation CQL Loss : 1.551767110824585\n","Eval_AverageReturn : -44.739131927490234\n","Eval_StdReturn : 13.38791561126709\n","Eval_MaxReturn : -20.0\n","Eval_MinReturn : -66.0\n","Eval_AverageEpLen : 45.73913043478261\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -68.720001\n","best mean reward -68.720001\n","running time 431.111891\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -68.72000122070312\n","Train_BestReturn : -68.72000122070312\n","TimeSinceStart : 431.11189126968384\n","Exploitation Critic Loss : 0.9540475606918335\n","Exploration Critic Loss : 2.0478391647338867\n","Exploration Model Loss : 0.000222005823161453\n","Exploitation Data q-values : -13.416460037231445\n","Exploitation OOD q-values : -11.849993705749512\n","Exploitation CQL Loss : 1.5664665699005127\n","Eval_AverageReturn : -28.58823585510254\n","Eval_StdReturn : 6.96680212020874\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -48.0\n","Eval_AverageEpLen : 29.58823529411765\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -51.599998\n","best mean reward -51.599998\n","running time 470.491906\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -51.599998474121094\n","Train_BestReturn : -51.599998474121094\n","TimeSinceStart : 470.4919059276581\n","Exploitation Critic Loss : 1.9963607788085938\n","Exploration Critic Loss : 3.794541597366333\n","Exploration Model Loss : 9.376031084684655e-05\n","Exploitation Data q-values : -13.31491470336914\n","Exploitation OOD q-values : -11.754049301147461\n","Exploitation CQL Loss : 1.560865879058838\n","Eval_AverageReturn : -28.457143783569336\n","Eval_StdReturn : 8.5701904296875\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 29.457142857142856\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -42.490002\n","best mean reward -42.490002\n","running time 509.755103\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -42.4900016784668\n","Train_BestReturn : -42.4900016784668\n","TimeSinceStart : 509.75510263442993\n","Exploitation Critic Loss : 0.4029424786567688\n","Exploration Critic Loss : 3.1679117679595947\n","Exploration Model Loss : 3.595713133108802e-05\n","Exploitation Data q-values : -13.054972648620605\n","Exploitation OOD q-values : -11.456954002380371\n","Exploitation CQL Loss : 1.5980185270309448\n","Eval_AverageReturn : -31.70967674255371\n","Eval_StdReturn : 14.999340057373047\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -98.0\n","Eval_AverageEpLen : 32.70967741935484\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -37.349998\n","best mean reward -37.349998\n","running time 545.967964\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -37.349998474121094\n","Train_BestReturn : -37.349998474121094\n","TimeSinceStart : 545.9679644107819\n","Exploitation Critic Loss : 0.8227794170379639\n","Exploration Critic Loss : 2.17492938041687\n","Exploration Model Loss : 4.482836811803281e-05\n","Exploitation Data q-values : -12.864632606506348\n","Exploitation OOD q-values : -11.256072044372559\n","Exploitation CQL Loss : 1.608560562133789\n","Eval_AverageReturn : -28.676469802856445\n","Eval_StdReturn : 9.706640243530273\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -58.0\n","Eval_AverageEpLen : 29.676470588235293\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -35.040001\n","best mean reward -35.040001\n","running time 584.582694\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -35.040000915527344\n","Train_BestReturn : -35.040000915527344\n","TimeSinceStart : 584.5826935768127\n","Exploitation Critic Loss : 0.23473256826400757\n","Exploration Critic Loss : 4.259309768676758\n","Exploration Model Loss : 2.5017776351887733e-05\n","Exploitation Data q-values : -12.293437957763672\n","Exploitation OOD q-values : -10.743810653686523\n","Exploitation CQL Loss : 1.5496282577514648\n","Eval_AverageReturn : -32.33333206176758\n","Eval_StdReturn : 15.6914701461792\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -96.0\n","Eval_AverageEpLen : 33.333333333333336\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -35.020000\n","best mean reward -35.020000\n","running time 622.904775\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -35.02000045776367\n","Train_BestReturn : -35.02000045776367\n","TimeSinceStart : 622.9047751426697\n","Exploitation Critic Loss : 0.22513478994369507\n","Exploration Critic Loss : 7.179862976074219\n","Exploration Model Loss : 1.4600615941162687e-05\n","Exploitation Data q-values : -12.099186897277832\n","Exploitation OOD q-values : -10.499323844909668\n","Exploitation CQL Loss : 1.5998631715774536\n","Eval_AverageReturn : -30.8125\n","Eval_StdReturn : 12.550990104675293\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -63.0\n","Eval_AverageEpLen : 31.8125\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -33.619999\n","best mean reward -33.619999\n","running time 660.405398\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -33.619998931884766\n","Train_BestReturn : -33.619998931884766\n","TimeSinceStart : 660.4053981304169\n","Exploitation Critic Loss : 0.24789714813232422\n","Exploration Critic Loss : 6.181528091430664\n","Exploration Model Loss : 6.402592407539487e-05\n","Exploitation Data q-values : -12.091867446899414\n","Exploitation OOD q-values : -10.526273727416992\n","Exploitation CQL Loss : 1.5655940771102905\n","Eval_AverageReturn : -32.83333206176758\n","Eval_StdReturn : 11.322790145874023\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -53.0\n","Eval_AverageEpLen : 33.833333333333336\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -32.759998\n","best mean reward -32.759998\n","running time 697.488059\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -32.7599983215332\n","Train_BestReturn : -32.7599983215332\n","TimeSinceStart : 697.4880585670471\n","Exploitation Critic Loss : 0.6488064527511597\n","Exploration Critic Loss : 7.725760459899902\n","Exploration Model Loss : 0.00022170584998093545\n","Exploitation Data q-values : -11.624943733215332\n","Exploitation OOD q-values : -10.069175720214844\n","Exploitation CQL Loss : 1.5557684898376465\n","Eval_AverageReturn : -25.710525512695312\n","Eval_StdReturn : 7.02592134475708\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -47.0\n","Eval_AverageEpLen : 26.710526315789473\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -31.690001\n","best mean reward -31.690001\n","running time 734.211877\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -31.690000534057617\n","Train_BestReturn : -31.690000534057617\n","TimeSinceStart : 734.2118773460388\n","Exploitation Critic Loss : 0.9081064462661743\n","Exploration Critic Loss : 7.660840034484863\n","Exploration Model Loss : 5.8284833357902244e-05\n","Exploitation Data q-values : -11.34827709197998\n","Exploitation OOD q-values : -9.778669357299805\n","Exploitation CQL Loss : 1.5696077346801758\n","Eval_AverageReturn : -32.35483932495117\n","Eval_StdReturn : 11.12552547454834\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -65.0\n","Eval_AverageEpLen : 33.354838709677416\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -30.160000\n","best mean reward -30.160000\n","running time 769.636752\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -30.15999984741211\n","Train_BestReturn : -30.15999984741211\n","TimeSinceStart : 769.6367518901825\n","Exploitation Critic Loss : 0.20040173828601837\n","Exploration Critic Loss : 9.257328033447266\n","Exploration Model Loss : 0.00013202076661400497\n","Exploitation Data q-values : -11.09150505065918\n","Exploitation OOD q-values : -9.568733215332031\n","Exploitation CQL Loss : 1.5227724313735962\n","Eval_AverageReturn : -26.783782958984375\n","Eval_StdReturn : 8.853569030761719\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -57.0\n","Eval_AverageEpLen : 27.783783783783782\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -29.340000\n","best mean reward -29.340000\n","running time 807.269230\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -29.34000015258789\n","Train_BestReturn : -29.34000015258789\n","TimeSinceStart : 807.2692296504974\n","Exploitation Critic Loss : 0.21339599788188934\n","Exploration Critic Loss : 8.884383201599121\n","Exploration Model Loss : 5.02532820974011e-05\n","Exploitation Data q-values : -11.272272109985352\n","Exploitation OOD q-values : -9.738285064697266\n","Exploitation CQL Loss : 1.5339869260787964\n","Eval_AverageReturn : -23.309524536132812\n","Eval_StdReturn : 5.198388576507568\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -36.0\n","Eval_AverageEpLen : 24.30952380952381\n","Buffer size : 31001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_expl.py --env_name PointmassMedium-v0 --use_rnd --unsupervised_exploration --exp_name q1_env2_rnd\n","!python cs285/scripts/run_hw5_expl.py --env_name PointmassMedium-v0 --unsupervised_exploration --exp_name q1_env2_random"]},{"cell_type":"markdown","metadata":{"id":"0G8Fc7EhOXS1"},"source":["## 2.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":555603,"status":"ok","timestamp":1669065066258,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"pFnfOlHR6fAp","outputId":"b7fa1785-9fc2-417a-c880-a8678c88bf60"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_dqn_PointmassMedium-v0_21-11-2022_21-01-52 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_dqn_PointmassMedium-v0_21-11-2022_21-01-52\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002202\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0022017955780029297\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 6.005880\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 6.005879878997803\n","Eval_AverageReturn : -148.0\n","Eval_StdReturn : 4.898979663848877\n","Eval_MaxReturn : -136.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.14285714285714\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -142.071426\n","best mean reward -inf\n","running time 12.593895\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -142.07142639160156\n","TimeSinceStart : 12.593895196914673\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -144.449997\n","best mean reward -inf\n","running time 44.309567\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -144.4499969482422\n","TimeSinceStart : 44.30956721305847\n","Exploitation Critic Loss : 0.06015836074948311\n","Exploration Critic Loss : 0.12286722660064697\n","Exploration Model Loss : 0.02422984503209591\n","Exploitation Data q-values : -3.7122678756713867\n","Exploitation OOD q-values : -2.1005783081054688\n","Exploitation CQL Loss : 1.6116896867752075\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -145.888885\n","best mean reward -inf\n","running time 76.874021\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -145.88888549804688\n","TimeSinceStart : 76.87402081489563\n","Exploitation Critic Loss : 0.27507251501083374\n","Exploration Critic Loss : 0.24236951768398285\n","Exploration Model Loss : 0.0037162438966333866\n","Exploitation Data q-values : -5.840256690979004\n","Exploitation OOD q-values : -4.237231254577637\n","Exploitation CQL Loss : 1.603026032447815\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -146.735291\n","best mean reward -inf\n","running time 108.639338\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -146.73529052734375\n","TimeSinceStart : 108.63933753967285\n","Exploitation Critic Loss : 0.18997284770011902\n","Exploration Critic Loss : 0.4799972176551819\n","Exploration Model Loss : 0.02572699636220932\n","Exploitation Data q-values : -7.5308027267456055\n","Exploitation OOD q-values : -5.922576427459717\n","Exploitation CQL Loss : 1.6082260608673096\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -145.195129\n","best mean reward -inf\n","running time 140.121212\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -145.19512939453125\n","TimeSinceStart : 140.1212122440338\n","Exploitation Critic Loss : 0.6234288215637207\n","Exploration Critic Loss : 0.6337130665779114\n","Exploration Model Loss : 0.0012628247495740652\n","Exploitation Data q-values : -9.55325698852539\n","Exploitation OOD q-values : -7.94558048248291\n","Exploitation CQL Loss : 1.6076769828796387\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -130.905655\n","best mean reward -inf\n","running time 170.060916\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -130.90565490722656\n","TimeSinceStart : 170.06091570854187\n","Exploitation Critic Loss : 0.8287449479103088\n","Exploration Critic Loss : 1.0813384056091309\n","Exploration Model Loss : 0.0010207550367340446\n","Exploitation Data q-values : -10.28400707244873\n","Exploitation OOD q-values : -8.703605651855469\n","Exploitation CQL Loss : 1.5804007053375244\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -124.562500\n","best mean reward -inf\n","running time 201.870086\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -124.5625\n","TimeSinceStart : 201.87008571624756\n","Exploitation Critic Loss : 0.5355969667434692\n","Exploration Critic Loss : 2.691901206970215\n","Exploration Model Loss : 0.001997289713472128\n","Exploitation Data q-values : -10.682892799377441\n","Exploitation OOD q-values : -9.0813627243042\n","Exploitation CQL Loss : 1.6015305519104004\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -126.742859\n","best mean reward -inf\n","running time 235.387250\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -126.74285888671875\n","TimeSinceStart : 235.38724970817566\n","Exploitation Critic Loss : 0.6567944288253784\n","Exploration Critic Loss : 1.0123043060302734\n","Exploration Model Loss : 0.0010608487064018846\n","Exploitation Data q-values : -11.968453407287598\n","Exploitation OOD q-values : -10.348307609558105\n","Exploitation CQL Loss : 1.6201446056365967\n","Eval_AverageReturn : -99.54545593261719\n","Eval_StdReturn : 34.9450569152832\n","Eval_MaxReturn : -54.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 100.36363636363636\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -128.857147\n","best mean reward -inf\n","running time 268.906523\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -128.85714721679688\n","TimeSinceStart : 268.90652298927307\n","Exploitation Critic Loss : 0.7150148153305054\n","Exploration Critic Loss : 1.0308300256729126\n","Exploration Model Loss : 0.00040215038461610675\n","Exploitation Data q-values : -12.233393669128418\n","Exploitation OOD q-values : -10.621370315551758\n","Exploitation CQL Loss : 1.612023115158081\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -130.619049\n","best mean reward -inf\n","running time 301.444109\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -130.61904907226562\n","TimeSinceStart : 301.4441087245941\n","Exploitation Critic Loss : 0.44846609234809875\n","Exploration Critic Loss : 0.815156102180481\n","Exploration Model Loss : 0.00037230420275591314\n","Exploitation Data q-values : -12.17280387878418\n","Exploitation OOD q-values : -10.52383804321289\n","Exploitation CQL Loss : 1.6489673852920532\n","Eval_AverageReturn : -148.2857208251953\n","Eval_StdReturn : 4.199125289916992\n","Eval_MaxReturn : -138.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.42857142857142\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -131.911118\n","best mean reward -inf\n","running time 332.619599\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -131.91111755371094\n","TimeSinceStart : 332.61959862709045\n","Exploitation Critic Loss : 2.6593339443206787\n","Exploration Critic Loss : 1.4260451793670654\n","Exploration Model Loss : 0.00023550308833364397\n","Exploitation Data q-values : -12.154953002929688\n","Exploitation OOD q-values : -10.494619369506836\n","Exploitation CQL Loss : 1.6603325605392456\n","Eval_AverageReturn : -139.0\n","Eval_StdReturn : 20.663978576660156\n","Eval_MaxReturn : -90.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 139.25\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -133.216492\n","best mean reward -inf\n","running time 363.162835\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -133.21649169921875\n","TimeSinceStart : 363.1628351211548\n","Exploitation Critic Loss : 1.7096446752548218\n","Exploration Critic Loss : 0.5004246234893799\n","Exploration Model Loss : 0.00015990891552064568\n","Exploitation Data q-values : -11.996927261352539\n","Exploitation OOD q-values : -10.291370391845703\n","Exploitation CQL Loss : 1.705557942390442\n","Eval_AverageReturn : -35.57143020629883\n","Eval_StdReturn : 10.331742286682129\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -61.0\n","Eval_AverageEpLen : 36.57142857142857\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -133.720001\n","best mean reward -133.720001\n","running time 397.357960\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -133.72000122070312\n","Train_BestReturn : -133.72000122070312\n","TimeSinceStart : 397.3579602241516\n","Exploitation Critic Loss : 0.21372108161449432\n","Exploration Critic Loss : 1.1416871547698975\n","Exploration Model Loss : 7.637519593117759e-05\n","Exploitation Data q-values : -12.143975257873535\n","Exploitation OOD q-values : -10.480378150939941\n","Exploitation CQL Loss : 1.6635980606079102\n","Eval_AverageReturn : -30.59375\n","Eval_StdReturn : 10.996759414672852\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -72.0\n","Eval_AverageEpLen : 31.59375\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -133.720001\n","best mean reward -133.720001\n","running time 432.313497\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -133.72000122070312\n","Train_BestReturn : -133.72000122070312\n","TimeSinceStart : 432.3134973049164\n","Exploitation Critic Loss : 0.23242269456386566\n","Exploration Critic Loss : 2.9584946632385254\n","Exploration Model Loss : 5.518013131222688e-05\n","Exploitation Data q-values : -11.632638931274414\n","Exploitation OOD q-values : -9.967927932739258\n","Exploitation CQL Loss : 1.6647111177444458\n","Eval_AverageReturn : -31.419355392456055\n","Eval_StdReturn : 8.023249626159668\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -45.0\n","Eval_AverageEpLen : 32.41935483870968\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -134.830002\n","best mean reward -133.720001\n","running time 466.708526\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -134.8300018310547\n","Train_BestReturn : -133.72000122070312\n","TimeSinceStart : 466.70852637290955\n","Exploitation Critic Loss : 0.47243252396583557\n","Exploration Critic Loss : 3.793917179107666\n","Exploration Model Loss : 3.261332676629536e-05\n","Exploitation Data q-values : -11.591962814331055\n","Exploitation OOD q-values : -9.960386276245117\n","Exploitation CQL Loss : 1.6315754652023315\n","Eval_AverageReturn : -32.25806427001953\n","Eval_StdReturn : 11.959522247314453\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -82.0\n","Eval_AverageEpLen : 33.25806451612903\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -134.830002\n","best mean reward -133.720001\n","running time 501.475688\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -134.8300018310547\n","Train_BestReturn : -133.72000122070312\n","TimeSinceStart : 501.4756875038147\n","Exploitation Critic Loss : 0.1557498574256897\n","Exploration Critic Loss : 2.343776226043701\n","Exploration Model Loss : 2.1317249775165692e-05\n","Exploitation Data q-values : -11.454360008239746\n","Exploitation OOD q-values : -9.805160522460938\n","Exploitation CQL Loss : 1.6492000818252563\n","Eval_AverageReturn : -31.21875\n","Eval_StdReturn : 10.567444801330566\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -61.0\n","Eval_AverageEpLen : 32.21875\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -134.830002\n","best mean reward -133.720001\n","running time 537.453862\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -134.8300018310547\n","Train_BestReturn : -133.72000122070312\n","TimeSinceStart : 537.4538621902466\n","Exploitation Critic Loss : 0.7082347273826599\n","Exploration Critic Loss : 2.0215039253234863\n","Exploration Model Loss : 2.37764779740246e-05\n","Exploitation Data q-values : -11.662927627563477\n","Exploitation OOD q-values : -9.996444702148438\n","Exploitation CQL Loss : 1.6664838790893555\n","Eval_AverageReturn : -30.75\n","Eval_StdReturn : 11.266654014587402\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -80.0\n","Eval_AverageEpLen : 31.75\n","Buffer size : 10001\n","Done logging...\n","\n","\n","Traceback (most recent call last):\n","  File \"cs285/scripts/run_hw5_expl.py\", line 136, in <module>\n","    main()\n","  File \"cs285/scripts/run_hw5_expl.py\", line 132, in main\n","    trainer.run_training_loop()\n","  File \"cs285/scripts/run_hw5_expl.py\", line 39, in run_training_loop\n","    eval_policy = self.rl_trainer.agent.actor,\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py\", line 201, in run_training_loop\n","    all_logs = self.train_agent()\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py\", line 260, in train_agent\n","    train_log = self.agent.train(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/agents/explore_or_exploit_agent.py\", line 87, in train\n","    expl_model_loss = self.exploration_model.update(next_ob_no)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/exploration/rnd_model.py\", line 67, in update\n","    err = self(ob_no)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/exploration/rnd_model.py\", line 51, in forward\n","    fh_out = self.f_hat(ob_no)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 139, in forward\n","    input = module(input)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","KeyboardInterrupt\n"]}],"source":["!python cs285/scripts/run_hw5_expl.py --env_name PointmassMedium-v0 --exp_name q2_dqn \\\n"," --use_rnd --unsupervised_exploration --offline_exploitation --cql_alpha=0 --seed 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":300,"status":"ok","timestamp":1669027565162,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"QUDoJfKJ-ezn","outputId":"d5a72c36-a95e-4727-ce03-c2243f95fc50"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_PointmassMedium-v0_21-11-2022_10-19-10 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_PointmassMedium-v0_21-11-2022_10-19-10\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002222\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.002222299575805664\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 5.932610\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 5.932609796524048\n","Eval_AverageReturn : -148.0\n","Eval_StdReturn : 4.898979663848877\n","Eval_MaxReturn : -136.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.14285714285714\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -142.071426\n","best mean reward -inf\n","running time 12.433046\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -142.07142639160156\n","TimeSinceStart : 12.433046340942383\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -144.449997\n","best mean reward -inf\n","running time 43.388862\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -144.4499969482422\n","TimeSinceStart : 43.38886213302612\n","Exploitation Critic Loss : 0.22031137347221375\n","Exploration Critic Loss : 0.2621401846408844\n","Exploration Model Loss : 0.0015901454025879502\n","Exploitation Data q-values : -3.72876238822937\n","Exploitation OOD q-values : -2.12913179397583\n","Exploitation CQL Loss : 1.5996302366256714\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -145.888885\n","best mean reward -inf\n","running time 72.900793\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -145.88888549804688\n","TimeSinceStart : 72.9007933139801\n","Exploitation Critic Loss : 0.4537157416343689\n","Exploration Critic Loss : 0.24646148085594177\n","Exploration Model Loss : 0.0019357947167009115\n","Exploitation Data q-values : -5.922465801239014\n","Exploitation OOD q-values : -4.325976371765137\n","Exploitation CQL Loss : 1.5964891910552979\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -145.647064\n","best mean reward -inf\n","running time 103.993675\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -145.64706420898438\n","TimeSinceStart : 103.99367475509644\n","Exploitation Critic Loss : 0.3787676990032196\n","Exploration Critic Loss : 0.1935310959815979\n","Exploration Model Loss : 0.08999921381473541\n","Exploitation Data q-values : -7.642073154449463\n","Exploitation OOD q-values : -6.068946838378906\n","Exploitation CQL Loss : 1.573126196861267\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -146.300003\n","best mean reward -inf\n","running time 132.274777\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -146.3000030517578\n","TimeSinceStart : 132.2747769355774\n","Exploitation Critic Loss : 1.0948193073272705\n","Exploration Critic Loss : 0.2923010587692261\n","Exploration Model Loss : 0.001432244200259447\n","Exploitation Data q-values : -9.655855178833008\n","Exploitation OOD q-values : -8.059537887573242\n","Exploitation CQL Loss : 1.596317172050476\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -146.063828\n","best mean reward -inf\n","running time 163.785852\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -146.06382751464844\n","TimeSinceStart : 163.78585243225098\n","Exploitation Critic Loss : 0.23166757822036743\n","Exploration Critic Loss : 0.432137131690979\n","Exploration Model Loss : 0.002968825865536928\n","Exploitation Data q-values : -10.718584060668945\n","Exploitation OOD q-values : -9.098333358764648\n","Exploitation CQL Loss : 1.6202512979507446\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -146.574081\n","best mean reward -inf\n","running time 193.516530\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -146.57408142089844\n","TimeSinceStart : 193.5165295600891\n","Exploitation Critic Loss : 0.28569287061691284\n","Exploration Critic Loss : 0.6957071423530579\n","Exploration Model Loss : 0.0009199597407132387\n","Exploitation Data q-values : -11.545103073120117\n","Exploitation OOD q-values : -9.847868919372559\n","Exploitation CQL Loss : 1.697234034538269\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -146.491806\n","best mean reward -inf\n","running time 223.619073\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -146.49180603027344\n","TimeSinceStart : 223.61907315254211\n","Exploitation Critic Loss : 0.3186835050582886\n","Exploration Critic Loss : 1.1159889698028564\n","Exploration Model Loss : 0.0006454287795349956\n","Exploitation Data q-values : -11.497536659240723\n","Exploitation OOD q-values : -9.824207305908203\n","Exploitation CQL Loss : 1.6733288764953613\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -146.058823\n","best mean reward -inf\n","running time 253.040543\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -146.05882263183594\n","TimeSinceStart : 253.04054307937622\n","Exploitation Critic Loss : 1.0980867147445679\n","Exploration Critic Loss : 1.3178634643554688\n","Exploration Model Loss : 0.0004111544112674892\n","Exploitation Data q-values : -11.139975547790527\n","Exploitation OOD q-values : -9.429901123046875\n","Exploitation CQL Loss : 1.7100750207901\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -146.426666\n","best mean reward -inf\n","running time 281.515386\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -146.42666625976562\n","TimeSinceStart : 281.5153856277466\n","Exploitation Critic Loss : 0.8634415864944458\n","Exploration Critic Loss : 0.8819459676742554\n","Exploration Model Loss : 0.00029080038075335324\n","Exploitation Data q-values : -10.54886245727539\n","Exploitation OOD q-values : -8.88045883178711\n","Exploitation CQL Loss : 1.66840398311615\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -146.691360\n","best mean reward -inf\n","running time 314.141052\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -146.6913604736328\n","TimeSinceStart : 314.14105224609375\n","Exploitation Critic Loss : 0.46131861209869385\n","Exploration Critic Loss : 1.7307502031326294\n","Exploration Model Loss : 0.0001651544007472694\n","Exploitation Data q-values : -10.620138168334961\n","Exploitation OOD q-values : -8.998283386230469\n","Exploitation CQL Loss : 1.6218537092208862\n","Eval_AverageReturn : -126.11111450195312\n","Eval_StdReturn : 35.26548385620117\n","Eval_MaxReturn : -52.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 126.66666666666667\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -146.954544\n","best mean reward -inf\n","running time 342.916583\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -146.9545440673828\n","TimeSinceStart : 342.91658329963684\n","Exploitation Critic Loss : 0.988177478313446\n","Exploration Critic Loss : 2.7371866703033447\n","Exploration Model Loss : 0.00013065262464806437\n","Exploitation Data q-values : -10.53384780883789\n","Exploitation OOD q-values : -8.890226364135742\n","Exploitation CQL Loss : 1.643622636795044\n","Eval_AverageReturn : -68.06666564941406\n","Eval_StdReturn : 28.49202537536621\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -122.0\n","Eval_AverageEpLen : 69.06666666666666\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -147.178940\n","best mean reward -inf\n","running time 375.114699\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -147.17893981933594\n","TimeSinceStart : 375.11469864845276\n","Exploitation Critic Loss : 0.5726185441017151\n","Exploration Critic Loss : 1.1348209381103516\n","Exploration Model Loss : 6.498526636278257e-05\n","Exploitation Data q-values : -10.808340072631836\n","Exploitation OOD q-values : -9.172708511352539\n","Exploitation CQL Loss : 1.635631799697876\n","Eval_AverageReturn : -67.26667022705078\n","Eval_StdReturn : 23.109785079956055\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -103.0\n","Eval_AverageEpLen : 68.26666666666667\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -147.320007\n","best mean reward -147.320007\n","running time 404.316904\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -147.32000732421875\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 404.31690430641174\n","Exploitation Critic Loss : 0.2847899794578552\n","Exploration Critic Loss : 2.5057387351989746\n","Exploration Model Loss : 4.5733631850453094e-05\n","Exploitation Data q-values : -11.022780418395996\n","Exploitation OOD q-values : -9.382207870483398\n","Exploitation CQL Loss : 1.6405726671218872\n","Eval_AverageReturn : -31.612903594970703\n","Eval_StdReturn : 8.82692813873291\n","Eval_MaxReturn : -19.0\n","Eval_MinReturn : -57.0\n","Eval_AverageEpLen : 32.61290322580645\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -148.169998\n","best mean reward -147.320007\n","running time 439.872574\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -148.1699981689453\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 439.8725736141205\n","Exploitation Critic Loss : 2.0452566146850586\n","Exploration Critic Loss : 1.7768194675445557\n","Exploration Model Loss : 2.6257201170665212e-05\n","Exploitation Data q-values : -11.159039497375488\n","Exploitation OOD q-values : -9.575294494628906\n","Exploitation CQL Loss : 1.5837454795837402\n","Eval_AverageReturn : -44.0\n","Eval_StdReturn : 18.4107723236084\n","Eval_MaxReturn : -21.0\n","Eval_MinReturn : -82.0\n","Eval_AverageEpLen : 45.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -148.429993\n","best mean reward -147.320007\n","running time 470.514616\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -148.42999267578125\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 470.51461577415466\n","Exploitation Critic Loss : 0.236734539270401\n","Exploration Critic Loss : 1.1546984910964966\n","Exploration Model Loss : 2.252724880236201e-05\n","Exploitation Data q-values : -11.524913787841797\n","Exploitation OOD q-values : -9.925549507141113\n","Exploitation CQL Loss : 1.5993642807006836\n","Eval_AverageReturn : -46.1363639831543\n","Eval_StdReturn : 17.125991821289062\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -79.0\n","Eval_AverageEpLen : 47.13636363636363\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -148.429993\n","best mean reward -147.320007\n","running time 503.276031\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -148.42999267578125\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 503.2760305404663\n","Exploitation Critic Loss : 1.3539979457855225\n","Exploration Critic Loss : 2.336878776550293\n","Exploration Model Loss : 1.3747563571087085e-05\n","Exploitation Data q-values : -12.039802551269531\n","Exploitation OOD q-values : -10.402055740356445\n","Exploitation CQL Loss : 1.6377471685409546\n","Eval_AverageReturn : -61.75\n","Eval_StdReturn : 15.497983932495117\n","Eval_MaxReturn : -35.0\n","Eval_MinReturn : -101.0\n","Eval_AverageEpLen : 62.75\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -148.429993\n","best mean reward -147.320007\n","running time 533.423145\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -148.42999267578125\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 533.4231445789337\n","Exploitation Critic Loss : 0.9741957187652588\n","Exploration Critic Loss : 1.5928810834884644\n","Exploration Model Loss : 5.326891550794244e-05\n","Exploitation Data q-values : -12.385871887207031\n","Exploitation OOD q-values : -10.780186653137207\n","Exploitation CQL Loss : 1.6056861877441406\n","Eval_AverageReturn : -41.625\n","Eval_StdReturn : 11.575161933898926\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -70.0\n","Eval_AverageEpLen : 42.625\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -148.800003\n","best mean reward -147.320007\n","running time 567.179058\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -148.8000030517578\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 567.1790580749512\n","Exploitation Critic Loss : 0.6749080419540405\n","Exploration Critic Loss : 0.907699465751648\n","Exploration Model Loss : 3.303690755274147e-05\n","Exploitation Data q-values : -12.744671821594238\n","Exploitation OOD q-values : -11.112226486206055\n","Exploitation CQL Loss : 1.6324453353881836\n","Eval_AverageReturn : -41.20833206176758\n","Eval_StdReturn : 13.904373168945312\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -85.0\n","Eval_AverageEpLen : 42.208333333333336\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -148.800003\n","best mean reward -147.320007\n","running time 597.858052\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -148.8000030517578\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 597.8580520153046\n","Exploitation Critic Loss : 1.8013731241226196\n","Exploration Critic Loss : 2.1432292461395264\n","Exploration Model Loss : 6.454442336689681e-05\n","Exploitation Data q-values : -12.72111701965332\n","Exploitation OOD q-values : -11.114326477050781\n","Exploitation CQL Loss : 1.6067898273468018\n","Eval_AverageReturn : -37.61538314819336\n","Eval_StdReturn : 13.967206001281738\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -78.0\n","Eval_AverageEpLen : 38.61538461538461\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -149.169998\n","best mean reward -147.320007\n","running time 631.560642\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -149.1699981689453\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 631.5606424808502\n","Exploitation Critic Loss : 0.3822730779647827\n","Exploration Critic Loss : 2.9094924926757812\n","Exploration Model Loss : 5.9364519984228536e-05\n","Exploitation Data q-values : -12.856788635253906\n","Exploitation OOD q-values : -11.266998291015625\n","Exploitation CQL Loss : 1.5897908210754395\n","Eval_AverageReturn : -36.925926208496094\n","Eval_StdReturn : 12.872382164001465\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -69.0\n","Eval_AverageEpLen : 37.925925925925924\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -149.460007\n","best mean reward -147.320007\n","running time 666.442237\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -149.4600067138672\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 666.4422373771667\n","Exploitation Critic Loss : 1.0165908336639404\n","Exploration Critic Loss : 1.554826259613037\n","Exploration Model Loss : 0.00015766861906740814\n","Exploitation Data q-values : -13.008916854858398\n","Exploitation OOD q-values : -11.372858047485352\n","Exploitation CQL Loss : 1.6360594034194946\n","Eval_AverageReturn : -34.482757568359375\n","Eval_StdReturn : 10.354477882385254\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -60.0\n","Eval_AverageEpLen : 35.48275862068966\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 698.425843\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 698.4258434772491\n","Exploitation Critic Loss : 1.068662166595459\n","Exploration Critic Loss : 1.5937377214431763\n","Exploration Model Loss : 4.888252806267701e-05\n","Exploitation Data q-values : -12.768636703491211\n","Exploitation OOD q-values : -11.136255264282227\n","Exploitation CQL Loss : 1.6323806047439575\n","Eval_AverageReturn : -39.46154022216797\n","Eval_StdReturn : 14.115541458129883\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -74.0\n","Eval_AverageEpLen : 40.46153846153846\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 733.323956\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 733.323956489563\n","Exploitation Critic Loss : 0.27759918570518494\n","Exploration Critic Loss : 2.2259130477905273\n","Exploration Model Loss : 0.00019218993838876486\n","Exploitation Data q-values : -12.750186920166016\n","Exploitation OOD q-values : -11.09770679473877\n","Exploitation CQL Loss : 1.652479887008667\n","Eval_AverageReturn : -41.45833206176758\n","Eval_StdReturn : 15.710874557495117\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -80.0\n","Eval_AverageEpLen : 42.458333333333336\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 764.440166\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 764.4401659965515\n","Exploitation Critic Loss : 0.2902333736419678\n","Exploration Critic Loss : 1.4714081287384033\n","Exploration Model Loss : 4.116835771128535e-05\n","Exploitation Data q-values : -12.632699012756348\n","Exploitation OOD q-values : -11.015909194946289\n","Exploitation CQL Loss : 1.6167893409729004\n","Eval_AverageReturn : -30.65625\n","Eval_StdReturn : 6.720348834991455\n","Eval_MaxReturn : -20.0\n","Eval_MinReturn : -45.0\n","Eval_AverageEpLen : 31.65625\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 800.156613\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 800.156613111496\n","Exploitation Critic Loss : 1.1400346755981445\n","Exploration Critic Loss : 3.2445032596588135\n","Exploration Model Loss : 6.1936862039146945e-06\n","Exploitation Data q-values : -12.538918495178223\n","Exploitation OOD q-values : -10.954666137695312\n","Exploitation CQL Loss : 1.5842528343200684\n","Eval_AverageReturn : -32.46666717529297\n","Eval_StdReturn : 10.784969329833984\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -59.0\n","Eval_AverageEpLen : 33.46666666666667\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 831.777376\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 831.7773764133453\n","Exploitation Critic Loss : 0.7707468271255493\n","Exploration Critic Loss : 1.8028266429901123\n","Exploration Model Loss : 7.032074790913612e-05\n","Exploitation Data q-values : -12.569515228271484\n","Exploitation OOD q-values : -11.017898559570312\n","Exploitation CQL Loss : 1.5516159534454346\n","Eval_AverageReturn : -44.59090805053711\n","Eval_StdReturn : 12.893476486206055\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -84.0\n","Eval_AverageEpLen : 45.59090909090909\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 865.871389\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 865.8713893890381\n","Exploitation Critic Loss : 1.0353093147277832\n","Exploration Critic Loss : 2.793701171875\n","Exploration Model Loss : 1.3050796042080037e-05\n","Exploitation Data q-values : -12.544075012207031\n","Exploitation OOD q-values : -10.924230575561523\n","Exploitation CQL Loss : 1.61984384059906\n","Eval_AverageReturn : -33.13333511352539\n","Eval_StdReturn : 11.735227584838867\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -62.0\n","Eval_AverageEpLen : 34.13333333333333\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 898.970995\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 898.9709947109222\n","Exploitation Critic Loss : 1.2057762145996094\n","Exploration Critic Loss : 3.490048885345459\n","Exploration Model Loss : 6.876127736177295e-05\n","Exploitation Data q-values : -12.551132202148438\n","Exploitation OOD q-values : -10.900230407714844\n","Exploitation CQL Loss : 1.6509015560150146\n","Eval_AverageReturn : -32.599998474121094\n","Eval_StdReturn : 8.96883487701416\n","Eval_MaxReturn : -20.0\n","Eval_MinReturn : -51.0\n","Eval_AverageEpLen : 33.6\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 933.492167\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 933.4921669960022\n","Exploitation Critic Loss : 0.25224411487579346\n","Exploration Critic Loss : 4.351875305175781\n","Exploration Model Loss : 6.223235686775297e-05\n","Exploitation Data q-values : -12.39006233215332\n","Exploitation OOD q-values : -10.788158416748047\n","Exploitation CQL Loss : 1.6019045114517212\n","Eval_AverageReturn : -36.37036895751953\n","Eval_StdReturn : 8.83657455444336\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -56.0\n","Eval_AverageEpLen : 37.370370370370374\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 964.705456\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 964.7054555416107\n","Exploitation Critic Loss : 2.1831414699554443\n","Exploration Critic Loss : 4.254273414611816\n","Exploration Model Loss : 1.5500811059609987e-05\n","Exploitation Data q-values : -12.549946784973145\n","Exploitation OOD q-values : -10.94970703125\n","Exploitation CQL Loss : 1.600239872932434\n","Eval_AverageReturn : -33.75862121582031\n","Eval_StdReturn : 9.831880569458008\n","Eval_MaxReturn : -19.0\n","Eval_MinReturn : -60.0\n","Eval_AverageEpLen : 34.758620689655174\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 998.524422\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 998.5244216918945\n","Exploitation Critic Loss : 0.25961732864379883\n","Exploration Critic Loss : 3.129210948944092\n","Exploration Model Loss : 0.000628128182142973\n","Exploitation Data q-values : -12.441488265991211\n","Exploitation OOD q-values : -10.816868782043457\n","Exploitation CQL Loss : 1.6246198415756226\n","Eval_AverageReturn : -34.10344696044922\n","Eval_StdReturn : 8.647682189941406\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -55.0\n","Eval_AverageEpLen : 35.10344827586207\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 1031.117312\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1031.1173124313354\n","Exploitation Critic Loss : 1.277035117149353\n","Exploration Critic Loss : 4.534503936767578\n","Exploration Model Loss : 1.131432100009988e-06\n","Exploitation Data q-values : -12.358757019042969\n","Exploitation OOD q-values : -10.772777557373047\n","Exploitation CQL Loss : 1.5859805345535278\n","Eval_AverageReturn : -28.14285659790039\n","Eval_StdReturn : 6.8374714851379395\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -49.0\n","Eval_AverageEpLen : 29.142857142857142\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 1065.646945\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1065.646945476532\n","Exploitation Critic Loss : 0.6354968547821045\n","Exploration Critic Loss : 2.893522024154663\n","Exploration Model Loss : 1.7193140138260787e-06\n","Exploitation Data q-values : -12.408496856689453\n","Exploitation OOD q-values : -10.789777755737305\n","Exploitation CQL Loss : 1.6187195777893066\n","Eval_AverageReturn : -33.06666564941406\n","Eval_StdReturn : 13.226069450378418\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -87.0\n","Eval_AverageEpLen : 34.06666666666667\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 1097.314702\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1097.3147015571594\n","Exploitation Critic Loss : 0.27644914388656616\n","Exploration Critic Loss : 2.2997612953186035\n","Exploration Model Loss : 2.772364496195223e-06\n","Exploitation Data q-values : -12.243019104003906\n","Exploitation OOD q-values : -10.622286796569824\n","Exploitation CQL Loss : 1.6207313537597656\n","Eval_AverageReturn : -26.486486434936523\n","Eval_StdReturn : 7.26197624206543\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -49.0\n","Eval_AverageEpLen : 27.486486486486488\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -149.520004\n","best mean reward -147.320007\n","running time 1131.897894\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -149.52000427246094\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1131.8978943824768\n","Exploitation Critic Loss : 0.4416539669036865\n","Exploration Critic Loss : 2.103677272796631\n","Exploration Model Loss : 2.7444775696494617e-06\n","Exploitation Data q-values : -12.299968719482422\n","Exploitation OOD q-values : -10.721710205078125\n","Exploitation CQL Loss : 1.578259825706482\n","Eval_AverageReturn : -25.421052932739258\n","Eval_StdReturn : 4.923795700073242\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -40.0\n","Eval_AverageEpLen : 26.42105263157895\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -149.389999\n","best mean reward -147.320007\n","running time 1167.415943\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -149.38999938964844\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1167.4159429073334\n","Exploitation Critic Loss : 1.0488426685333252\n","Exploration Critic Loss : 2.0575530529022217\n","Exploration Model Loss : 1.1474372740849503e-06\n","Exploitation Data q-values : -12.318904876708984\n","Exploitation OOD q-values : -10.709966659545898\n","Exploitation CQL Loss : 1.6089377403259277\n","Eval_AverageReturn : -32.733333587646484\n","Eval_StdReturn : 11.701661109924316\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -83.0\n","Eval_AverageEpLen : 33.733333333333334\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -149.610001\n","best mean reward -147.320007\n","running time 1199.130269\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -149.61000061035156\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1199.1302690505981\n","Exploitation Critic Loss : 0.24082614481449127\n","Exploration Critic Loss : 0.9433397054672241\n","Exploration Model Loss : 4.871332293987507e-06\n","Exploitation Data q-values : -12.786920547485352\n","Exploitation OOD q-values : -11.196460723876953\n","Exploitation CQL Loss : 1.590459942817688\n","Eval_AverageReturn : -30.0\n","Eval_StdReturn : 10.345544815063477\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -69.0\n","Eval_AverageEpLen : 31.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -149.610001\n","best mean reward -147.320007\n","running time 1233.407655\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -149.61000061035156\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1233.4076552391052\n","Exploitation Critic Loss : 1.8634520769119263\n","Exploration Critic Loss : 1.6098291873931885\n","Exploration Model Loss : 3.1029871934151743e-06\n","Exploitation Data q-values : -12.497936248779297\n","Exploitation OOD q-values : -10.847817420959473\n","Exploitation CQL Loss : 1.6501188278198242\n","Eval_AverageReturn : -26.486486434936523\n","Eval_StdReturn : 4.682466983795166\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -36.0\n","Eval_AverageEpLen : 27.486486486486488\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -149.610001\n","best mean reward -147.320007\n","running time 1269.149886\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -149.61000061035156\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1269.1498856544495\n","Exploitation Critic Loss : 0.28032758831977844\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.650104522705078\n","Exploitation OOD q-values : -11.01937198638916\n","Exploitation CQL Loss : 1.6307317018508911\n","Eval_AverageReturn : -34.44827651977539\n","Eval_StdReturn : 9.37930965423584\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -51.0\n","Eval_AverageEpLen : 35.44827586206897\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -149.610001\n","best mean reward -147.320007\n","running time 1306.148998\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -149.61000061035156\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1306.148997783661\n","Exploitation Critic Loss : 0.7820723056793213\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.8558349609375\n","Exploitation OOD q-values : -11.228373527526855\n","Exploitation CQL Loss : 1.6274616718292236\n","Eval_AverageReturn : -28.441177368164062\n","Eval_StdReturn : 7.224106788635254\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -49.0\n","Eval_AverageEpLen : 29.441176470588236\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -149.610001\n","best mean reward -147.320007\n","running time 1341.349261\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -149.61000061035156\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1341.3492608070374\n","Exploitation Critic Loss : 1.1402230262756348\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.721293449401855\n","Exploitation OOD q-values : -11.113182067871094\n","Exploitation CQL Loss : 1.6081113815307617\n","Eval_AverageReturn : -35.35714340209961\n","Eval_StdReturn : 10.132036209106445\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -55.0\n","Eval_AverageEpLen : 36.357142857142854\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -149.610001\n","best mean reward -147.320007\n","running time 1378.860564\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -149.61000061035156\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1378.8605635166168\n","Exploitation Critic Loss : 0.24099478125572205\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.782251358032227\n","Exploitation OOD q-values : -11.171655654907227\n","Exploitation CQL Loss : 1.6105949878692627\n","Eval_AverageReturn : -30.78125\n","Eval_StdReturn : 9.30300521850586\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -59.0\n","Eval_AverageEpLen : 31.78125\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -149.610001\n","best mean reward -147.320007\n","running time 1416.755791\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -149.61000061035156\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1416.755791425705\n","Exploitation Critic Loss : 0.27829456329345703\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.86117935180664\n","Exploitation OOD q-values : -11.23890209197998\n","Exploitation CQL Loss : 1.622278094291687\n","Eval_AverageReturn : -30.875\n","Eval_StdReturn : 9.102712631225586\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -59.0\n","Eval_AverageEpLen : 31.875\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -149.610001\n","best mean reward -147.320007\n","running time 1451.593427\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -149.61000061035156\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1451.5934274196625\n","Exploitation Critic Loss : 1.669374942779541\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.614774703979492\n","Exploitation OOD q-values : -10.969093322753906\n","Exploitation CQL Loss : 1.6456815004348755\n","Eval_AverageReturn : -25.552631378173828\n","Eval_StdReturn : 3.51461124420166\n","Eval_MaxReturn : -19.0\n","Eval_MinReturn : -33.0\n","Eval_AverageEpLen : 26.55263157894737\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -149.610001\n","best mean reward -147.320007\n","running time 1490.212044\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -149.61000061035156\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1490.2120442390442\n","Exploitation Critic Loss : 0.9283739328384399\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.3148193359375\n","Exploitation OOD q-values : -10.684752464294434\n","Exploitation CQL Loss : 1.6300666332244873\n","Eval_AverageReturn : -28.171428680419922\n","Eval_StdReturn : 5.315668106079102\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -41.0\n","Eval_AverageEpLen : 29.17142857142857\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -149.610001\n","best mean reward -147.320007\n","running time 1526.747829\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -149.61000061035156\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1526.7478287220001\n","Exploitation Critic Loss : 0.49135804176330566\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.572534561157227\n","Exploitation OOD q-values : -10.941804885864258\n","Exploitation CQL Loss : 1.630730152130127\n","Eval_AverageReturn : -24.075000762939453\n","Eval_StdReturn : 4.070549964904785\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -33.0\n","Eval_AverageEpLen : 25.075\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -149.610001\n","best mean reward -147.320007\n","running time 1565.829780\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -149.61000061035156\n","Train_BestReturn : -147.32000732421875\n","TimeSinceStart : 1565.8297798633575\n","Exploitation Critic Loss : 0.6052271127700806\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.5663480758667\n","Exploitation OOD q-values : -10.927743911743164\n","Exploitation CQL Loss : 1.6386040449142456\n","Eval_AverageReturn : -26.1842098236084\n","Eval_StdReturn : 5.495717525482178\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -44.0\n","Eval_AverageEpLen : 27.18421052631579\n","Buffer size : 10001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_expl.py --env_name PointmassMedium-v0 --exp_name q2_cql --use_rnd --unsupervised_exploration --offline_exploitation \\\n"," --cql_alpha=0.1 --seed 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kTH-tXkI-B-"},"outputs":[],"source":["#@markdown You can visualize your runs with tensorboard from within the notebook\n","\n","## requires tensorflow==2.3.0\n","%load_ext tensorboard\n","%tensorboard --logdir /content/cs285_f2020/homework_fall2022/hw5/data/"]},{"cell_type":"markdown","metadata":{"id":"PhI0pXVCCl6J"},"source":["## 2.2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1563183,"status":"ok","timestamp":1669066894332,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"ovNWWYP5Cm2f","outputId":"ef1a17f9-f9a5-403d-c9bb-d79a42670cf5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_numsteps_5000_PointmassMedium-v0_21-11-2022_21-15-33 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_numsteps_5000_PointmassMedium-v0_21-11-2022_21-15-33\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002129\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0021288394927978516\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -140.714279\n","best mean reward -inf\n","running time 6.392205\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -140.7142791748047\n","TimeSinceStart : 6.392204999923706\n","Eval_AverageReturn : -148.0\n","Eval_StdReturn : 4.898979663848877\n","Eval_MaxReturn : -136.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.14285714285714\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -145.000000\n","best mean reward -inf\n","running time 13.254468\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -145.0\n","TimeSinceStart : 13.254467964172363\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -146.750000\n","best mean reward -inf\n","running time 49.512060\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -146.75\n","TimeSinceStart : 49.51206040382385\n","Exploitation Critic Loss : 0.2297380566596985\n","Exploration Critic Loss : 0.1423833668231964\n","Exploration Model Loss : 0.3648727536201477\n","Exploitation Data q-values : -3.8900556564331055\n","Exploitation OOD q-values : -2.2855076789855957\n","Exploitation CQL Loss : 1.6045477390289307\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -147.592590\n","best mean reward -inf\n","running time 78.303973\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -147.59259033203125\n","TimeSinceStart : 78.30397319793701\n","Exploitation Critic Loss : 0.27203431725502014\n","Exploration Critic Loss : 0.2850491404533386\n","Exploration Model Loss : 0.09513766318559647\n","Exploitation Data q-values : -6.002832412719727\n","Exploitation OOD q-values : -4.390895843505859\n","Exploitation CQL Loss : 1.6119362115859985\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -148.030304\n","best mean reward -inf\n","running time 111.000993\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -148.03030395507812\n","TimeSinceStart : 111.00099349021912\n","Exploitation Critic Loss : 0.3760872483253479\n","Exploration Critic Loss : 0.3321129381656647\n","Exploration Model Loss : 0.0032105606514960527\n","Exploitation Data q-values : -7.759191513061523\n","Exploitation OOD q-values : -6.174626350402832\n","Exploitation CQL Loss : 1.5845650434494019\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -148.375000\n","best mean reward -inf\n","running time 144.070112\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -148.375\n","TimeSinceStart : 144.07011151313782\n","Exploitation Critic Loss : 0.8037326335906982\n","Exploration Critic Loss : 0.4506448805332184\n","Exploration Model Loss : 0.0019627774599939585\n","Exploitation Data q-values : -9.8595552444458\n","Exploitation OOD q-values : -8.288045883178711\n","Exploitation CQL Loss : 1.5715088844299316\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -148.617020\n","best mean reward -inf\n","running time 173.451678\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -148.6170196533203\n","TimeSinceStart : 173.45167803764343\n","Exploitation Critic Loss : 0.8775572776794434\n","Exploration Critic Loss : 0.7317288517951965\n","Exploration Model Loss : 0.0010011795675382018\n","Exploitation Data q-values : -11.242387771606445\n","Exploitation OOD q-values : -9.64189338684082\n","Exploitation CQL Loss : 1.600494146347046\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -148.773590\n","best mean reward -inf\n","running time 205.928016\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -148.77359008789062\n","TimeSinceStart : 205.9280161857605\n","Exploitation Critic Loss : 1.2068325281143188\n","Exploration Critic Loss : 0.8571643233299255\n","Exploration Model Loss : 0.0014039238449186087\n","Exploitation Data q-values : -11.774784088134766\n","Exploitation OOD q-values : -10.191520690917969\n","Exploitation CQL Loss : 1.5832639932632446\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -148.916672\n","best mean reward -inf\n","running time 239.423143\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -148.9166717529297\n","TimeSinceStart : 239.42314267158508\n","Exploitation Critic Loss : 1.6115612983703613\n","Exploration Critic Loss : 1.0391597747802734\n","Exploration Model Loss : 0.0005980668938718736\n","Exploitation Data q-values : -12.373598098754883\n","Exploitation OOD q-values : -10.755745887756348\n","Exploitation CQL Loss : 1.6178518533706665\n","Eval_AverageReturn : -147.2857208251953\n","Eval_StdReturn : 6.648614883422852\n","Eval_MaxReturn : -131.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 147.42857142857142\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -149.029846\n","best mean reward -inf\n","running time 268.135557\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -149.02984619140625\n","TimeSinceStart : 268.1355571746826\n","Exploitation Critic Loss : 1.3082106113433838\n","Exploration Critic Loss : 0.9990173578262329\n","Exploration Model Loss : 0.0008507031016051769\n","Exploitation Data q-values : -12.702165603637695\n","Exploitation OOD q-values : -11.063776016235352\n","Exploitation CQL Loss : 1.638388991355896\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -149.109589\n","best mean reward -inf\n","running time 299.730794\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -149.10958862304688\n","TimeSinceStart : 299.7307941913605\n","Exploitation Critic Loss : 1.771270990371704\n","Exploration Critic Loss : 0.816008448600769\n","Exploration Model Loss : 0.00035072106402367353\n","Exploitation Data q-values : -12.716632843017578\n","Exploitation OOD q-values : -11.045780181884766\n","Exploitation CQL Loss : 1.6708515882492065\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -149.187500\n","best mean reward -inf\n","running time 328.287625\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -149.1875\n","TimeSinceStart : 328.2876250743866\n","Exploitation Critic Loss : 2.705702543258667\n","Exploration Critic Loss : 0.6816109418869019\n","Exploration Model Loss : 0.0006695007905364037\n","Exploitation Data q-values : -13.31459903717041\n","Exploitation OOD q-values : -11.660955429077148\n","Exploitation CQL Loss : 1.6536444425582886\n","Eval_AverageReturn : -147.85714721679688\n","Eval_StdReturn : 5.248906135559082\n","Eval_MaxReturn : -135.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -148.574707\n","best mean reward -inf\n","running time 359.484830\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -148.57470703125\n","TimeSinceStart : 359.48482966423035\n","Exploitation Critic Loss : 1.7787837982177734\n","Exploration Critic Loss : 0.7773830890655518\n","Exploration Model Loss : 0.00010860501060960814\n","Exploitation Data q-values : -13.157524108886719\n","Exploitation OOD q-values : -11.458431243896484\n","Exploitation CQL Loss : 1.6990931034088135\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -148.680847\n","best mean reward -inf\n","running time 390.698781\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -148.68084716796875\n","TimeSinceStart : 390.69878101348877\n","Exploitation Critic Loss : 0.812477707862854\n","Exploration Critic Loss : 1.1317471265792847\n","Exploration Model Loss : 0.00015297446225304157\n","Exploitation Data q-values : -13.162759780883789\n","Exploitation OOD q-values : -11.412164688110352\n","Exploitation CQL Loss : 1.7505955696105957\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -148.759995\n","best mean reward -inf\n","running time 422.643029\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -148.75999450683594\n","TimeSinceStart : 422.64302921295166\n","Exploitation Critic Loss : 0.9607365131378174\n","Exploration Critic Loss : 0.5657747387886047\n","Exploration Model Loss : 0.00017748495156411082\n","Exploitation Data q-values : -12.546915054321289\n","Exploitation OOD q-values : -10.735939979553223\n","Exploitation CQL Loss : 1.8109737634658813\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -149.410004\n","best mean reward -149.410004\n","running time 454.236159\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -149.41000366210938\n","Train_BestReturn : -149.41000366210938\n","TimeSinceStart : 454.23615860939026\n","Exploitation Critic Loss : 3.4235966205596924\n","Exploration Critic Loss : 0.9583408832550049\n","Exploration Model Loss : 2.537075306463521e-05\n","Exploitation Data q-values : -11.679557800292969\n","Exploitation OOD q-values : -9.897680282592773\n","Exploitation CQL Loss : 1.7818775177001953\n","Eval_AverageReturn : -145.7142791748047\n","Eval_StdReturn : 10.497814178466797\n","Eval_MaxReturn : -120.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 145.85714285714286\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -149.410004\n","best mean reward -149.410004\n","running time 485.667692\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -149.41000366210938\n","Train_BestReturn : -149.41000366210938\n","TimeSinceStart : 485.6676924228668\n","Exploitation Critic Loss : 2.2202680110931396\n","Exploration Critic Loss : 0.9045089483261108\n","Exploration Model Loss : 1.8991888282471336e-05\n","Exploitation Data q-values : -9.981964111328125\n","Exploitation OOD q-values : -8.007037162780762\n","Exploitation CQL Loss : 1.974926471710205\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -149.410004\n","best mean reward -149.410004\n","running time 514.163813\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -149.41000366210938\n","Train_BestReturn : -149.41000366210938\n","TimeSinceStart : 514.1638128757477\n","Exploitation Critic Loss : 0.6081119775772095\n","Exploration Critic Loss : 0.6287292838096619\n","Exploration Model Loss : 2.2687268938170746e-05\n","Exploitation Data q-values : -9.316902160644531\n","Exploitation OOD q-values : -7.388979434967041\n","Exploitation CQL Loss : 1.9279232025146484\n","Eval_AverageReturn : -112.88888549804688\n","Eval_StdReturn : 42.21754455566406\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 113.55555555555556\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -149.410004\n","best mean reward -149.410004\n","running time 546.950758\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -149.41000366210938\n","Train_BestReturn : -149.41000366210938\n","TimeSinceStart : 546.9507579803467\n","Exploitation Critic Loss : 1.0716811418533325\n","Exploration Critic Loss : 0.848533570766449\n","Exploration Model Loss : 0.0002103836741298437\n","Exploitation Data q-values : -8.011157035827637\n","Exploitation OOD q-values : -6.128135681152344\n","Exploitation CQL Loss : 1.8830218315124512\n","Eval_AverageReturn : -113.0\n","Eval_StdReturn : 38.20994567871094\n","Eval_MaxReturn : -54.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 113.55555555555556\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -149.410004\n","best mean reward -149.410004\n","running time 578.280819\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -149.41000366210938\n","Train_BestReturn : -149.41000366210938\n","TimeSinceStart : 578.2808187007904\n","Exploitation Critic Loss : 0.6334630250930786\n","Exploration Critic Loss : 0.6948798894882202\n","Exploration Model Loss : 9.568698442308232e-05\n","Exploitation Data q-values : -5.936617851257324\n","Exploitation OOD q-values : -3.9522500038146973\n","Exploitation CQL Loss : 1.9843685626983643\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -149.410004\n","best mean reward -149.410004\n","running time 611.144433\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -149.41000366210938\n","Train_BestReturn : -149.41000366210938\n","TimeSinceStart : 611.1444334983826\n","Exploitation Critic Loss : 0.5446941256523132\n","Exploration Critic Loss : 0.6166135668754578\n","Exploration Model Loss : 4.8500834964215755e-05\n","Exploitation Data q-values : -5.452729225158691\n","Exploitation OOD q-values : -3.5439910888671875\n","Exploitation CQL Loss : 1.9087376594543457\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -149.410004\n","best mean reward -149.410004\n","running time 643.577788\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -149.41000366210938\n","Train_BestReturn : -149.41000366210938\n","TimeSinceStart : 643.5777883529663\n","Exploitation Critic Loss : 0.37955522537231445\n","Exploration Critic Loss : 0.7754549384117126\n","Exploration Model Loss : 0.0001315938716288656\n","Exploitation Data q-values : -5.278009414672852\n","Exploitation OOD q-values : -3.4947640895843506\n","Exploitation CQL Loss : 1.783245325088501\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -149.410004\n","best mean reward -149.410004\n","running time 675.488302\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -149.41000366210938\n","Train_BestReturn : -149.41000366210938\n","TimeSinceStart : 675.4883015155792\n","Exploitation Critic Loss : 0.5871173143386841\n","Exploration Critic Loss : 1.1314070224761963\n","Exploration Model Loss : 3.127865420538001e-05\n","Exploitation Data q-values : -5.234082221984863\n","Exploitation OOD q-values : -3.520864963531494\n","Exploitation CQL Loss : 1.7132172584533691\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -149.410004\n","best mean reward -149.410004\n","running time 706.021125\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -149.41000366210938\n","Train_BestReturn : -149.41000366210938\n","TimeSinceStart : 706.0211250782013\n","Exploitation Critic Loss : 0.4179668128490448\n","Exploration Critic Loss : 0.7956172227859497\n","Exploration Model Loss : 6.405093154171482e-05\n","Exploitation Data q-values : -6.545706748962402\n","Exploitation OOD q-values : -4.9029107093811035\n","Exploitation CQL Loss : 1.6427958011627197\n","Eval_AverageReturn : -124.88888549804688\n","Eval_StdReturn : 33.22463607788086\n","Eval_MaxReturn : -43.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 125.44444444444444\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -149.410004\n","best mean reward -149.410004\n","running time 738.324395\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -149.41000366210938\n","Train_BestReturn : -149.41000366210938\n","TimeSinceStart : 738.3243947029114\n","Exploitation Critic Loss : 0.7841416597366333\n","Exploration Critic Loss : 0.8721188306808472\n","Exploration Model Loss : 5.418085493147373e-05\n","Exploitation Data q-values : -8.032941818237305\n","Exploitation OOD q-values : -6.423667907714844\n","Exploitation CQL Loss : 1.6092742681503296\n","Eval_AverageReturn : -117.0\n","Eval_StdReturn : 30.87249755859375\n","Eval_MaxReturn : -64.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 117.66666666666667\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -148.809998\n","best mean reward -148.809998\n","running time 768.577134\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -148.80999755859375\n","Train_BestReturn : -148.80999755859375\n","TimeSinceStart : 768.5771336555481\n","Exploitation Critic Loss : 0.7233790755271912\n","Exploration Critic Loss : 0.7256098985671997\n","Exploration Model Loss : 7.390339305857196e-05\n","Exploitation Data q-values : -9.306310653686523\n","Exploitation OOD q-values : -7.686375617980957\n","Exploitation CQL Loss : 1.619934320449829\n","Eval_AverageReturn : -144.7142791748047\n","Eval_StdReturn : 12.94730281829834\n","Eval_MaxReturn : -113.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 144.85714285714286\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -148.429993\n","best mean reward -148.429993\n","running time 802.941843\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -148.42999267578125\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 802.9418430328369\n","Exploitation Critic Loss : 0.5733332633972168\n","Exploration Critic Loss : 0.6545313000679016\n","Exploration Model Loss : 1.4278484741225839e-05\n","Exploitation Data q-values : -10.89913558959961\n","Exploitation OOD q-values : -9.26980209350586\n","Exploitation CQL Loss : 1.629333734512329\n","Eval_AverageReturn : -90.2727279663086\n","Eval_StdReturn : 36.683143615722656\n","Eval_MaxReturn : -36.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 91.18181818181819\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -148.610001\n","best mean reward -148.429993\n","running time 834.768221\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -148.61000061035156\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 834.7682206630707\n","Exploitation Critic Loss : 0.2041482925415039\n","Exploration Critic Loss : 0.4469870328903198\n","Exploration Model Loss : 6.157093821457238e-07\n","Exploitation Data q-values : -11.503805160522461\n","Exploitation OOD q-values : -9.878044128417969\n","Exploitation CQL Loss : 1.6257603168487549\n","Eval_AverageReturn : -82.58333587646484\n","Eval_StdReturn : 33.70078659057617\n","Eval_MaxReturn : -42.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 83.5\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -148.610001\n","best mean reward -148.429993\n","running time 865.820795\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -148.61000061035156\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 865.820794582367\n","Exploitation Critic Loss : 2.925649881362915\n","Exploration Critic Loss : 1.0818095207214355\n","Exploration Model Loss : 8.515958325006068e-05\n","Exploitation Data q-values : -12.118252754211426\n","Exploitation OOD q-values : -10.479053497314453\n","Exploitation CQL Loss : 1.639199137687683\n","Eval_AverageReturn : -110.9000015258789\n","Eval_StdReturn : 37.04983139038086\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 111.5\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -148.610001\n","best mean reward -148.429993\n","running time 896.661598\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -148.61000061035156\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 896.6615979671478\n","Exploitation Critic Loss : 1.9085640907287598\n","Exploration Critic Loss : 0.7138277292251587\n","Exploration Model Loss : 3.4051100783472066e-07\n","Exploitation Data q-values : -12.65588665008545\n","Exploitation OOD q-values : -11.071463584899902\n","Exploitation CQL Loss : 1.5844236612319946\n","Eval_AverageReturn : -109.69999694824219\n","Eval_StdReturn : 38.29634475708008\n","Eval_MaxReturn : -53.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 110.3\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -148.610001\n","best mean reward -148.429993\n","running time 928.947899\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -148.61000061035156\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 928.9478986263275\n","Exploitation Critic Loss : 1.4447836875915527\n","Exploration Critic Loss : 0.7490836977958679\n","Exploration Model Loss : 9.990084799937904e-05\n","Exploitation Data q-values : -13.034116744995117\n","Exploitation OOD q-values : -11.41797924041748\n","Exploitation CQL Loss : 1.6161376237869263\n","Eval_AverageReturn : -85.33333587646484\n","Eval_StdReturn : 31.43865394592285\n","Eval_MaxReturn : -51.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 86.16666666666667\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -148.610001\n","best mean reward -148.429993\n","running time 959.041084\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -148.61000061035156\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 959.0410840511322\n","Exploitation Critic Loss : 0.20805206894874573\n","Exploration Critic Loss : 0.4751289188861847\n","Exploration Model Loss : 1.1694705790432636e-05\n","Exploitation Data q-values : -13.566268920898438\n","Exploitation OOD q-values : -11.930841445922852\n","Exploitation CQL Loss : 1.6354267597198486\n","Eval_AverageReturn : -114.33333587646484\n","Eval_StdReturn : 30.364452362060547\n","Eval_MaxReturn : -52.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 115.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -148.610001\n","best mean reward -148.429993\n","running time 989.307471\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -148.61000061035156\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 989.3074705600739\n","Exploitation Critic Loss : 0.2017320990562439\n","Exploration Critic Loss : 0.4274812936782837\n","Exploration Model Loss : 3.0293233066913672e-05\n","Exploitation Data q-values : -13.779565811157227\n","Exploitation OOD q-values : -12.162666320800781\n","Exploitation CQL Loss : 1.6169006824493408\n","Eval_AverageReturn : -83.16666412353516\n","Eval_StdReturn : 33.77334976196289\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -136.0\n","Eval_AverageEpLen : 84.16666666666667\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -148.589996\n","best mean reward -148.429993\n","running time 1025.320837\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -148.58999633789062\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1025.3208374977112\n","Exploitation Critic Loss : 1.7013922929763794\n","Exploration Critic Loss : 0.5953872799873352\n","Exploration Model Loss : 2.285411574121099e-05\n","Exploitation Data q-values : -13.979076385498047\n","Exploitation OOD q-values : -12.337181091308594\n","Exploitation CQL Loss : 1.6418957710266113\n","Eval_AverageReturn : -81.0\n","Eval_StdReturn : 23.5012264251709\n","Eval_MaxReturn : -39.0\n","Eval_MinReturn : -123.0\n","Eval_AverageEpLen : 82.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -148.589996\n","best mean reward -148.429993\n","running time 1058.261583\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -148.58999633789062\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1058.261583328247\n","Exploitation Critic Loss : 0.22520340979099274\n","Exploration Critic Loss : 0.48853030800819397\n","Exploration Model Loss : 1.8666451069293544e-06\n","Exploitation Data q-values : -13.904440879821777\n","Exploitation OOD q-values : -12.285385131835938\n","Exploitation CQL Loss : 1.6190557479858398\n","Eval_AverageReturn : -63.5\n","Eval_StdReturn : 24.05982208251953\n","Eval_MaxReturn : -30.0\n","Eval_MinReturn : -125.0\n","Eval_AverageEpLen : 64.5\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -148.589996\n","best mean reward -148.429993\n","running time 1088.122718\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -148.58999633789062\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1088.122718334198\n","Exploitation Critic Loss : 3.3422625064849854\n","Exploration Critic Loss : 0.7846102118492126\n","Exploration Model Loss : 4.867038660449907e-05\n","Exploitation Data q-values : -13.856184005737305\n","Exploitation OOD q-values : -12.184854507446289\n","Exploitation CQL Loss : 1.6713298559188843\n","Eval_AverageReturn : -82.66666412353516\n","Eval_StdReturn : 17.41327667236328\n","Eval_MaxReturn : -62.0\n","Eval_MinReturn : -124.0\n","Eval_AverageEpLen : 83.66666666666667\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -148.449997\n","best mean reward -148.429993\n","running time 1120.961184\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -148.4499969482422\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1120.9611835479736\n","Exploitation Critic Loss : 0.9166598916053772\n","Exploration Critic Loss : 0.4855247437953949\n","Exploration Model Loss : 3.4484529010114784e-07\n","Exploitation Data q-values : -13.644563674926758\n","Exploitation OOD q-values : -12.005054473876953\n","Exploitation CQL Loss : 1.6395093202590942\n","Eval_AverageReturn : -49.349998474121094\n","Eval_StdReturn : 15.919405937194824\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -80.0\n","Eval_AverageEpLen : 50.35\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -148.449997\n","best mean reward -148.429993\n","running time 1156.203022\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -148.4499969482422\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1156.2030215263367\n","Exploitation Critic Loss : 0.9485392570495605\n","Exploration Critic Loss : 0.44721418619155884\n","Exploration Model Loss : 5.595935022029153e-07\n","Exploitation Data q-values : -13.772383689880371\n","Exploitation OOD q-values : -12.14450740814209\n","Exploitation CQL Loss : 1.6278762817382812\n","Eval_AverageReturn : -52.73684310913086\n","Eval_StdReturn : 17.371610641479492\n","Eval_MaxReturn : -27.0\n","Eval_MinReturn : -96.0\n","Eval_AverageEpLen : 53.73684210526316\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -148.449997\n","best mean reward -148.429993\n","running time 1186.771291\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -148.4499969482422\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1186.771291255951\n","Exploitation Critic Loss : 0.9298276305198669\n","Exploration Critic Loss : 0.40902256965637207\n","Exploration Model Loss : 1.6721189240342937e-05\n","Exploitation Data q-values : -13.670804977416992\n","Exploitation OOD q-values : -12.01824951171875\n","Exploitation CQL Loss : 1.6525555849075317\n","Eval_AverageReturn : -68.80000305175781\n","Eval_StdReturn : 22.93817710876465\n","Eval_MaxReturn : -32.0\n","Eval_MinReturn : -122.0\n","Eval_AverageEpLen : 69.8\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -148.449997\n","best mean reward -148.429993\n","running time 1222.485067\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -148.4499969482422\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1222.485066652298\n","Exploitation Critic Loss : 1.806369423866272\n","Exploration Critic Loss : 0.6207772493362427\n","Exploration Model Loss : 3.8975013012532145e-05\n","Exploitation Data q-values : -13.752164840698242\n","Exploitation OOD q-values : -12.101451873779297\n","Exploitation CQL Loss : 1.6507136821746826\n","Eval_AverageReturn : -53.894737243652344\n","Eval_StdReturn : 15.26569938659668\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -86.0\n","Eval_AverageEpLen : 54.89473684210526\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -149.050003\n","best mean reward -148.429993\n","running time 1258.364361\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -149.0500030517578\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1258.3643610477448\n","Exploitation Critic Loss : 0.7859241962432861\n","Exploration Critic Loss : 0.2706509828567505\n","Exploration Model Loss : 0.00010387687507318333\n","Exploitation Data q-values : -13.593324661254883\n","Exploitation OOD q-values : -11.9520263671875\n","Exploitation CQL Loss : 1.6412968635559082\n","Eval_AverageReturn : -49.25\n","Eval_StdReturn : 13.783595085144043\n","Eval_MaxReturn : -30.0\n","Eval_MinReturn : -80.0\n","Eval_AverageEpLen : 50.25\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -149.429993\n","best mean reward -148.429993\n","running time 1292.266672\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -149.42999267578125\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1292.2666718959808\n","Exploitation Critic Loss : 2.116420269012451\n","Exploration Critic Loss : 0.6659662127494812\n","Exploration Model Loss : 0.0010076324688270688\n","Exploitation Data q-values : -13.656591415405273\n","Exploitation OOD q-values : -12.01405143737793\n","Exploitation CQL Loss : 1.6425410509109497\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 20.770170211791992\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -101.0\n","Eval_AverageEpLen : 51.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -149.839996\n","best mean reward -148.429993\n","running time 1322.958587\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -149.83999633789062\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1322.95858669281\n","Exploitation Critic Loss : 1.5015044212341309\n","Exploration Critic Loss : 0.44840872287750244\n","Exploration Model Loss : 5.025057907914743e-05\n","Exploitation Data q-values : -13.467240333557129\n","Exploitation OOD q-values : -11.820706367492676\n","Exploitation CQL Loss : 1.6465343236923218\n","Eval_AverageReturn : -50.25\n","Eval_StdReturn : 22.40507698059082\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -129.0\n","Eval_AverageEpLen : 51.25\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -149.080002\n","best mean reward -148.429993\n","running time 1356.336110\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -149.0800018310547\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1356.3361103534698\n","Exploitation Critic Loss : 0.2492671012878418\n","Exploration Critic Loss : 0.360331654548645\n","Exploration Model Loss : 0.00031189530272968113\n","Exploitation Data q-values : -14.035331726074219\n","Exploitation OOD q-values : -12.376823425292969\n","Exploitation CQL Loss : 1.6585075855255127\n","Eval_AverageReturn : -43.4782600402832\n","Eval_StdReturn : 16.774383544921875\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -93.0\n","Eval_AverageEpLen : 44.47826086956522\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -149.080002\n","best mean reward -148.429993\n","running time 1391.192061\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -149.0800018310547\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1391.1920611858368\n","Exploitation Critic Loss : 0.9204407334327698\n","Exploration Critic Loss : 0.28210002183914185\n","Exploration Model Loss : 0.00026297022122889757\n","Exploitation Data q-values : -13.67558479309082\n","Exploitation OOD q-values : -12.019449234008789\n","Exploitation CQL Loss : 1.6561360359191895\n","Eval_AverageReturn : -42.75\n","Eval_StdReturn : 14.17524242401123\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -91.0\n","Eval_AverageEpLen : 43.75\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -149.080002\n","best mean reward -148.429993\n","running time 1425.054486\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -149.0800018310547\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1425.0544855594635\n","Exploitation Critic Loss : 2.417588710784912\n","Exploration Critic Loss : 0.43460217118263245\n","Exploration Model Loss : 0.0001064670505002141\n","Exploitation Data q-values : -13.61174201965332\n","Exploitation OOD q-values : -11.95859146118164\n","Exploitation CQL Loss : 1.6531503200531006\n","Eval_AverageReturn : -40.68000030517578\n","Eval_StdReturn : 15.994298934936523\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -83.0\n","Eval_AverageEpLen : 41.68\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -149.080002\n","best mean reward -148.429993\n","running time 1459.165523\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -149.0800018310547\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1459.165522813797\n","Exploitation Critic Loss : 0.9784286618232727\n","Exploration Critic Loss : 0.4604423940181732\n","Exploration Model Loss : 1.1708834790624678e-08\n","Exploitation Data q-values : -13.462526321411133\n","Exploitation OOD q-values : -11.795463562011719\n","Exploitation CQL Loss : 1.667062759399414\n","Eval_AverageReturn : -44.59090805053711\n","Eval_StdReturn : 20.053699493408203\n","Eval_MaxReturn : -19.0\n","Eval_MinReturn : -106.0\n","Eval_AverageEpLen : 45.59090909090909\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -149.100006\n","best mean reward -148.429993\n","running time 1493.610945\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -149.10000610351562\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1493.610945224762\n","Exploitation Critic Loss : 0.7670137286186218\n","Exploration Critic Loss : 0.1395530253648758\n","Exploration Model Loss : 2.5504765289952047e-05\n","Exploitation Data q-values : -13.270004272460938\n","Exploitation OOD q-values : -11.603669166564941\n","Exploitation CQL Loss : 1.6663342714309692\n","Eval_AverageReturn : -39.42307662963867\n","Eval_StdReturn : 12.524354934692383\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -72.0\n","Eval_AverageEpLen : 40.42307692307692\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -149.100006\n","best mean reward -148.429993\n","running time 1524.939639\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -149.10000610351562\n","Train_BestReturn : -148.42999267578125\n","TimeSinceStart : 1524.9396390914917\n","Exploitation Critic Loss : 0.9314976334571838\n","Exploration Critic Loss : 0.1844187080860138\n","Exploration Model Loss : 9.218270133715123e-05\n","Exploitation Data q-values : -13.123851776123047\n","Exploitation OOD q-values : -11.483503341674805\n","Exploitation CQL Loss : 1.6403477191925049\n","Eval_AverageReturn : -35.53571319580078\n","Eval_StdReturn : 10.857789039611816\n","Eval_MaxReturn : -20.0\n","Eval_MinReturn : -66.0\n","Eval_AverageEpLen : 36.535714285714285\n","Buffer size : 5001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_expl.py --env_name PointmassMedium-v0 --use_rnd \\\n"," --num_exploration_steps=5000 --offline_exploitation --cql_alpha=0.1 \\\n"," --unsupervised_exploration --exp_name q2_cql_numsteps_5000 --seed 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1610834,"status":"ok","timestamp":1669068579345,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"bddtVofbC3up","outputId":"c73691b8-efdd-49ab-9421-67adf4f8c52f"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_numsteps_15000_PointmassMedium-v0_21-11-2022_21-42-50 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_cql_numsteps_15000_PointmassMedium-v0_21-11-2022_21-42-50\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002040\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.002040386199951172\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -141.571426\n","best mean reward -inf\n","running time 6.014273\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -141.57142639160156\n","TimeSinceStart : 6.014273405075073\n","Eval_AverageReturn : -148.0\n","Eval_StdReturn : 4.898979663848877\n","Eval_MaxReturn : -136.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.14285714285714\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -142.500000\n","best mean reward -inf\n","running time 12.554984\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -142.5\n","TimeSinceStart : 12.554983615875244\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -144.750000\n","best mean reward -inf\n","running time 43.489393\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -144.75\n","TimeSinceStart : 43.48939347267151\n","Exploitation Critic Loss : 0.1884966790676117\n","Exploration Critic Loss : 0.27158308029174805\n","Exploration Model Loss : 0.1869012862443924\n","Exploitation Data q-values : -3.6734120845794678\n","Exploitation OOD q-values : -2.0809733867645264\n","Exploitation CQL Loss : 1.5924385786056519\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -146.111115\n","best mean reward -inf\n","running time 77.734349\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -146.11111450195312\n","TimeSinceStart : 77.73434948921204\n","Exploitation Critic Loss : 0.33993226289749146\n","Exploration Critic Loss : 0.2539540231227875\n","Exploration Model Loss : 0.002306346781551838\n","Exploitation Data q-values : -5.730076313018799\n","Exploitation OOD q-values : -4.1428961753845215\n","Exploitation CQL Loss : 1.5871803760528564\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -146.911758\n","best mean reward -inf\n","running time 114.194854\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -146.91175842285156\n","TimeSinceStart : 114.19485449790955\n","Exploitation Critic Loss : 0.8200626373291016\n","Exploration Critic Loss : 0.3670865595340729\n","Exploration Model Loss : 0.04817800223827362\n","Exploitation Data q-values : -7.495506286621094\n","Exploitation OOD q-values : -5.932415008544922\n","Exploitation CQL Loss : 1.5630912780761719\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -147.375000\n","best mean reward -inf\n","running time 149.980060\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -147.375\n","TimeSinceStart : 149.980060338974\n","Exploitation Critic Loss : 0.9588668346405029\n","Exploration Critic Loss : 0.5537874698638916\n","Exploration Model Loss : 0.0014229535590857267\n","Exploitation Data q-values : -9.16305160522461\n","Exploitation OOD q-values : -7.558447360992432\n","Exploitation CQL Loss : 1.6046040058135986\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -147.765961\n","best mean reward -inf\n","running time 183.422877\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -147.76596069335938\n","TimeSinceStart : 183.4228765964508\n","Exploitation Critic Loss : 0.8364457488059998\n","Exploration Critic Loss : 0.7050160765647888\n","Exploration Model Loss : 0.0010092704324051738\n","Exploitation Data q-values : -10.170112609863281\n","Exploitation OOD q-values : -8.567179679870605\n","Exploitation CQL Loss : 1.6029325723648071\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -148.055557\n","best mean reward -inf\n","running time 213.297157\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -148.05555725097656\n","TimeSinceStart : 213.2971568107605\n","Exploitation Critic Loss : 0.2462160587310791\n","Exploration Critic Loss : 0.8496770858764648\n","Exploration Model Loss : 0.007663615979254246\n","Exploitation Data q-values : -10.843446731567383\n","Exploitation OOD q-values : -9.247694969177246\n","Exploitation CQL Loss : 1.5957515239715576\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -148.250000\n","best mean reward -inf\n","running time 245.520664\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -148.25\n","TimeSinceStart : 245.52066445350647\n","Exploitation Critic Loss : 0.21937154233455658\n","Exploration Critic Loss : 0.4842326045036316\n","Exploration Model Loss : 0.0004758053692057729\n","Exploitation Data q-values : -11.404108047485352\n","Exploitation OOD q-values : -9.817131042480469\n","Exploitation CQL Loss : 1.586978793144226\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -148.432831\n","best mean reward -inf\n","running time 276.411287\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -148.43283081054688\n","TimeSinceStart : 276.4112865924835\n","Exploitation Critic Loss : 1.41029691696167\n","Exploration Critic Loss : 0.650492250919342\n","Exploration Model Loss : 0.0069334679283201694\n","Exploitation Data q-values : -11.90485954284668\n","Exploitation OOD q-values : -10.299537658691406\n","Exploitation CQL Loss : 1.6053224802017212\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -148.081085\n","best mean reward -inf\n","running time 306.372262\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -148.08108520507812\n","TimeSinceStart : 306.372261762619\n","Exploitation Critic Loss : 0.7949114441871643\n","Exploration Critic Loss : 0.4738682508468628\n","Exploration Model Loss : 0.00022916974558029324\n","Exploitation Data q-values : -12.24428939819336\n","Exploitation OOD q-values : -10.630228042602539\n","Exploitation CQL Loss : 1.6140607595443726\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -142.035721\n","best mean reward -inf\n","running time 341.606682\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -142.0357208251953\n","TimeSinceStart : 341.60668206214905\n","Exploitation Critic Loss : 1.017610788345337\n","Exploration Critic Loss : 0.5003036260604858\n","Exploration Model Loss : 0.00042934788507409394\n","Exploitation Data q-values : -12.260490417480469\n","Exploitation OOD q-values : -10.686254501342773\n","Exploitation CQL Loss : 1.5742355585098267\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -137.297867\n","best mean reward -inf\n","running time 373.759038\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -137.29786682128906\n","TimeSinceStart : 373.759037733078\n","Exploitation Critic Loss : 0.8420913219451904\n","Exploration Critic Loss : 0.5319982171058655\n","Exploration Model Loss : 0.00016802850586827844\n","Exploitation Data q-values : -12.236296653747559\n","Exploitation OOD q-values : -10.669330596923828\n","Exploitation CQL Loss : 1.5669664144515991\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -131.919998\n","best mean reward -131.919998\n","running time 403.611635\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -131.9199981689453\n","Train_BestReturn : -131.9199981689453\n","TimeSinceStart : 403.6116352081299\n","Exploitation Critic Loss : 1.0499613285064697\n","Exploration Critic Loss : 0.7130546569824219\n","Exploration Model Loss : 9.875222167465836e-05\n","Exploitation Data q-values : -11.515036582946777\n","Exploitation OOD q-values : -9.90658950805664\n","Exploitation CQL Loss : 1.6084481477737427\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -130.990005\n","best mean reward -130.990005\n","running time 436.595953\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -130.99000549316406\n","Train_BestReturn : -130.99000549316406\n","TimeSinceStart : 436.59595346450806\n","Exploitation Critic Loss : 2.2174856662750244\n","Exploration Critic Loss : 0.6316413283348083\n","Exploration Model Loss : 0.00010296850086888298\n","Exploitation Data q-values : -11.760455131530762\n","Exploitation OOD q-values : -10.189223289489746\n","Exploitation CQL Loss : 1.5712318420410156\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -130.990005\n","best mean reward -130.990005\n","running time 467.697507\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -130.99000549316406\n","Train_BestReturn : -130.99000549316406\n","TimeSinceStart : 467.6975066661835\n","Exploitation Critic Loss : 2.052933692932129\n","Exploration Critic Loss : 0.5927832126617432\n","Exploration Model Loss : 2.871163815143518e-05\n","Exploitation Data q-values : -12.284087181091309\n","Exploitation OOD q-values : -10.691969871520996\n","Exploitation CQL Loss : 1.5921173095703125\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -130.679993\n","best mean reward -130.679993\n","running time 497.353473\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -130.67999267578125\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 497.3534734249115\n","Exploitation Critic Loss : 0.2887861132621765\n","Exploration Critic Loss : 0.6689860820770264\n","Exploration Model Loss : 8.016341598704457e-05\n","Exploitation Data q-values : -12.405427932739258\n","Exploitation OOD q-values : -10.800851821899414\n","Exploitation CQL Loss : 1.6045761108398438\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -130.679993\n","best mean reward -130.679993\n","running time 531.613429\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -130.67999267578125\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 531.613429069519\n","Exploitation Critic Loss : 0.2750856280326843\n","Exploration Critic Loss : 0.6078544855117798\n","Exploration Model Loss : 1.5065260413393844e-05\n","Exploitation Data q-values : -12.303857803344727\n","Exploitation OOD q-values : -10.712934494018555\n","Exploitation CQL Loss : 1.590923547744751\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -130.679993\n","best mean reward -130.679993\n","running time 563.435199\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -130.67999267578125\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 563.4351987838745\n","Exploitation Critic Loss : 1.666790246963501\n","Exploration Critic Loss : 0.7974313497543335\n","Exploration Model Loss : 0.00047410064144060016\n","Exploitation Data q-values : -12.561605453491211\n","Exploitation OOD q-values : -10.984245300292969\n","Exploitation CQL Loss : 1.5773603916168213\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -130.679993\n","best mean reward -130.679993\n","running time 592.199818\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -130.67999267578125\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 592.1998178958893\n","Exploitation Critic Loss : 0.3582615852355957\n","Exploration Critic Loss : 0.6844549179077148\n","Exploration Model Loss : 4.763107426697388e-05\n","Exploitation Data q-values : -12.57845687866211\n","Exploitation OOD q-values : -10.988893508911133\n","Exploitation CQL Loss : 1.589563012123108\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -130.679993\n","best mean reward -130.679993\n","running time 624.435601\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -130.67999267578125\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 624.435601234436\n","Exploitation Critic Loss : 0.5707880854606628\n","Exploration Critic Loss : 0.6793323159217834\n","Exploration Model Loss : 0.00011445908603491262\n","Exploitation Data q-values : -12.373101234436035\n","Exploitation OOD q-values : -10.772361755371094\n","Exploitation CQL Loss : 1.6007392406463623\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -130.679993\n","best mean reward -130.679993\n","running time 655.715290\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -130.67999267578125\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 655.7152900695801\n","Exploitation Critic Loss : 1.6686317920684814\n","Exploration Critic Loss : 0.6037881970405579\n","Exploration Model Loss : 0.00010516465408727527\n","Exploitation Data q-values : -12.381328582763672\n","Exploitation OOD q-values : -10.784158706665039\n","Exploitation CQL Loss : 1.5971693992614746\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -130.679993\n","best mean reward -130.679993\n","running time 684.218328\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -130.67999267578125\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 684.2183284759521\n","Exploitation Critic Loss : 0.30284273624420166\n","Exploration Critic Loss : 0.5474955439567566\n","Exploration Model Loss : 7.53540443838574e-05\n","Exploitation Data q-values : -12.33761215209961\n","Exploitation OOD q-values : -10.764593124389648\n","Exploitation CQL Loss : 1.5730189085006714\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -131.050003\n","best mean reward -130.679993\n","running time 718.896083\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -131.0500030517578\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 718.8960826396942\n","Exploitation Critic Loss : 1.0617928504943848\n","Exploration Critic Loss : 0.6139636635780334\n","Exploration Model Loss : 0.0003022880118805915\n","Exploitation Data q-values : -12.846664428710938\n","Exploitation OOD q-values : -11.25497055053711\n","Exploitation CQL Loss : 1.5916943550109863\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -133.800003\n","best mean reward -130.679993\n","running time 751.149108\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -133.8000030517578\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 751.1491084098816\n","Exploitation Critic Loss : 0.3156978487968445\n","Exploration Critic Loss : 0.4907996952533722\n","Exploration Model Loss : 1.8341097529628314e-05\n","Exploitation Data q-values : -12.299047470092773\n","Exploitation OOD q-values : -10.692182540893555\n","Exploitation CQL Loss : 1.6068649291992188\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -136.710007\n","best mean reward -130.679993\n","running time 781.956817\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -136.7100067138672\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 781.9568166732788\n","Exploitation Critic Loss : 0.2887185513973236\n","Exploration Critic Loss : 0.40368860960006714\n","Exploration Model Loss : 0.0001698463165666908\n","Exploitation Data q-values : -13.253470420837402\n","Exploitation OOD q-values : -11.65683364868164\n","Exploitation CQL Loss : 1.5966355800628662\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -140.759995\n","best mean reward -130.679993\n","running time 812.074203\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -140.75999450683594\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 812.0742027759552\n","Exploitation Critic Loss : 0.3364739418029785\n","Exploration Critic Loss : 0.5176206231117249\n","Exploration Model Loss : 0.00012493277608882636\n","Exploitation Data q-values : -12.773903846740723\n","Exploitation OOD q-values : -11.148200988769531\n","Exploitation CQL Loss : 1.625702977180481\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -145.059998\n","best mean reward -130.679993\n","running time 844.153085\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -145.05999755859375\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 844.1530854701996\n","Exploitation Critic Loss : 1.833739995956421\n","Exploration Critic Loss : 0.43874621391296387\n","Exploration Model Loss : 0.0003087364020757377\n","Exploitation Data q-values : -12.951811790466309\n","Exploitation OOD q-values : -11.329480171203613\n","Exploitation CQL Loss : 1.6223316192626953\n","Eval_AverageReturn : -50.54999923706055\n","Eval_StdReturn : 19.556264877319336\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -112.0\n","Eval_AverageEpLen : 51.55\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -147.949997\n","best mean reward -130.679993\n","running time 876.561906\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -147.9499969482422\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 876.561906337738\n","Exploitation Critic Loss : 0.38795626163482666\n","Exploration Critic Loss : 0.5881987810134888\n","Exploration Model Loss : 0.00013487096293829381\n","Exploitation Data q-values : -12.563053131103516\n","Exploitation OOD q-values : -10.939621925354004\n","Exploitation CQL Loss : 1.62343168258667\n","Eval_AverageReturn : -45.272727966308594\n","Eval_StdReturn : 13.08427906036377\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -82.0\n","Eval_AverageEpLen : 46.27272727272727\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -149.690002\n","best mean reward -130.679993\n","running time 910.467659\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -149.69000244140625\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 910.4676587581635\n","Exploitation Critic Loss : 0.9069806337356567\n","Exploration Critic Loss : 0.5377012491226196\n","Exploration Model Loss : 6.392212526407093e-05\n","Exploitation Data q-values : -12.901199340820312\n","Exploitation OOD q-values : -11.2811279296875\n","Exploitation CQL Loss : 1.620072364807129\n","Eval_AverageReturn : -54.94444274902344\n","Eval_StdReturn : 24.190155029296875\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -115.0\n","Eval_AverageEpLen : 55.94444444444444\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -149.690002\n","best mean reward -130.679993\n","running time 943.404669\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -149.69000244140625\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 943.4046692848206\n","Exploitation Critic Loss : 1.2688385248184204\n","Exploration Critic Loss : 0.5192855596542358\n","Exploration Model Loss : 4.923083906760439e-05\n","Exploitation Data q-values : -13.089799880981445\n","Exploitation OOD q-values : -11.500905990600586\n","Exploitation CQL Loss : 1.5888941287994385\n","Eval_AverageReturn : -40.66666793823242\n","Eval_StdReturn : 14.33139419555664\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -86.0\n","Eval_AverageEpLen : 41.666666666666664\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 977.672564\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 977.672563791275\n","Exploitation Critic Loss : 1.2687913179397583\n","Exploration Critic Loss : 0.45411792397499084\n","Exploration Model Loss : 6.325988215394318e-05\n","Exploitation Data q-values : -12.568572044372559\n","Exploitation OOD q-values : -10.940071105957031\n","Exploitation CQL Loss : 1.628501057624817\n","Eval_AverageReturn : -41.119998931884766\n","Eval_StdReturn : 9.937082290649414\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -65.0\n","Eval_AverageEpLen : 42.12\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1008.583637\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1008.5836367607117\n","Exploitation Critic Loss : 1.1119184494018555\n","Exploration Critic Loss : 0.4671930968761444\n","Exploration Model Loss : 3.976148218498565e-05\n","Exploitation Data q-values : -13.167400360107422\n","Exploitation OOD q-values : -11.55490493774414\n","Exploitation CQL Loss : 1.6124948263168335\n","Eval_AverageReturn : -50.599998474121094\n","Eval_StdReturn : 21.905250549316406\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -124.0\n","Eval_AverageEpLen : 51.6\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1041.519648\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1041.5196480751038\n","Exploitation Critic Loss : 0.31644150614738464\n","Exploration Critic Loss : 0.4423811435699463\n","Exploration Model Loss : 0.00018614517466630787\n","Exploitation Data q-values : -12.805164337158203\n","Exploitation OOD q-values : -11.178037643432617\n","Exploitation CQL Loss : 1.627126932144165\n","Eval_AverageReturn : -32.064517974853516\n","Eval_StdReturn : 10.248321533203125\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -53.0\n","Eval_AverageEpLen : 33.064516129032256\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1078.766737\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1078.7667367458344\n","Exploitation Critic Loss : 1.1551201343536377\n","Exploration Critic Loss : 0.43458878993988037\n","Exploration Model Loss : 1.5686438814555004e-07\n","Exploitation Data q-values : -13.024467468261719\n","Exploitation OOD q-values : -11.425100326538086\n","Exploitation CQL Loss : 1.599367618560791\n","Eval_AverageReturn : -35.39285659790039\n","Eval_StdReturn : 10.962075233459473\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -56.0\n","Eval_AverageEpLen : 36.392857142857146\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1114.112383\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1114.112382888794\n","Exploitation Critic Loss : 0.2971413731575012\n","Exploration Critic Loss : 0.4162406623363495\n","Exploration Model Loss : 0.001514364150352776\n","Exploitation Data q-values : -13.045473098754883\n","Exploitation OOD q-values : -11.426985740661621\n","Exploitation CQL Loss : 1.6184881925582886\n","Eval_AverageReturn : -42.0\n","Eval_StdReturn : 13.338541030883789\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -70.0\n","Eval_AverageEpLen : 43.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1145.372875\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1145.3728754520416\n","Exploitation Critic Loss : 0.8692439794540405\n","Exploration Critic Loss : 0.45522797107696533\n","Exploration Model Loss : 3.7177454942138866e-05\n","Exploitation Data q-values : -12.585380554199219\n","Exploitation OOD q-values : -10.970651626586914\n","Exploitation CQL Loss : 1.6147288084030151\n","Eval_AverageReturn : -42.79166793823242\n","Eval_StdReturn : 15.15470027923584\n","Eval_MaxReturn : -21.0\n","Eval_MinReturn : -76.0\n","Eval_AverageEpLen : 43.791666666666664\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1178.932628\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1178.9326283931732\n","Exploitation Critic Loss : 2.725693941116333\n","Exploration Critic Loss : 0.4781514108181\n","Exploration Model Loss : 4.315447199587652e-07\n","Exploitation Data q-values : -12.829277992248535\n","Exploitation OOD q-values : -11.209291458129883\n","Exploitation CQL Loss : 1.6199864149093628\n","Eval_AverageReturn : -37.14814758300781\n","Eval_StdReturn : 12.854466438293457\n","Eval_MaxReturn : -21.0\n","Eval_MinReturn : -67.0\n","Eval_AverageEpLen : 38.148148148148145\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1213.677064\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1213.6770644187927\n","Exploitation Critic Loss : 1.3003684282302856\n","Exploration Critic Loss : 0.5819056034088135\n","Exploration Model Loss : 0.00031052259146235883\n","Exploitation Data q-values : -12.36146068572998\n","Exploitation OOD q-values : -10.743539810180664\n","Exploitation CQL Loss : 1.617921233177185\n","Eval_AverageReturn : -45.681819915771484\n","Eval_StdReturn : 15.612892150878906\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -88.0\n","Eval_AverageEpLen : 46.68181818181818\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1244.885985\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1244.885984659195\n","Exploitation Critic Loss : 1.5768669843673706\n","Exploration Critic Loss : 0.6055566072463989\n","Exploration Model Loss : 7.594098860863596e-08\n","Exploitation Data q-values : -13.090339660644531\n","Exploitation OOD q-values : -11.50003433227539\n","Exploitation CQL Loss : 1.5903050899505615\n","Eval_AverageReturn : -47.619049072265625\n","Eval_StdReturn : 18.227336883544922\n","Eval_MaxReturn : -19.0\n","Eval_MinReturn : -95.0\n","Eval_AverageEpLen : 48.61904761904762\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1281.552109\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1281.5521092414856\n","Exploitation Critic Loss : 0.33879464864730835\n","Exploration Critic Loss : 0.5634856820106506\n","Exploration Model Loss : 2.5119759811786935e-05\n","Exploitation Data q-values : -12.416168212890625\n","Exploitation OOD q-values : -10.80435848236084\n","Exploitation CQL Loss : 1.6118096113204956\n","Eval_AverageReturn : -43.0\n","Eval_StdReturn : 15.617715835571289\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -88.0\n","Eval_AverageEpLen : 44.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1315.626643\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1315.6266429424286\n","Exploitation Critic Loss : 0.6577744483947754\n","Exploration Critic Loss : 0.5081958770751953\n","Exploration Model Loss : 5.085294318973865e-08\n","Exploitation Data q-values : -12.787988662719727\n","Exploitation OOD q-values : -11.169482231140137\n","Exploitation CQL Loss : 1.6185071468353271\n","Eval_AverageReturn : -39.7599983215332\n","Eval_StdReturn : 13.860100746154785\n","Eval_MaxReturn : -19.0\n","Eval_MinReturn : -76.0\n","Eval_AverageEpLen : 40.76\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1350.887491\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1350.8874912261963\n","Exploitation Critic Loss : 0.4156525731086731\n","Exploration Critic Loss : 0.5239725112915039\n","Exploration Model Loss : 1.3249580206320388e-06\n","Exploitation Data q-values : -12.073954582214355\n","Exploitation OOD q-values : -10.471355438232422\n","Exploitation CQL Loss : 1.6025983095169067\n","Eval_AverageReturn : -35.07143020629883\n","Eval_StdReturn : 12.121198654174805\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -62.0\n","Eval_AverageEpLen : 36.07142857142857\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1383.946065\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1383.9460649490356\n","Exploitation Critic Loss : 0.5788058638572693\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.250240325927734\n","Exploitation OOD q-values : -10.629863739013672\n","Exploitation CQL Loss : 1.6203763484954834\n","Eval_AverageReturn : -36.33333206176758\n","Eval_StdReturn : 11.195237159729004\n","Eval_MaxReturn : -20.0\n","Eval_MinReturn : -66.0\n","Eval_AverageEpLen : 37.333333333333336\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1421.810011\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1421.8100113868713\n","Exploitation Critic Loss : 1.9328415393829346\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.888300895690918\n","Exploitation OOD q-values : -11.252050399780273\n","Exploitation CQL Loss : 1.6362504959106445\n","Eval_AverageReturn : -36.44444274902344\n","Eval_StdReturn : 10.754270553588867\n","Eval_MaxReturn : -21.0\n","Eval_MinReturn : -69.0\n","Eval_AverageEpLen : 37.44444444444444\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1463.741612\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1463.7416117191315\n","Exploitation Critic Loss : 1.116284728050232\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.56886100769043\n","Exploitation OOD q-values : -10.969361305236816\n","Exploitation CQL Loss : 1.5994997024536133\n","Eval_AverageReturn : -34.344825744628906\n","Eval_StdReturn : 9.487835884094238\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -59.0\n","Eval_AverageEpLen : 35.3448275862069\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1501.726345\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1501.7263448238373\n","Exploitation Critic Loss : 0.2725893259048462\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.760738372802734\n","Exploitation OOD q-values : -11.124092102050781\n","Exploitation CQL Loss : 1.6366474628448486\n","Eval_AverageReturn : -55.77777862548828\n","Eval_StdReturn : 27.155019760131836\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 56.72222222222222\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -150.000000\n","best mean reward -130.679993\n","running time 1538.108423\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1538.1084232330322\n","Exploitation Critic Loss : 0.33512526750564575\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.523075103759766\n","Exploitation OOD q-values : -10.931352615356445\n","Exploitation CQL Loss : 1.591722846031189\n","Eval_AverageReturn : -55.83333206176758\n","Eval_StdReturn : 22.62803077697754\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -110.0\n","Eval_AverageEpLen : 56.833333333333336\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -149.649994\n","best mean reward -130.679993\n","running time 1572.153976\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -149.64999389648438\n","Train_BestReturn : -130.67999267578125\n","TimeSinceStart : 1572.153976202011\n","Exploitation Critic Loss : 1.4775699377059937\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Data q-values : -12.530054092407227\n","Exploitation OOD q-values : -10.882165908813477\n","Exploitation CQL Loss : 1.6478891372680664\n","Eval_AverageReturn : -52.94736862182617\n","Eval_StdReturn : 19.70432472229004\n","Eval_MaxReturn : -21.0\n","Eval_MinReturn : -94.0\n","Eval_AverageEpLen : 53.94736842105263\n","Buffer size : 15001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_expl.py --env_name PointmassMedium-v0 --use_rnd \\\n"," --num_exploration_steps=15000 --offline_exploitation --cql_alpha=0.1 \\\n"," --unsupervised_exploration --exp_name q2_cql_numsteps_15000 --seed 10"]},{"cell_type":"markdown","metadata":{"id":"g8uWfqd7Jdeb"},"source":["## 2.3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1581998,"status":"ok","timestamp":1669074041298,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"8fhRT1pfJeGA","outputId":"98b0f212-7a66-425f-86d0-afbffc24e1fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_alpha_0.02_PointmassMedium-v0_21-11-2022_23-14-21 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_alpha_0.02_PointmassMedium-v0_21-11-2022_23-14-21\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002198\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0021979808807373047\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 6.012508\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 6.012507677078247\n","Eval_AverageReturn : -148.0\n","Eval_StdReturn : 4.898979663848877\n","Eval_MaxReturn : -136.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.14285714285714\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -147.076920\n","best mean reward -inf\n","running time 12.464131\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -147.07691955566406\n","TimeSinceStart : 12.464131116867065\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -148.100006\n","best mean reward -inf\n","running time 46.165453\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -148.10000610351562\n","TimeSinceStart : 46.16545271873474\n","Exploitation Critic Loss : 0.06170982867479324\n","Exploration Critic Loss : 0.23501938581466675\n","Exploration Model Loss : 0.003443858353421092\n","Exploitation Data q-values : -3.717459201812744\n","Exploitation OOD q-values : -2.1053524017333984\n","Exploitation CQL Loss : 1.6121065616607666\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -148.538467\n","best mean reward -inf\n","running time 77.936170\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -148.53846740722656\n","TimeSinceStart : 77.93617033958435\n","Exploitation Critic Loss : 0.5050070881843567\n","Exploration Critic Loss : 0.40018230676651\n","Exploration Model Loss : 0.0025919515173882246\n","Exploitation Data q-values : -5.946537971496582\n","Exploitation OOD q-values : -4.329968452453613\n","Exploitation CQL Loss : 1.6165688037872314\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -148.848480\n","best mean reward -inf\n","running time 108.014438\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -148.84848022460938\n","TimeSinceStart : 108.01443791389465\n","Exploitation Critic Loss : 0.737006664276123\n","Exploration Critic Loss : 0.5372496843338013\n","Exploration Model Loss : 0.0017094359500333667\n","Exploitation Data q-values : -7.828643321990967\n","Exploitation OOD q-values : -6.191281318664551\n","Exploitation CQL Loss : 1.637361764907837\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -149.050003\n","best mean reward -inf\n","running time 139.598136\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -149.0500030517578\n","TimeSinceStart : 139.5981364250183\n","Exploitation Critic Loss : 0.3485569953918457\n","Exploration Critic Loss : 0.5668397545814514\n","Exploration Model Loss : 0.0015617155004292727\n","Exploitation Data q-values : -9.537906646728516\n","Exploitation OOD q-values : -7.9368205070495605\n","Exploitation CQL Loss : 1.6010863780975342\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -149.173920\n","best mean reward -inf\n","running time 173.619466\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -149.17391967773438\n","TimeSinceStart : 173.6194658279419\n","Exploitation Critic Loss : 0.39339911937713623\n","Exploration Critic Loss : 0.5193716883659363\n","Exploration Model Loss : 0.002252047648653388\n","Exploitation Data q-values : -10.977251052856445\n","Exploitation OOD q-values : -9.361464500427246\n","Exploitation CQL Loss : 1.6157861948013306\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -149.283020\n","best mean reward -inf\n","running time 202.565875\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -149.28302001953125\n","TimeSinceStart : 202.56587505340576\n","Exploitation Critic Loss : 0.04599708318710327\n","Exploration Critic Loss : 0.5089643001556396\n","Exploration Model Loss : 0.000790068821515888\n","Exploitation Data q-values : -11.85403060913086\n","Exploitation OOD q-values : -10.256705284118652\n","Exploitation CQL Loss : 1.597326397895813\n","Eval_AverageReturn : -130.625\n","Eval_StdReturn : 32.82886505126953\n","Eval_MaxReturn : -55.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 131.0\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -149.366669\n","best mean reward -inf\n","running time 235.372833\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -149.36666870117188\n","TimeSinceStart : 235.37283301353455\n","Exploitation Critic Loss : 1.0221737623214722\n","Exploration Critic Loss : 0.6209517121315002\n","Exploration Model Loss : 0.0005329704727046192\n","Exploitation Data q-values : -12.418749809265137\n","Exploitation OOD q-values : -10.838443756103516\n","Exploitation CQL Loss : 1.580305814743042\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -149.424240\n","best mean reward -inf\n","running time 266.901576\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -149.4242401123047\n","TimeSinceStart : 266.9015760421753\n","Exploitation Critic Loss : 1.6303913593292236\n","Exploration Critic Loss : 0.6369593143463135\n","Exploration Model Loss : 0.00039413091144524515\n","Exploitation Data q-values : -13.138917922973633\n","Exploitation OOD q-values : -11.51777172088623\n","Exploitation CQL Loss : 1.621146559715271\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -149.479446\n","best mean reward -inf\n","running time 298.392422\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -149.4794464111328\n","TimeSinceStart : 298.3924219608307\n","Exploitation Critic Loss : 0.6384375095367432\n","Exploration Critic Loss : 0.401522696018219\n","Exploration Model Loss : 0.0002460857795085758\n","Exploitation Data q-values : -13.825601577758789\n","Exploitation OOD q-values : -12.2160062789917\n","Exploitation CQL Loss : 1.6095951795578003\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -149.524994\n","best mean reward -inf\n","running time 330.367702\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -149.52499389648438\n","TimeSinceStart : 330.3677017688751\n","Exploitation Critic Loss : 0.8137521743774414\n","Exploration Critic Loss : 0.593626081943512\n","Exploration Model Loss : 0.00018952842219732702\n","Exploitation Data q-values : -14.40165901184082\n","Exploitation OOD q-values : -12.809820175170898\n","Exploitation CQL Loss : 1.5918397903442383\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -149.558136\n","best mean reward -inf\n","running time 361.454782\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -149.55813598632812\n","TimeSinceStart : 361.4547824859619\n","Exploitation Critic Loss : 0.8613266348838806\n","Exploration Critic Loss : 0.4669604003429413\n","Exploration Model Loss : 9.274044714402407e-05\n","Exploitation Data q-values : -14.637243270874023\n","Exploitation OOD q-values : -13.025054931640625\n","Exploitation CQL Loss : 1.612189531326294\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -149.591400\n","best mean reward -inf\n","running time 392.710515\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -149.59140014648438\n","TimeSinceStart : 392.7105145454407\n","Exploitation Critic Loss : 2.2053894996643066\n","Exploration Critic Loss : 0.8145778179168701\n","Exploration Model Loss : 7.539488433394581e-05\n","Exploitation Data q-values : -14.719059944152832\n","Exploitation OOD q-values : -13.079154968261719\n","Exploitation CQL Loss : 1.6399061679840088\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -149.619995\n","best mean reward -inf\n","running time 422.477277\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -149.6199951171875\n","TimeSinceStart : 422.47727704048157\n","Exploitation Critic Loss : 1.6479929685592651\n","Exploration Critic Loss : 0.8673974871635437\n","Exploration Model Loss : 4.337666541687213e-05\n","Exploitation Data q-values : -15.002606391906738\n","Exploitation OOD q-values : -13.39816951751709\n","Exploitation CQL Loss : 1.6044367551803589\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -149.619995\n","best mean reward -149.619995\n","running time 452.824935\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -149.6199951171875\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 452.8249349594116\n","Exploitation Critic Loss : 0.074605792760849\n","Exploration Critic Loss : 0.8058077692985535\n","Exploration Model Loss : 6.56511474517174e-05\n","Exploitation Data q-values : -14.866303443908691\n","Exploitation OOD q-values : -13.274928092956543\n","Exploitation CQL Loss : 1.5913760662078857\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 484.410851\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 484.41085052490234\n","Exploitation Critic Loss : 2.1815226078033447\n","Exploration Critic Loss : 1.0712413787841797\n","Exploration Model Loss : 1.8279200958204456e-05\n","Exploitation Data q-values : -14.885213851928711\n","Exploitation OOD q-values : -13.223979949951172\n","Exploitation CQL Loss : 1.661233901977539\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 518.118867\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 518.1188669204712\n","Exploitation Critic Loss : 2.2266366481781006\n","Exploration Critic Loss : 1.0494521856307983\n","Exploration Model Loss : 2.0931256585754454e-05\n","Exploitation Data q-values : -14.375059127807617\n","Exploitation OOD q-values : -12.738713264465332\n","Exploitation CQL Loss : 1.6363458633422852\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 546.645465\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 546.6454651355743\n","Exploitation Critic Loss : 1.473393201828003\n","Exploration Critic Loss : 1.4569605588912964\n","Exploration Model Loss : 0.00011554831871762872\n","Exploitation Data q-values : -14.684011459350586\n","Exploitation OOD q-values : -13.069774627685547\n","Exploitation CQL Loss : 1.61423659324646\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 578.175613\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 578.1756126880646\n","Exploitation Critic Loss : 0.11174114048480988\n","Exploration Critic Loss : 0.7820076942443848\n","Exploration Model Loss : 0.00010971630399581045\n","Exploitation Data q-values : -14.284536361694336\n","Exploitation OOD q-values : -12.678918838500977\n","Exploitation CQL Loss : 1.6056166887283325\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 609.762530\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 609.7625300884247\n","Exploitation Critic Loss : 1.0837115049362183\n","Exploration Critic Loss : 1.1102906465530396\n","Exploration Model Loss : 0.00017568314797244966\n","Exploitation Data q-values : -14.11128044128418\n","Exploitation OOD q-values : -12.449193954467773\n","Exploitation CQL Loss : 1.662084937095642\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 639.511626\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 639.5116257667542\n","Exploitation Critic Loss : 0.850421667098999\n","Exploration Critic Loss : 0.9133723378181458\n","Exploration Model Loss : 3.712975376402028e-05\n","Exploitation Data q-values : -14.066502571105957\n","Exploitation OOD q-values : -12.439535140991211\n","Exploitation CQL Loss : 1.626966953277588\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 669.450037\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 669.4500365257263\n","Exploitation Critic Loss : 0.5033329129219055\n","Exploration Critic Loss : 0.9240825176239014\n","Exploration Model Loss : 0.00015425922174472362\n","Exploitation Data q-values : -13.811637878417969\n","Exploitation OOD q-values : -12.168392181396484\n","Exploitation CQL Loss : 1.6432453393936157\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 703.981346\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 703.9813456535339\n","Exploitation Critic Loss : 1.4822394847869873\n","Exploration Critic Loss : 1.2853223085403442\n","Exploration Model Loss : 4.715082832262851e-05\n","Exploitation Data q-values : -13.962258338928223\n","Exploitation OOD q-values : -12.330343246459961\n","Exploitation CQL Loss : 1.6319162845611572\n","Eval_AverageReturn : -135.0\n","Eval_StdReturn : 39.6862678527832\n","Eval_MaxReturn : -30.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 135.125\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 735.359584\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 735.3595840930939\n","Exploitation Critic Loss : 2.1050496101379395\n","Exploration Critic Loss : 1.4334211349487305\n","Exploration Model Loss : 2.6250709197483957e-05\n","Exploitation Data q-values : -14.057533264160156\n","Exploitation OOD q-values : -12.397233963012695\n","Exploitation CQL Loss : 1.6602988243103027\n","Eval_AverageReturn : -143.125\n","Eval_StdReturn : 18.18954086303711\n","Eval_MaxReturn : -95.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 143.25\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 764.399000\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 764.3989999294281\n","Exploitation Critic Loss : 0.8489243388175964\n","Exploration Critic Loss : 0.8887609243392944\n","Exploration Model Loss : 9.19890298973769e-05\n","Exploitation Data q-values : -14.246454238891602\n","Exploitation OOD q-values : -12.588565826416016\n","Exploitation CQL Loss : 1.657888412475586\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 795.719349\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 795.7193491458893\n","Exploitation Critic Loss : 0.893864631652832\n","Exploration Critic Loss : 0.5797362327575684\n","Exploration Model Loss : 2.0325482182670385e-05\n","Exploitation Data q-values : -13.900447845458984\n","Exploitation OOD q-values : -12.222240447998047\n","Exploitation CQL Loss : 1.678207278251648\n","Eval_AverageReturn : -149.42857360839844\n","Eval_StdReturn : 1.3997083902359009\n","Eval_MaxReturn : -146.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 149.57142857142858\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 827.496109\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 827.4961087703705\n","Exploitation Critic Loss : 1.2757928371429443\n","Exploration Critic Loss : 0.7920855283737183\n","Exploration Model Loss : 0.00011291007103864104\n","Exploitation Data q-values : -13.83912467956543\n","Exploitation OOD q-values : -12.163235664367676\n","Exploitation CQL Loss : 1.6758885383605957\n","Eval_AverageReturn : -102.30000305175781\n","Eval_StdReturn : 45.34986114501953\n","Eval_MaxReturn : -33.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 102.9\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 857.328988\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 857.3289878368378\n","Exploitation Critic Loss : 3.194140672683716\n","Exploration Critic Loss : 1.1603764295578003\n","Exploration Model Loss : 4.127100692130625e-05\n","Exploitation Data q-values : -13.384994506835938\n","Exploitation OOD q-values : -11.71933364868164\n","Exploitation CQL Loss : 1.6656618118286133\n","Eval_AverageReturn : -136.5\n","Eval_StdReturn : 24.484689712524414\n","Eval_MaxReturn : -76.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 136.875\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 891.023596\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 891.023595571518\n","Exploitation Critic Loss : 0.10845322906970978\n","Exploration Critic Loss : 0.5318089723587036\n","Exploration Model Loss : 0.00012324251292739064\n","Exploitation Data q-values : -12.855156898498535\n","Exploitation OOD q-values : -11.241106986999512\n","Exploitation CQL Loss : 1.6140505075454712\n","Eval_AverageReturn : -104.5\n","Eval_StdReturn : 36.47533416748047\n","Eval_MaxReturn : -39.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 105.2\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 922.846211\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 922.8462107181549\n","Exploitation Critic Loss : 1.1059727668762207\n","Exploration Critic Loss : 1.2396769523620605\n","Exploration Model Loss : 0.00019197550136595964\n","Exploitation Data q-values : -12.73776626586914\n","Exploitation OOD q-values : -11.080049514770508\n","Exploitation CQL Loss : 1.6577178239822388\n","Eval_AverageReturn : -64.52941131591797\n","Eval_StdReturn : 30.252113342285156\n","Eval_MaxReturn : -19.0\n","Eval_MinReturn : -141.0\n","Eval_AverageEpLen : 65.52941176470588\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 957.401550\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 957.4015498161316\n","Exploitation Critic Loss : 1.9333735704421997\n","Exploration Critic Loss : 1.0216505527496338\n","Exploration Model Loss : 0.00036886311136186123\n","Exploitation Data q-values : -12.958741188049316\n","Exploitation OOD q-values : -11.307287216186523\n","Exploitation CQL Loss : 1.6514531373977661\n","Eval_AverageReturn : -42.65217208862305\n","Eval_StdReturn : 14.088565826416016\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -67.0\n","Eval_AverageEpLen : 43.65217391304348\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 988.259499\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 988.2594993114471\n","Exploitation Critic Loss : 0.3887222409248352\n","Exploration Critic Loss : 0.5910744667053223\n","Exploration Model Loss : 1.6692723647793173e-06\n","Exploitation Data q-values : -12.357454299926758\n","Exploitation OOD q-values : -10.708667755126953\n","Exploitation CQL Loss : 1.6487865447998047\n","Eval_AverageReturn : -79.38461303710938\n","Eval_StdReturn : 41.44048309326172\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 80.23076923076923\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1019.654583\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1019.6545825004578\n","Exploitation Critic Loss : 1.727844476699829\n","Exploration Critic Loss : 0.7581939101219177\n","Exploration Model Loss : 0.0005189941148273647\n","Exploitation Data q-values : -12.46156120300293\n","Exploitation OOD q-values : -10.79347038269043\n","Exploitation CQL Loss : 1.6680912971496582\n","Eval_AverageReturn : -41.0\n","Eval_StdReturn : 13.369742393493652\n","Eval_MaxReturn : -19.0\n","Eval_MinReturn : -64.0\n","Eval_AverageEpLen : 42.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1056.931710\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1056.931710243225\n","Exploitation Critic Loss : 1.8080697059631348\n","Exploration Critic Loss : 0.914169192314148\n","Exploration Model Loss : 0.00014091003686189651\n","Exploitation Data q-values : -12.671629905700684\n","Exploitation OOD q-values : -11.007509231567383\n","Exploitation CQL Loss : 1.6641206741333008\n","Eval_AverageReturn : -41.125\n","Eval_StdReturn : 14.799529075622559\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -69.0\n","Eval_AverageEpLen : 42.125\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1091.493836\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1091.4938356876373\n","Exploitation Critic Loss : 1.3740297555923462\n","Exploration Critic Loss : 0.5284042954444885\n","Exploration Model Loss : 1.2327438525971957e-05\n","Exploitation Data q-values : -12.450077056884766\n","Exploitation OOD q-values : -10.773679733276367\n","Exploitation CQL Loss : 1.6763968467712402\n","Eval_AverageReturn : -37.69230651855469\n","Eval_StdReturn : 12.362378120422363\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -62.0\n","Eval_AverageEpLen : 38.69230769230769\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1126.049432\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1126.0494318008423\n","Exploitation Critic Loss : 1.6115033626556396\n","Exploration Critic Loss : 0.6609514951705933\n","Exploration Model Loss : 2.4327509891008958e-05\n","Exploitation Data q-values : -11.886488914489746\n","Exploitation OOD q-values : -10.208513259887695\n","Exploitation CQL Loss : 1.6779768466949463\n","Eval_AverageReturn : -38.730770111083984\n","Eval_StdReturn : 14.500816345214844\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -67.0\n","Eval_AverageEpLen : 39.73076923076923\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1157.908454\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1157.9084537029266\n","Exploitation Critic Loss : 0.7246338725090027\n","Exploration Critic Loss : 0.49483275413513184\n","Exploration Model Loss : 4.177059054200072e-06\n","Exploitation Data q-values : -11.964897155761719\n","Exploitation OOD q-values : -10.281689643859863\n","Exploitation CQL Loss : 1.6832079887390137\n","Eval_AverageReturn : -42.739131927490234\n","Eval_StdReturn : 16.176250457763672\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -92.0\n","Eval_AverageEpLen : 43.73913043478261\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1192.860971\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1192.8609709739685\n","Exploitation Critic Loss : 0.7160828113555908\n","Exploration Critic Loss : 0.3846198320388794\n","Exploration Model Loss : 2.2500939849123824e-06\n","Exploitation Data q-values : -11.972541809082031\n","Exploitation OOD q-values : -10.26917552947998\n","Exploitation CQL Loss : 1.7033660411834717\n","Eval_AverageReturn : -33.400001525878906\n","Eval_StdReturn : 8.16333293914795\n","Eval_MaxReturn : -21.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 34.4\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1230.511741\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1230.5117409229279\n","Exploitation Critic Loss : 1.0174040794372559\n","Exploration Critic Loss : 0.6400575041770935\n","Exploration Model Loss : 0.0002025921130552888\n","Exploitation Data q-values : -12.210569381713867\n","Exploitation OOD q-values : -10.516237258911133\n","Exploitation CQL Loss : 1.6943323612213135\n","Eval_AverageReturn : -30.18181800842285\n","Eval_StdReturn : 10.314610481262207\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -58.0\n","Eval_AverageEpLen : 31.181818181818183\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1266.012531\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1266.012531042099\n","Exploitation Critic Loss : 2.385906219482422\n","Exploration Critic Loss : 0.6889747977256775\n","Exploration Model Loss : 0.00010517402552068233\n","Exploitation Data q-values : -12.359931945800781\n","Exploitation OOD q-values : -10.633495330810547\n","Exploitation CQL Loss : 1.7264368534088135\n","Eval_AverageReturn : -118.66666412353516\n","Eval_StdReturn : 30.155153274536133\n","Eval_MaxReturn : -61.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 119.33333333333333\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1298.713934\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1298.7139341831207\n","Exploitation Critic Loss : 2.295081853866577\n","Exploration Critic Loss : 0.4559791684150696\n","Exploration Model Loss : 0.0001815506984712556\n","Exploitation Data q-values : -12.520989418029785\n","Exploitation OOD q-values : -10.864131927490234\n","Exploitation CQL Loss : 1.6568584442138672\n","Eval_AverageReturn : -35.60714340209961\n","Eval_StdReturn : 14.380887985229492\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -69.0\n","Eval_AverageEpLen : 36.607142857142854\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1330.966279\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1330.9662792682648\n","Exploitation Critic Loss : 0.09913063049316406\n","Exploration Critic Loss : 0.5642525553703308\n","Exploration Model Loss : 4.4762091420125216e-05\n","Exploitation Data q-values : -12.510764122009277\n","Exploitation OOD q-values : -10.829590797424316\n","Exploitation CQL Loss : 1.6811734437942505\n","Eval_AverageReturn : -40.119998931884766\n","Eval_StdReturn : 15.975781440734863\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -83.0\n","Eval_AverageEpLen : 41.12\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1364.953239\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1364.9532389640808\n","Exploitation Critic Loss : 0.1390150636434555\n","Exploration Critic Loss : 0.25961121916770935\n","Exploration Model Loss : 0.00031482535996474326\n","Exploitation Data q-values : -11.9480619430542\n","Exploitation OOD q-values : -10.247237205505371\n","Exploitation CQL Loss : 1.700824499130249\n","Eval_AverageReturn : -33.82758712768555\n","Eval_StdReturn : 11.029955863952637\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -57.0\n","Eval_AverageEpLen : 34.827586206896555\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1402.484579\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1402.4845786094666\n","Exploitation Critic Loss : 0.9753524661064148\n","Exploration Critic Loss : 0.393815815448761\n","Exploration Model Loss : 0.0001125240305555053\n","Exploitation Data q-values : -12.00979995727539\n","Exploitation OOD q-values : -10.291306495666504\n","Exploitation CQL Loss : 1.7184933423995972\n","Eval_AverageReturn : -34.13793182373047\n","Eval_StdReturn : 11.21182918548584\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -71.0\n","Eval_AverageEpLen : 35.13793103448276\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1438.356685\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1438.3566851615906\n","Exploitation Critic Loss : 0.2738252878189087\n","Exploration Critic Loss : 0.4727775752544403\n","Exploration Model Loss : 1.471042082812346e-06\n","Exploitation Data q-values : -11.79757308959961\n","Exploitation OOD q-values : -10.138967514038086\n","Exploitation CQL Loss : 1.6586060523986816\n","Eval_AverageReturn : -34.71428680419922\n","Eval_StdReturn : 12.916923522949219\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -64.0\n","Eval_AverageEpLen : 35.714285714285715\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1473.590868\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1473.5908675193787\n","Exploitation Critic Loss : 1.970909833908081\n","Exploration Critic Loss : 0.5116754770278931\n","Exploration Model Loss : 0.00011308778630336747\n","Exploitation Data q-values : -11.906503677368164\n","Exploitation OOD q-values : -10.194934844970703\n","Exploitation CQL Loss : 1.7115696668624878\n","Eval_AverageReturn : -35.07143020629883\n","Eval_StdReturn : 11.566356658935547\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -63.0\n","Eval_AverageEpLen : 36.07142857142857\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1505.930953\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1505.9309525489807\n","Exploitation Critic Loss : 0.32260993123054504\n","Exploration Critic Loss : 0.49983954429626465\n","Exploration Model Loss : 2.2188230559549993e-06\n","Exploitation Data q-values : -11.392465591430664\n","Exploitation OOD q-values : -9.685992240905762\n","Exploitation CQL Loss : 1.7064729928970337\n","Eval_AverageReturn : -36.33333206176758\n","Eval_StdReturn : 11.734722137451172\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -64.0\n","Eval_AverageEpLen : 37.333333333333336\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.619995\n","running time 1540.987515\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.6199951171875\n","TimeSinceStart : 1540.9875149726868\n","Exploitation Critic Loss : 0.931884765625\n","Exploration Critic Loss : 0.5985658764839172\n","Exploration Model Loss : 2.2415993043978233e-06\n","Exploitation Data q-values : -11.695390701293945\n","Exploitation OOD q-values : -10.033286094665527\n","Exploitation CQL Loss : 1.6621049642562866\n","Eval_AverageReturn : -34.17241287231445\n","Eval_StdReturn : 12.67924976348877\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -79.0\n","Eval_AverageEpLen : 35.172413793103445\n","Buffer size : 10001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_expl.py --env_name PointmassMedium-v0 --use_rnd \\\n"," --unsupervised_exploration --offline_exploitation --cql_alpha=0.02 \\\n"," --exp_name q2_alpha_0.02 --seed 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1529124,"status":"ok","timestamp":1669076373108,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"nRLnlKLIJj-D","outputId":"36888fbc-79a9-45bf-af73-f037b8c59a18"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_alpha_0.5_PointmassMedium-v0_21-11-2022_23-54-06 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q2_alpha_0.5_PointmassMedium-v0_21-11-2022_23-54-06\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002048\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.002048492431640625\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -141.428574\n","best mean reward -inf\n","running time 6.148895\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -141.42857360839844\n","TimeSinceStart : 6.148895025253296\n","Eval_AverageReturn : -148.0\n","Eval_StdReturn : 4.898979663848877\n","Eval_MaxReturn : -136.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.14285714285714\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -145.384613\n","best mean reward -inf\n","running time 12.446843\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -145.38461303710938\n","TimeSinceStart : 12.446843147277832\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -147.000000\n","best mean reward -inf\n","running time 43.105275\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -147.0\n","TimeSinceStart : 43.10527467727661\n","Exploitation Critic Loss : 0.832726240158081\n","Exploration Critic Loss : 0.1263597160577774\n","Exploration Model Loss : 0.302033007144928\n","Exploitation Data q-values : -3.7283968925476074\n","Exploitation OOD q-values : -2.145744800567627\n","Exploitation CQL Loss : 1.582651972770691\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -147.777771\n","best mean reward -inf\n","running time 75.802633\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -147.77777099609375\n","TimeSinceStart : 75.8026328086853\n","Exploitation Critic Loss : 1.1108219623565674\n","Exploration Critic Loss : 0.2539733350276947\n","Exploration Model Loss : 0.002325530396774411\n","Exploitation Data q-values : -5.712898254394531\n","Exploitation OOD q-values : -4.192148685455322\n","Exploitation CQL Loss : 1.5207496881484985\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -148.181824\n","best mean reward -inf\n","running time 107.168698\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -148.18182373046875\n","TimeSinceStart : 107.16869831085205\n","Exploitation Critic Loss : 0.9385559558868408\n","Exploration Critic Loss : 0.32967108488082886\n","Exploration Model Loss : 0.0019327558111399412\n","Exploitation Data q-values : -7.456923484802246\n","Exploitation OOD q-values : -5.9610185623168945\n","Exploitation CQL Loss : 1.4959051609039307\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -148.500000\n","best mean reward -inf\n","running time 138.479871\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -148.5\n","TimeSinceStart : 138.4798707962036\n","Exploitation Critic Loss : 1.4983932971954346\n","Exploration Critic Loss : 0.5916078090667725\n","Exploration Model Loss : 0.0019275115337222815\n","Exploitation Data q-values : -9.134838104248047\n","Exploitation OOD q-values : -7.691030502319336\n","Exploitation CQL Loss : 1.4438081979751587\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -148.723404\n","best mean reward -inf\n","running time 166.826521\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -148.72340393066406\n","TimeSinceStart : 166.82652068138123\n","Exploitation Critic Loss : 1.1057192087173462\n","Exploration Critic Loss : 0.6670641899108887\n","Exploration Model Loss : 0.0016059126937761903\n","Exploitation Data q-values : -10.239095687866211\n","Exploitation OOD q-values : -8.79506778717041\n","Exploitation CQL Loss : 1.444027304649353\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -148.867920\n","best mean reward -inf\n","running time 198.136524\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -148.867919921875\n","TimeSinceStart : 198.13652396202087\n","Exploitation Critic Loss : 1.5597273111343384\n","Exploration Critic Loss : 0.5866937041282654\n","Exploration Model Loss : 0.0007451979909092188\n","Exploitation Data q-values : -11.230979919433594\n","Exploitation OOD q-values : -9.756660461425781\n","Exploitation CQL Loss : 1.4743189811706543\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -149.000000\n","best mean reward -inf\n","running time 229.665425\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -149.0\n","TimeSinceStart : 229.6654245853424\n","Exploitation Critic Loss : 1.2493007183074951\n","Exploration Critic Loss : 0.7888512015342712\n","Exploration Model Loss : 0.0006247403216548264\n","Exploitation Data q-values : -12.187997817993164\n","Exploitation OOD q-values : -10.7457275390625\n","Exploitation CQL Loss : 1.4422714710235596\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -149.104477\n","best mean reward -inf\n","running time 263.397838\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -149.10447692871094\n","TimeSinceStart : 263.39783811569214\n","Exploitation Critic Loss : 2.917071580886841\n","Exploration Critic Loss : 0.7055737376213074\n","Exploration Model Loss : 0.00038576271617785096\n","Exploitation Data q-values : -12.751864433288574\n","Exploitation OOD q-values : -11.301600456237793\n","Exploitation CQL Loss : 1.4502644538879395\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -149.178085\n","best mean reward -inf\n","running time 292.491030\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -149.17808532714844\n","TimeSinceStart : 292.4910295009613\n","Exploitation Critic Loss : 3.6278979778289795\n","Exploration Critic Loss : 0.5041624903678894\n","Exploration Model Loss : 0.00026052433531731367\n","Exploitation Data q-values : -13.227157592773438\n","Exploitation OOD q-values : -11.761944770812988\n","Exploitation CQL Loss : 1.4652128219604492\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -149.250000\n","best mean reward -inf\n","running time 325.732251\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -149.25\n","TimeSinceStart : 325.7322506904602\n","Exploitation Critic Loss : 3.343338966369629\n","Exploration Critic Loss : 0.46507030725479126\n","Exploration Model Loss : 0.00015042675659060478\n","Exploitation Data q-values : -13.980772972106934\n","Exploitation OOD q-values : -12.552309036254883\n","Exploitation CQL Loss : 1.4284629821777344\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -149.310349\n","best mean reward -inf\n","running time 360.086583\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -149.3103485107422\n","TimeSinceStart : 360.0865831375122\n","Exploitation Critic Loss : 0.7638306021690369\n","Exploration Critic Loss : 0.3046160340309143\n","Exploration Model Loss : 0.00011744823132175952\n","Exploitation Data q-values : -14.363409042358398\n","Exploitation OOD q-values : -12.908742904663086\n","Exploitation CQL Loss : 1.4546661376953125\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -149.354843\n","best mean reward -inf\n","running time 389.560428\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -149.35484313964844\n","TimeSinceStart : 389.5604283809662\n","Exploitation Critic Loss : 2.7857651710510254\n","Exploration Critic Loss : 0.4107469320297241\n","Exploration Model Loss : 7.008596730884165e-05\n","Exploitation Data q-values : -14.571950912475586\n","Exploitation OOD q-values : -13.140523910522461\n","Exploitation CQL Loss : 1.431427240371704\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -149.399994\n","best mean reward -inf\n","running time 423.686849\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -149.39999389648438\n","TimeSinceStart : 423.6868488788605\n","Exploitation Critic Loss : 2.3752079010009766\n","Exploration Critic Loss : 0.3707239031791687\n","Exploration Model Loss : 5.648325168294832e-05\n","Exploitation Data q-values : -14.865655899047852\n","Exploitation OOD q-values : -13.419785499572754\n","Exploitation CQL Loss : 1.4458715915679932\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 455.877169\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 455.87716913223267\n","Exploitation Critic Loss : 2.1155006885528564\n","Exploration Critic Loss : 0.38689106702804565\n","Exploration Model Loss : 3.645689139375463e-05\n","Exploitation Data q-values : -15.031805038452148\n","Exploitation OOD q-values : -13.607599258422852\n","Exploitation CQL Loss : 1.424204707145691\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 487.507789\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 487.50778889656067\n","Exploitation Critic Loss : 0.7861665487289429\n","Exploration Critic Loss : 0.3288589119911194\n","Exploration Model Loss : 2.3509359380113892e-05\n","Exploitation Data q-values : -15.382156372070312\n","Exploitation OOD q-values : -13.910100936889648\n","Exploitation CQL Loss : 1.4720546007156372\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 516.099936\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 516.0999364852905\n","Exploitation Critic Loss : 3.1721534729003906\n","Exploration Critic Loss : 0.4109673798084259\n","Exploration Model Loss : 2.0640143702621572e-05\n","Exploitation Data q-values : -15.462644577026367\n","Exploitation OOD q-values : -13.990198135375977\n","Exploitation CQL Loss : 1.472446084022522\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 547.672305\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 547.6723051071167\n","Exploitation Critic Loss : 0.7724165320396423\n","Exploration Critic Loss : 0.30506426095962524\n","Exploration Model Loss : 4.5065309677738696e-05\n","Exploitation Data q-values : -15.672131538391113\n","Exploitation OOD q-values : -14.22021770477295\n","Exploitation CQL Loss : 1.4519141912460327\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 581.043297\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 581.043297290802\n","Exploitation Critic Loss : 0.785662055015564\n","Exploration Critic Loss : 0.42777901887893677\n","Exploration Model Loss : 0.00012292283645365387\n","Exploitation Data q-values : -15.395221710205078\n","Exploitation OOD q-values : -13.969792366027832\n","Exploitation CQL Loss : 1.4254298210144043\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 615.004864\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 615.0048637390137\n","Exploitation Critic Loss : 2.5142829418182373\n","Exploration Critic Loss : 0.5017542839050293\n","Exploration Model Loss : 7.978534995345399e-05\n","Exploitation Data q-values : -15.635940551757812\n","Exploitation OOD q-values : -14.20240592956543\n","Exploitation CQL Loss : 1.4335336685180664\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 643.853618\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 643.8536179065704\n","Exploitation Critic Loss : 3.380937337875366\n","Exploration Critic Loss : 0.5864362716674805\n","Exploration Model Loss : 7.601377728860825e-05\n","Exploitation Data q-values : -15.428815841674805\n","Exploitation OOD q-values : -14.010183334350586\n","Exploitation CQL Loss : 1.4186331033706665\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 675.484473\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 675.4844725131989\n","Exploitation Critic Loss : 2.181028366088867\n","Exploration Critic Loss : 0.6089767217636108\n","Exploration Model Loss : 0.0001262197911273688\n","Exploitation Data q-values : -15.5654296875\n","Exploitation OOD q-values : -14.129945755004883\n","Exploitation CQL Loss : 1.4354848861694336\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 707.494229\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 707.4942293167114\n","Exploitation Critic Loss : 1.7366104125976562\n","Exploration Critic Loss : 0.4358775019645691\n","Exploration Model Loss : 5.6249868066515774e-05\n","Exploitation Data q-values : -15.637659072875977\n","Exploitation OOD q-values : -14.179891586303711\n","Exploitation CQL Loss : 1.4577689170837402\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 737.354695\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 737.3546946048737\n","Exploitation Critic Loss : 0.8390370607376099\n","Exploration Critic Loss : 0.4916868507862091\n","Exploration Model Loss : 0.00015701106167398393\n","Exploitation Data q-values : -15.591915130615234\n","Exploitation OOD q-values : -14.144308090209961\n","Exploitation CQL Loss : 1.4476062059402466\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 767.624854\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 767.6248543262482\n","Exploitation Critic Loss : 2.6818175315856934\n","Exploration Critic Loss : 0.5393810868263245\n","Exploration Model Loss : 0.00014228939835447818\n","Exploitation Data q-values : -15.901446342468262\n","Exploitation OOD q-values : -14.438562393188477\n","Exploitation CQL Loss : 1.4628854990005493\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 802.929007\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 802.9290068149567\n","Exploitation Critic Loss : 1.5160715579986572\n","Exploration Critic Loss : 0.5083357095718384\n","Exploration Model Loss : 1.8294842448085546e-05\n","Exploitation Data q-values : -16.01225471496582\n","Exploitation OOD q-values : -14.526524543762207\n","Exploitation CQL Loss : 1.4857293367385864\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 835.287509\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 835.2875089645386\n","Exploitation Critic Loss : 0.7946987748146057\n","Exploration Critic Loss : 0.422546923160553\n","Exploration Model Loss : 1.00085787835269e-06\n","Exploitation Data q-values : -15.655101776123047\n","Exploitation OOD q-values : -14.223255157470703\n","Exploitation CQL Loss : 1.431847333908081\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 863.889896\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 863.8898956775665\n","Exploitation Critic Loss : 2.39296555519104\n","Exploration Critic Loss : 0.5423998832702637\n","Exploration Model Loss : 9.606297680875286e-05\n","Exploitation Data q-values : -15.695158004760742\n","Exploitation OOD q-values : -14.223522186279297\n","Exploitation CQL Loss : 1.4716354608535767\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 895.090577\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 895.0905773639679\n","Exploitation Critic Loss : 1.537790298461914\n","Exploration Critic Loss : 0.4686122238636017\n","Exploration Model Loss : 1.9832990801660344e-05\n","Exploitation Data q-values : -15.680810928344727\n","Exploitation OOD q-values : -14.223113059997559\n","Exploitation CQL Loss : 1.4576976299285889\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 926.873045\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 926.8730447292328\n","Exploitation Critic Loss : 3.906895875930786\n","Exploration Critic Loss : 0.6363137364387512\n","Exploration Model Loss : 2.781641887850128e-05\n","Exploitation Data q-values : -15.64871597290039\n","Exploitation OOD q-values : -14.215179443359375\n","Exploitation CQL Loss : 1.4335380792617798\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 960.433531\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 960.4335312843323\n","Exploitation Critic Loss : 3.068840265274048\n","Exploration Critic Loss : 0.599197268486023\n","Exploration Model Loss : 9.994327854201401e-08\n","Exploitation Data q-values : -15.759654998779297\n","Exploitation OOD q-values : -14.295234680175781\n","Exploitation CQL Loss : 1.4644193649291992\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 990.795534\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 990.7955341339111\n","Exploitation Critic Loss : 4.133521556854248\n","Exploration Critic Loss : 0.561190128326416\n","Exploration Model Loss : 3.357789069013961e-07\n","Exploitation Data q-values : -15.74282455444336\n","Exploitation OOD q-values : -14.235258102416992\n","Exploitation CQL Loss : 1.507566213607788\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1022.729146\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1022.729145526886\n","Exploitation Critic Loss : 2.4676589965820312\n","Exploration Critic Loss : 0.49692490696907043\n","Exploration Model Loss : 5.637178901451989e-07\n","Exploitation Data q-values : -15.5559663772583\n","Exploitation OOD q-values : -14.099788665771484\n","Exploitation CQL Loss : 1.456176996231079\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1055.406393\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1055.4063930511475\n","Exploitation Critic Loss : 1.5942103862762451\n","Exploration Critic Loss : 0.48095306754112244\n","Exploration Model Loss : 0.0005023535923101008\n","Exploitation Data q-values : -15.768110275268555\n","Exploitation OOD q-values : -14.330322265625\n","Exploitation CQL Loss : 1.4377872943878174\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1087.357366\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1087.357365846634\n","Exploitation Critic Loss : 2.4916951656341553\n","Exploration Critic Loss : 0.45429810881614685\n","Exploration Model Loss : 1.7051390841515968e-06\n","Exploitation Data q-values : -15.68238639831543\n","Exploitation OOD q-values : -14.210201263427734\n","Exploitation CQL Loss : 1.472184658050537\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1116.313753\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1116.3137526512146\n","Exploitation Critic Loss : 1.611533284187317\n","Exploration Critic Loss : 0.49624544382095337\n","Exploration Model Loss : 6.756031507393345e-05\n","Exploitation Data q-values : -15.887142181396484\n","Exploitation OOD q-values : -14.428352355957031\n","Exploitation CQL Loss : 1.45879065990448\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1149.980310\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1149.9803099632263\n","Exploitation Critic Loss : 0.8626911640167236\n","Exploration Critic Loss : 0.38585540652275085\n","Exploration Model Loss : 2.465306351950858e-06\n","Exploitation Data q-values : -15.684099197387695\n","Exploitation OOD q-values : -14.210622787475586\n","Exploitation CQL Loss : 1.4734768867492676\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1182.150638\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1182.150637626648\n","Exploitation Critic Loss : 1.8175573348999023\n","Exploration Critic Loss : 0.47521933913230896\n","Exploration Model Loss : 5.36192674189806e-05\n","Exploitation Data q-values : -15.6511869430542\n","Exploitation OOD q-values : -14.182443618774414\n","Exploitation CQL Loss : 1.4687427282333374\n","Eval_AverageReturn : -139.0\n","Eval_StdReturn : 29.10326385498047\n","Eval_MaxReturn : -62.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 139.125\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1210.701671\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1210.7016711235046\n","Exploitation Critic Loss : 0.8245212435722351\n","Exploration Critic Loss : 0.3738917410373688\n","Exploration Model Loss : 1.320422615691541e-08\n","Exploitation Data q-values : -16.029422760009766\n","Exploitation OOD q-values : -14.589459419250488\n","Exploitation CQL Loss : 1.4399619102478027\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1241.542822\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1241.5428216457367\n","Exploitation Critic Loss : 2.7845396995544434\n","Exploration Critic Loss : 0.4627695679664612\n","Exploration Model Loss : 6.996342705178904e-08\n","Exploitation Data q-values : -15.94102668762207\n","Exploitation OOD q-values : -14.500814437866211\n","Exploitation CQL Loss : 1.440211296081543\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1272.933614\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1272.9336142539978\n","Exploitation Critic Loss : 1.48957097530365\n","Exploration Critic Loss : 0.3205900490283966\n","Exploration Model Loss : 0.00032425104291178286\n","Exploitation Data q-values : -15.781988143920898\n","Exploitation OOD q-values : -14.261106491088867\n","Exploitation CQL Loss : 1.5208805799484253\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1304.445091\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1304.4450907707214\n","Exploitation Critic Loss : 2.2847607135772705\n","Exploration Critic Loss : 0.4345279335975647\n","Exploration Model Loss : 9.924627192958724e-06\n","Exploitation Data q-values : -15.766401290893555\n","Exploitation OOD q-values : -14.278456687927246\n","Exploitation CQL Loss : 1.4879432916641235\n","Eval_AverageReturn : -141.625\n","Eval_StdReturn : 22.158166885375977\n","Eval_MaxReturn : -83.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 141.75\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1337.950490\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1337.9504902362823\n","Exploitation Critic Loss : 3.8501265048980713\n","Exploration Critic Loss : 0.46667638421058655\n","Exploration Model Loss : 9.481661436439026e-06\n","Exploitation Data q-values : -15.637628555297852\n","Exploitation OOD q-values : -14.13805866241455\n","Exploitation CQL Loss : 1.4995698928833008\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1369.438995\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1369.438994884491\n","Exploitation Critic Loss : 1.8555121421813965\n","Exploration Critic Loss : 0.44380515813827515\n","Exploration Model Loss : 1.1641910759863094e-06\n","Exploitation Data q-values : -16.073165893554688\n","Exploitation OOD q-values : -14.573469161987305\n","Exploitation CQL Loss : 1.499696969985962\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1400.801024\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1400.8010244369507\n","Exploitation Critic Loss : 0.8352571725845337\n","Exploration Critic Loss : 0.33832305669784546\n","Exploration Model Loss : 7.683666325419836e-08\n","Exploitation Data q-values : -15.983871459960938\n","Exploitation OOD q-values : -14.512357711791992\n","Exploitation CQL Loss : 1.4715136289596558\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1429.319646\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1429.3196456432343\n","Exploitation Critic Loss : 0.8643273115158081\n","Exploration Critic Loss : 0.39848512411117554\n","Exploration Model Loss : 4.042719865537947e-06\n","Exploitation Data q-values : -16.008838653564453\n","Exploitation OOD q-values : -14.520339965820312\n","Exploitation CQL Loss : 1.4884984493255615\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1460.952470\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1460.9524703025818\n","Exploitation Critic Loss : 1.570849895477295\n","Exploration Critic Loss : 0.4825197756290436\n","Exploration Model Loss : 2.6269637601217255e-05\n","Exploitation Data q-values : -15.939339637756348\n","Exploitation OOD q-values : -14.44847297668457\n","Exploitation CQL Loss : 1.4908679723739624\n","Eval_AverageReturn : -66.0\n","Eval_StdReturn : 25.221683502197266\n","Eval_MaxReturn : -31.0\n","Eval_MinReturn : -121.0\n","Eval_AverageEpLen : 67.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -150.000000\n","best mean reward -150.000000\n","running time 1493.701509\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -150.0\n","TimeSinceStart : 1493.701509475708\n","Exploitation Critic Loss : 2.8815996646881104\n","Exploration Critic Loss : 0.45048561692237854\n","Exploration Model Loss : 0.00014603057934436947\n","Exploitation Data q-values : -15.765088081359863\n","Exploitation OOD q-values : -14.236600875854492\n","Exploitation CQL Loss : 1.5284876823425293\n","Eval_AverageReturn : -143.57142639160156\n","Eval_StdReturn : 11.878637313842773\n","Eval_MaxReturn : -116.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 143.85714285714286\n","Buffer size : 10001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_expl.py --env_name PointmassMedium-v0 --use_rnd \\\n","--unsupervised_exploration --offline_exploitation --cql_alpha=0.5 \\\n","--exp_name q2_alpha_0.5 --seed 10"]},{"cell_type":"markdown","metadata":{"id":"YvnQflic32Xc"},"source":["## 3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1816140,"status":"ok","timestamp":1669088356250,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"AVoW9l6h325K","outputId":"8b3bbefb-cfd5-41e5-8744-5f77e30a9313"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_medium_dqn_PointmassMedium-v0_22-11-2022_03-09-02 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_medium_dqn_PointmassMedium-v0_22-11-2022_03-09-02\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.003470\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0034699440002441406\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 5.963368\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 5.96336817741394\n","Eval_AverageReturn : -148.0\n","Eval_StdReturn : 4.898979663848877\n","Eval_MaxReturn : -136.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.14285714285714\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -144.307693\n","best mean reward -inf\n","running time 12.320158\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -144.3076934814453\n","TimeSinceStart : 12.320157527923584\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -146.300003\n","best mean reward -inf\n","running time 44.283076\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -146.3000030517578\n","TimeSinceStart : 44.28307580947876\n","Exploitation Critic Loss : 1.9587829113006592\n","Exploration Critic Loss : 0.08979243040084839\n","Exploration Model Loss : 0.34134265780448914\n","Exploitation Data q-values : 1.0452890396118164\n","Exploitation OOD q-values : 2.6544418334960938\n","Exploitation CQL Loss : 1.6091526746749878\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -147.259262\n","best mean reward -inf\n","running time 76.372127\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -147.25926208496094\n","TimeSinceStart : 76.37212681770325\n","Exploitation Critic Loss : 0.7920747399330139\n","Exploration Critic Loss : 0.13634325563907623\n","Exploration Model Loss : 0.0016649464378133416\n","Exploitation Data q-values : 1.875360131263733\n","Exploitation OOD q-values : 3.7472527027130127\n","Exploitation CQL Loss : 1.8718925714492798\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -147.757568\n","best mean reward -inf\n","running time 105.972064\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -147.757568359375\n","TimeSinceStart : 105.97206449508667\n","Exploitation Critic Loss : 8.628120422363281\n","Exploration Critic Loss : 0.21790073812007904\n","Exploration Model Loss : 0.006675056181848049\n","Exploitation Data q-values : 3.53126859664917\n","Exploitation OOD q-values : 5.642716884613037\n","Exploitation CQL Loss : 2.111448287963867\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -148.149994\n","best mean reward -inf\n","running time 140.432223\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -148.14999389648438\n","TimeSinceStart : 140.43222284317017\n","Exploitation Critic Loss : 10.648122787475586\n","Exploration Critic Loss : 0.23761916160583496\n","Exploration Model Loss : 0.0008793544257059693\n","Exploitation Data q-values : 5.87888240814209\n","Exploitation OOD q-values : 8.335470199584961\n","Exploitation CQL Loss : 2.4565882682800293\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -148.425537\n","best mean reward -inf\n","running time 169.567915\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -148.425537109375\n","TimeSinceStart : 169.56791472434998\n","Exploitation Critic Loss : 13.656243324279785\n","Exploration Critic Loss : 0.22687053680419922\n","Exploration Model Loss : 0.0009906489867717028\n","Exploitation Data q-values : 8.684808731079102\n","Exploitation OOD q-values : 11.373895645141602\n","Exploitation CQL Loss : 2.689086675643921\n","Eval_AverageReturn : -139.75\n","Eval_StdReturn : 27.11895179748535\n","Eval_MaxReturn : -68.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 139.875\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -148.603775\n","best mean reward -inf\n","running time 201.410293\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -148.60377502441406\n","TimeSinceStart : 201.4102931022644\n","Exploitation Critic Loss : 12.145318984985352\n","Exploration Critic Loss : 0.28848496079444885\n","Exploration Model Loss : 0.0007531648152507842\n","Exploitation Data q-values : 18.161407470703125\n","Exploitation OOD q-values : 21.733787536621094\n","Exploitation CQL Loss : 3.5723793506622314\n","Eval_AverageReturn : -76.92308044433594\n","Eval_StdReturn : 34.86559295654297\n","Eval_MaxReturn : -36.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 77.84615384615384\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -148.766663\n","best mean reward -inf\n","running time 234.976950\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -148.76666259765625\n","TimeSinceStart : 234.9769504070282\n","Exploitation Critic Loss : 20.158292770385742\n","Exploration Critic Loss : 0.27862173318862915\n","Exploration Model Loss : 0.0004105298430658877\n","Exploitation Data q-values : 30.290729522705078\n","Exploitation OOD q-values : 34.57574462890625\n","Exploitation CQL Loss : 4.285015106201172\n","Eval_AverageReturn : -144.85714721679688\n","Eval_StdReturn : 12.597375869750977\n","Eval_MaxReturn : -114.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 145.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -148.895523\n","best mean reward -inf\n","running time 263.557006\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -148.89552307128906\n","TimeSinceStart : 263.55700635910034\n","Exploitation Critic Loss : 66.04451751708984\n","Exploration Critic Loss : 0.21620436012744904\n","Exploration Model Loss : 0.00031282228883355856\n","Exploitation Data q-values : 49.629859924316406\n","Exploitation OOD q-values : 55.032249450683594\n","Exploitation CQL Loss : 5.40238618850708\n","Eval_AverageReturn : -40.959999084472656\n","Eval_StdReturn : 12.950613975524902\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -70.0\n","Eval_AverageEpLen : 41.96\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -148.986298\n","best mean reward -inf\n","running time 297.701934\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -148.98629760742188\n","TimeSinceStart : 297.70193433761597\n","Exploitation Critic Loss : 93.71431732177734\n","Exploration Critic Loss : 0.2932848632335663\n","Exploration Model Loss : 0.0002587101480457932\n","Exploitation Data q-values : 50.405372619628906\n","Exploitation OOD q-values : 55.520042419433594\n","Exploitation CQL Loss : 5.114669322967529\n","Eval_AverageReturn : -38.653846740722656\n","Eval_StdReturn : 15.681793212890625\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -71.0\n","Eval_AverageEpLen : 39.65384615384615\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -149.074997\n","best mean reward -inf\n","running time 333.669364\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -149.0749969482422\n","TimeSinceStart : 333.6693642139435\n","Exploitation Critic Loss : 77.4240951538086\n","Exploration Critic Loss : 0.2538784146308899\n","Exploration Model Loss : 0.0001795757852960378\n","Exploitation Data q-values : 65.21038055419922\n","Exploitation OOD q-values : 70.73030090332031\n","Exploitation CQL Loss : 5.5199151039123535\n","Eval_AverageReturn : -35.53571319580078\n","Eval_StdReturn : 10.574503898620605\n","Eval_MaxReturn : -19.0\n","Eval_MinReturn : -56.0\n","Eval_AverageEpLen : 36.535714285714285\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -149.149429\n","best mean reward -inf\n","running time 367.464255\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -149.14942932128906\n","TimeSinceStart : 367.4642548561096\n","Exploitation Critic Loss : 80.7164535522461\n","Exploration Critic Loss : 0.2996646761894226\n","Exploration Model Loss : 0.00010670635674614459\n","Exploitation Data q-values : 64.07803344726562\n","Exploitation OOD q-values : 68.97406005859375\n","Exploitation CQL Loss : 4.896017074584961\n","Eval_AverageReturn : -39.439998626708984\n","Eval_StdReturn : 16.97075080871582\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -73.0\n","Eval_AverageEpLen : 40.44\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -149.204300\n","best mean reward -inf\n","running time 402.319767\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -149.2042999267578\n","TimeSinceStart : 402.319766998291\n","Exploitation Critic Loss : 19.705888748168945\n","Exploration Critic Loss : 0.11071380972862244\n","Exploration Model Loss : 7.784638000885025e-05\n","Exploitation Data q-values : 67.31169891357422\n","Exploitation OOD q-values : 71.78080749511719\n","Exploitation CQL Loss : 4.469099998474121\n","Eval_AverageReturn : -57.33333206176758\n","Eval_StdReturn : 25.486379623413086\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -125.0\n","Eval_AverageEpLen : 58.333333333333336\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -149.259995\n","best mean reward -inf\n","running time 433.186002\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -149.25999450683594\n","TimeSinceStart : 433.1860017776489\n","Exploitation Critic Loss : 139.66949462890625\n","Exploration Critic Loss : 0.12478771805763245\n","Exploration Model Loss : 4.3890202505281195e-05\n","Exploitation Data q-values : 72.45294189453125\n","Exploitation OOD q-values : 76.78901672363281\n","Exploitation CQL Loss : 4.336071968078613\n","Eval_AverageReturn : -77.15384674072266\n","Eval_StdReturn : 33.592227935791016\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -128.0\n","Eval_AverageEpLen : 78.15384615384616\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -148.729996\n","best mean reward -148.729996\n","running time 466.707249\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -148.72999572753906\n","Train_BestReturn : -148.72999572753906\n","TimeSinceStart : 466.7072489261627\n","Exploitation Critic Loss : 9.197423934936523\n","Exploration Critic Loss : 0.0372452549636364\n","Exploration Model Loss : 2.818979373842012e-05\n","Exploitation Data q-values : 67.61790466308594\n","Exploitation OOD q-values : 71.74568176269531\n","Exploitation CQL Loss : 4.127779006958008\n","Eval_AverageReturn : -94.45454406738281\n","Eval_StdReturn : 36.03120803833008\n","Eval_MaxReturn : -41.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 95.18181818181819\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -148.979996\n","best mean reward -148.729996\n","running time 498.807028\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -148.97999572753906\n","Train_BestReturn : -148.72999572753906\n","TimeSinceStart : 498.8070282936096\n","Exploitation Critic Loss : 41.696346282958984\n","Exploration Critic Loss : 0.08442294597625732\n","Exploration Model Loss : 2.0362374925753102e-05\n","Exploitation Data q-values : 69.55437469482422\n","Exploitation OOD q-values : 72.98787689208984\n","Exploitation CQL Loss : 3.433502435684204\n","Eval_AverageReturn : -53.73684310913086\n","Eval_StdReturn : 25.606006622314453\n","Eval_MaxReturn : -20.0\n","Eval_MinReturn : -108.0\n","Eval_AverageEpLen : 54.73684210526316\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -146.080002\n","best mean reward -146.080002\n","running time 530.306462\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -146.0800018310547\n","Train_BestReturn : -146.0800018310547\n","TimeSinceStart : 530.3064615726471\n","Exploitation Critic Loss : 69.19568634033203\n","Exploration Critic Loss : 0.22055745124816895\n","Exploration Model Loss : 5.021960168960504e-05\n","Exploitation Data q-values : 62.66398620605469\n","Exploitation OOD q-values : 65.51744079589844\n","Exploitation CQL Loss : 2.8534584045410156\n","Eval_AverageReturn : -62.9375\n","Eval_StdReturn : 34.51715087890625\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 63.875\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -134.789993\n","best mean reward -134.789993\n","running time 567.572921\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -134.7899932861328\n","Train_BestReturn : -134.7899932861328\n","TimeSinceStart : 567.5729205608368\n","Exploitation Critic Loss : 11.347275733947754\n","Exploration Critic Loss : 0.3530336916446686\n","Exploration Model Loss : 3.879728683386929e-05\n","Exploitation Data q-values : 59.86171340942383\n","Exploitation OOD q-values : 62.35257339477539\n","Exploitation CQL Loss : 2.4908571243286133\n","Eval_AverageReturn : -34.27586364746094\n","Eval_StdReturn : 11.159955024719238\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -64.0\n","Eval_AverageEpLen : 35.275862068965516\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -119.809998\n","best mean reward -119.809998\n","running time 602.279010\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -119.80999755859375\n","Train_BestReturn : -119.80999755859375\n","TimeSinceStart : 602.2790095806122\n","Exploitation Critic Loss : 19.78990364074707\n","Exploration Critic Loss : 0.2559197247028351\n","Exploration Model Loss : 0.0004183887504041195\n","Exploitation Data q-values : 56.91967010498047\n","Exploitation OOD q-values : 59.2586669921875\n","Exploitation CQL Loss : 2.3389956951141357\n","Eval_AverageReturn : -33.03333282470703\n","Eval_StdReturn : 8.272377014160156\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -48.0\n","Eval_AverageEpLen : 34.03333333333333\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -92.919998\n","best mean reward -92.919998\n","running time 637.937970\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -92.91999816894531\n","Train_BestReturn : -92.91999816894531\n","TimeSinceStart : 637.9379703998566\n","Exploitation Critic Loss : 32.785518646240234\n","Exploration Critic Loss : 0.4370196461677551\n","Exploration Model Loss : 7.686504250159487e-05\n","Exploitation Data q-values : 52.3089714050293\n","Exploitation OOD q-values : 54.41595458984375\n","Exploitation CQL Loss : 2.1069788932800293\n","Eval_AverageReturn : -31.74193572998047\n","Eval_StdReturn : 9.999895095825195\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -57.0\n","Eval_AverageEpLen : 32.74193548387097\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -61.450001\n","best mean reward -61.450001\n","running time 675.629952\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -61.45000076293945\n","Train_BestReturn : -61.45000076293945\n","TimeSinceStart : 675.6299517154694\n","Exploitation Critic Loss : 25.43695068359375\n","Exploration Critic Loss : 0.3598983585834503\n","Exploration Model Loss : 0.00010720823775045574\n","Exploitation Data q-values : 48.993507385253906\n","Exploitation OOD q-values : 51.026611328125\n","Exploitation CQL Loss : 2.033101797103882\n","Eval_AverageReturn : -33.400001525878906\n","Eval_StdReturn : 12.376321792602539\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -63.0\n","Eval_AverageEpLen : 34.4\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -39.689999\n","best mean reward -39.689999\n","running time 714.478547\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -39.689998626708984\n","Train_BestReturn : -39.689998626708984\n","TimeSinceStart : 714.4785468578339\n","Exploitation Critic Loss : 9.42318344116211\n","Exploration Critic Loss : 0.35892367362976074\n","Exploration Model Loss : 0.00011714128777384758\n","Exploitation Data q-values : 45.350616455078125\n","Exploitation OOD q-values : 47.265438079833984\n","Exploitation CQL Loss : 1.9148164987564087\n","Eval_AverageReturn : -29.75757598876953\n","Eval_StdReturn : 9.779478073120117\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -54.0\n","Eval_AverageEpLen : 30.757575757575758\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -33.279999\n","best mean reward -33.279999\n","running time 750.156173\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -33.279998779296875\n","Train_BestReturn : -33.279998779296875\n","TimeSinceStart : 750.1561732292175\n","Exploitation Critic Loss : 8.391825675964355\n","Exploration Critic Loss : 0.5560309290885925\n","Exploration Model Loss : 2.170244806620758e-05\n","Exploitation Data q-values : 44.36321258544922\n","Exploitation OOD q-values : 46.389671325683594\n","Exploitation CQL Loss : 2.0264570713043213\n","Eval_AverageReturn : -25.128204345703125\n","Eval_StdReturn : 5.520030975341797\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -41.0\n","Eval_AverageEpLen : 26.128205128205128\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -31.430000\n","best mean reward -31.430000\n","running time 793.386360\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -31.43000030517578\n","Train_BestReturn : -31.43000030517578\n","TimeSinceStart : 793.3863596916199\n","Exploitation Critic Loss : 3.940728187561035\n","Exploration Critic Loss : 0.4991503357887268\n","Exploration Model Loss : 6.59317011013627e-05\n","Exploitation Data q-values : 40.638938903808594\n","Exploitation OOD q-values : 42.62799072265625\n","Exploitation CQL Loss : 1.9890480041503906\n","Eval_AverageReturn : -24.325000762939453\n","Eval_StdReturn : 6.174088954925537\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -48.0\n","Eval_AverageEpLen : 25.325\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -29.719999\n","best mean reward -29.719999\n","running time 834.839880\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -29.719999313354492\n","Train_BestReturn : -29.719999313354492\n","TimeSinceStart : 834.839879989624\n","Exploitation Critic Loss : 3.985684633255005\n","Exploration Critic Loss : 0.42915263772010803\n","Exploration Model Loss : 0.00011854818149004132\n","Exploitation Data q-values : 41.093101501464844\n","Exploitation OOD q-values : 43.029842376708984\n","Exploitation CQL Loss : 1.9367432594299316\n","Eval_AverageReturn : -22.65116310119629\n","Eval_StdReturn : 5.043456077575684\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -41.0\n","Eval_AverageEpLen : 23.651162790697676\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -28.780001\n","best mean reward -28.780001\n","running time 875.489604\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -28.780000686645508\n","Train_BestReturn : -28.780000686645508\n","TimeSinceStart : 875.489604473114\n","Exploitation Critic Loss : 4.937994003295898\n","Exploration Critic Loss : 0.42917969822883606\n","Exploration Model Loss : 0.0002377755008637905\n","Exploitation Data q-values : 40.05632781982422\n","Exploitation OOD q-values : 42.355735778808594\n","Exploitation CQL Loss : 2.2994091510772705\n","Eval_AverageReturn : -22.488372802734375\n","Eval_StdReturn : 4.5717644691467285\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -33.0\n","Eval_AverageEpLen : 23.488372093023255\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -28.090000\n","best mean reward -28.090000\n","running time 913.600828\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -28.09000015258789\n","Train_BestReturn : -28.09000015258789\n","TimeSinceStart : 913.6008281707764\n","Exploitation Critic Loss : 4.214151859283447\n","Exploration Critic Loss : 0.39886486530303955\n","Exploration Model Loss : 6.353270873660222e-05\n","Exploitation Data q-values : 42.382057189941406\n","Exploitation OOD q-values : 44.65957260131836\n","Exploitation CQL Loss : 2.277512550354004\n","Eval_AverageReturn : -22.022727966308594\n","Eval_StdReturn : 3.97999906539917\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -29.0\n","Eval_AverageEpLen : 23.022727272727273\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -28.209999\n","best mean reward -28.090000\n","running time 954.788796\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -28.209999084472656\n","Train_BestReturn : -28.09000015258789\n","TimeSinceStart : 954.7887961864471\n","Exploitation Critic Loss : 4.4737749099731445\n","Exploration Critic Loss : 0.3187430500984192\n","Exploration Model Loss : 4.6440876758424565e-05\n","Exploitation Data q-values : 40.983890533447266\n","Exploitation OOD q-values : 43.152801513671875\n","Exploitation CQL Loss : 2.168910264968872\n","Eval_AverageReturn : -22.534883499145508\n","Eval_StdReturn : 4.244552135467529\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -34.0\n","Eval_AverageEpLen : 23.53488372093023\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -27.530001\n","best mean reward -27.530001\n","running time 998.536606\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -27.530000686645508\n","Train_BestReturn : -27.530000686645508\n","TimeSinceStart : 998.5366058349609\n","Exploitation Critic Loss : 6.849158763885498\n","Exploration Critic Loss : 0.2880735695362091\n","Exploration Model Loss : 9.642022632760927e-05\n","Exploitation Data q-values : 43.28187561035156\n","Exploitation OOD q-values : 45.40291213989258\n","Exploitation CQL Loss : 2.121039390563965\n","Eval_AverageReturn : -22.418603897094727\n","Eval_StdReturn : 3.761695146560669\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -31.0\n","Eval_AverageEpLen : 23.41860465116279\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -27.299999\n","best mean reward -27.299999\n","running time 1039.181222\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -27.299999237060547\n","Train_BestReturn : -27.299999237060547\n","TimeSinceStart : 1039.181221961975\n","Exploitation Critic Loss : 6.49952507019043\n","Exploration Critic Loss : 0.2086617648601532\n","Exploration Model Loss : 6.240381935640471e-07\n","Exploitation Data q-values : 46.457435607910156\n","Exploitation OOD q-values : 48.86389923095703\n","Exploitation CQL Loss : 2.4064629077911377\n","Eval_AverageReturn : -23.414634704589844\n","Eval_StdReturn : 4.968071937561035\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -39.0\n","Eval_AverageEpLen : 24.414634146341463\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -26.990000\n","best mean reward -26.990000\n","running time 1076.226275\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -26.989999771118164\n","Train_BestReturn : -26.989999771118164\n","TimeSinceStart : 1076.2262749671936\n","Exploitation Critic Loss : 18.63215446472168\n","Exploration Critic Loss : 0.13193899393081665\n","Exploration Model Loss : 1.0416982831884525e-06\n","Exploitation Data q-values : 44.931976318359375\n","Exploitation OOD q-values : 46.99024963378906\n","Exploitation CQL Loss : 2.0582706928253174\n","Eval_AverageReturn : -21.021739959716797\n","Eval_StdReturn : 2.5833003520965576\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -26.0\n","Eval_AverageEpLen : 22.02173913043478\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -28.160000\n","best mean reward -26.990000\n","running time 1116.556727\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -28.15999984741211\n","Train_BestReturn : -26.989999771118164\n","TimeSinceStart : 1116.556726694107\n","Exploitation Critic Loss : 4.524887561798096\n","Exploration Critic Loss : 0.07460426539182663\n","Exploration Model Loss : 0.0005539142875932157\n","Exploitation Data q-values : 46.087242126464844\n","Exploitation OOD q-values : 48.178279876708984\n","Exploitation CQL Loss : 2.0910396575927734\n","Eval_AverageReturn : -21.600000381469727\n","Eval_StdReturn : 3.74996280670166\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -29.0\n","Eval_AverageEpLen : 22.6\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -28.160000\n","best mean reward -26.990000\n","running time 1157.409427\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -28.15999984741211\n","Train_BestReturn : -26.989999771118164\n","TimeSinceStart : 1157.409426689148\n","Exploitation Critic Loss : 17.512039184570312\n","Exploration Critic Loss : 0.08759087324142456\n","Exploration Model Loss : 1.0204847967543174e-05\n","Exploitation Data q-values : 47.62227249145508\n","Exploitation OOD q-values : 49.686283111572266\n","Exploitation CQL Loss : 2.064012289047241\n","Eval_AverageReturn : -20.913043975830078\n","Eval_StdReturn : 3.2358460426330566\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -31.0\n","Eval_AverageEpLen : 21.91304347826087\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -27.950001\n","best mean reward -26.990000\n","running time 1199.882141\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -27.950000762939453\n","Train_BestReturn : -26.989999771118164\n","TimeSinceStart : 1199.8821408748627\n","Exploitation Critic Loss : 4.764707088470459\n","Exploration Critic Loss : 0.0665174052119255\n","Exploration Model Loss : 5.926849411252988e-08\n","Exploitation Data q-values : 48.139156341552734\n","Exploitation OOD q-values : 50.01154327392578\n","Exploitation CQL Loss : 1.8723864555358887\n","Eval_AverageReturn : -22.395349502563477\n","Eval_StdReturn : 4.0008111000061035\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -32.0\n","Eval_AverageEpLen : 23.3953488372093\n","Buffer size : 35001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -27.719999\n","best mean reward -26.990000\n","running time 1238.836117\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -27.719999313354492\n","Train_BestReturn : -26.989999771118164\n","TimeSinceStart : 1238.8361172676086\n","Exploitation Critic Loss : 4.300831317901611\n","Exploration Critic Loss : 0.07424939423799515\n","Exploration Model Loss : 4.602774481554661e-07\n","Exploitation Data q-values : 49.49369430541992\n","Exploitation OOD q-values : 51.656036376953125\n","Exploitation CQL Loss : 2.162346601486206\n","Eval_AverageReturn : -20.978260040283203\n","Eval_StdReturn : 3.2869250774383545\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -29.0\n","Eval_AverageEpLen : 21.97826086956522\n","Buffer size : 36001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -28.110001\n","best mean reward -26.990000\n","running time 1279.557778\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -28.110000610351562\n","Train_BestReturn : -26.989999771118164\n","TimeSinceStart : 1279.5577776432037\n","Exploitation Critic Loss : 17.762895584106445\n","Exploration Critic Loss : 0.11441437900066376\n","Exploration Model Loss : 7.375638766404791e-09\n","Exploitation Data q-values : 50.654273986816406\n","Exploitation OOD q-values : 52.57181930541992\n","Exploitation CQL Loss : 1.9175474643707275\n","Eval_AverageReturn : -21.0\n","Eval_StdReturn : 3.5077555179595947\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -32.0\n","Eval_AverageEpLen : 22.0\n","Buffer size : 37001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -27.879999\n","best mean reward -26.990000\n","running time 1320.293697\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -27.8799991607666\n","Train_BestReturn : -26.989999771118164\n","TimeSinceStart : 1320.2936971187592\n","Exploitation Critic Loss : 6.861178398132324\n","Exploration Critic Loss : 0.11825583130121231\n","Exploration Model Loss : 2.547725489421282e-05\n","Exploitation Data q-values : 50.65190887451172\n","Exploitation OOD q-values : 52.71246337890625\n","Exploitation CQL Loss : 2.060549736022949\n","Eval_AverageReturn : -21.53333282470703\n","Eval_StdReturn : 3.5814335346221924\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -30.0\n","Eval_AverageEpLen : 22.533333333333335\n","Buffer size : 38001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -27.139999\n","best mean reward -26.990000\n","running time 1361.435647\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -27.139999389648438\n","Train_BestReturn : -26.989999771118164\n","TimeSinceStart : 1361.4356472492218\n","Exploitation Critic Loss : 5.327826499938965\n","Exploration Critic Loss : 0.10128244757652283\n","Exploration Model Loss : 8.800702744338196e-06\n","Exploitation Data q-values : 49.93746566772461\n","Exploitation OOD q-values : 51.716739654541016\n","Exploitation CQL Loss : 1.7792737483978271\n","Eval_AverageReturn : -22.488372802734375\n","Eval_StdReturn : 3.829943895339966\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -29.0\n","Eval_AverageEpLen : 23.488372093023255\n","Buffer size : 39001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -27.020000\n","best mean reward -26.990000\n","running time 1398.785244\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -27.020000457763672\n","Train_BestReturn : -26.989999771118164\n","TimeSinceStart : 1398.7852435112\n","Exploitation Critic Loss : 4.8490705490112305\n","Exploration Critic Loss : 0.08784233033657074\n","Exploration Model Loss : 4.5272172428667545e-06\n","Exploitation Data q-values : 50.03092956542969\n","Exploitation OOD q-values : 51.70663070678711\n","Exploitation CQL Loss : 1.6757012605667114\n","Eval_AverageReturn : -21.288888931274414\n","Eval_StdReturn : 3.249767541885376\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -32.0\n","Eval_AverageEpLen : 22.288888888888888\n","Buffer size : 40001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -27.850000\n","best mean reward -26.990000\n","running time 1442.174702\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -27.850000381469727\n","Train_BestReturn : -26.989999771118164\n","TimeSinceStart : 1442.1747016906738\n","Exploitation Critic Loss : 3.942190170288086\n","Exploration Critic Loss : 0.08277175575494766\n","Exploration Model Loss : 0.00014212603855412453\n","Exploitation Data q-values : 50.64909362792969\n","Exploitation OOD q-values : 52.44071960449219\n","Exploitation CQL Loss : 1.7916263341903687\n","Eval_AverageReturn : -21.727272033691406\n","Eval_StdReturn : 3.725609540939331\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -31.0\n","Eval_AverageEpLen : 22.727272727272727\n","Buffer size : 41001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -27.530001\n","best mean reward -26.990000\n","running time 1483.147028\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -27.530000686645508\n","Train_BestReturn : -26.989999771118164\n","TimeSinceStart : 1483.1470284461975\n","Exploitation Critic Loss : 4.611841678619385\n","Exploration Critic Loss : 0.08345738053321838\n","Exploration Model Loss : 2.032488964687218e-06\n","Exploitation Data q-values : 49.072410583496094\n","Exploitation OOD q-values : 50.95513916015625\n","Exploitation CQL Loss : 1.8827285766601562\n","Eval_AverageReturn : -20.89130401611328\n","Eval_StdReturn : 3.672626256942749\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -29.0\n","Eval_AverageEpLen : 21.891304347826086\n","Buffer size : 42001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -26.700001\n","best mean reward -26.700001\n","running time 1524.962171\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -26.700000762939453\n","Train_BestReturn : -26.700000762939453\n","TimeSinceStart : 1524.962170600891\n","Exploitation Critic Loss : 5.789435386657715\n","Exploration Critic Loss : 0.11225331574678421\n","Exploration Model Loss : 1.958739449037239e-05\n","Exploitation Data q-values : 51.15660095214844\n","Exploitation OOD q-values : 52.81489562988281\n","Exploitation CQL Loss : 1.658294439315796\n","Eval_AverageReturn : -21.622222900390625\n","Eval_StdReturn : 3.68353009223938\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -29.0\n","Eval_AverageEpLen : 22.622222222222224\n","Buffer size : 43001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -26.240000\n","best mean reward -26.240000\n","running time 1563.592256\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -26.239999771118164\n","Train_BestReturn : -26.239999771118164\n","TimeSinceStart : 1563.5922558307648\n","Exploitation Critic Loss : 3.86686635017395\n","Exploration Critic Loss : 0.07210710644721985\n","Exploration Model Loss : 7.72567773310584e-07\n","Exploitation Data q-values : 49.30872344970703\n","Exploitation OOD q-values : 50.8782958984375\n","Exploitation CQL Loss : 1.5695714950561523\n","Eval_AverageReturn : -21.266666412353516\n","Eval_StdReturn : 3.791217803955078\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -32.0\n","Eval_AverageEpLen : 22.266666666666666\n","Buffer size : 44001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -25.990000\n","best mean reward -25.990000\n","running time 1605.578191\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -25.989999771118164\n","Train_BestReturn : -25.989999771118164\n","TimeSinceStart : 1605.5781905651093\n","Exploitation Critic Loss : 3.9209933280944824\n","Exploration Critic Loss : 0.09322500228881836\n","Exploration Model Loss : 5.350611445464892e-06\n","Exploitation Data q-values : 51.118934631347656\n","Exploitation OOD q-values : 52.98252868652344\n","Exploitation CQL Loss : 1.8635984659194946\n","Eval_AverageReturn : -20.978260040283203\n","Eval_StdReturn : 3.3718085289001465\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -29.0\n","Eval_AverageEpLen : 21.97826086956522\n","Buffer size : 45001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -26.520000\n","best mean reward -25.990000\n","running time 1650.887827\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -26.520000457763672\n","Train_BestReturn : -25.989999771118164\n","TimeSinceStart : 1650.8878271579742\n","Exploitation Critic Loss : 13.427800178527832\n","Exploration Critic Loss : 0.11281294375658035\n","Exploration Model Loss : 2.0379524357849732e-05\n","Exploitation Data q-values : 52.33705520629883\n","Exploitation OOD q-values : 53.95735168457031\n","Exploitation CQL Loss : 1.620294451713562\n","Eval_AverageReturn : -21.55555534362793\n","Eval_StdReturn : 4.469926834106445\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -33.0\n","Eval_AverageEpLen : 22.555555555555557\n","Buffer size : 46001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -26.790001\n","best mean reward -25.990000\n","running time 1692.501992\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -26.790000915527344\n","Train_BestReturn : -25.989999771118164\n","TimeSinceStart : 1692.5019915103912\n","Exploitation Critic Loss : 14.208969116210938\n","Exploration Critic Loss : 0.1190643236041069\n","Exploration Model Loss : 8.457882358925417e-06\n","Exploitation Data q-values : 52.931427001953125\n","Exploitation OOD q-values : 54.55585479736328\n","Exploitation CQL Loss : 1.6244280338287354\n","Eval_AverageReturn : -21.22222137451172\n","Eval_StdReturn : 3.5458202362060547\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -30.0\n","Eval_AverageEpLen : 22.22222222222222\n","Buffer size : 47001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -27.260000\n","best mean reward -25.990000\n","running time 1732.584925\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -27.260000228881836\n","Train_BestReturn : -25.989999771118164\n","TimeSinceStart : 1732.5849254131317\n","Exploitation Critic Loss : 3.662952423095703\n","Exploration Critic Loss : 0.07738974690437317\n","Exploration Model Loss : 2.898400907724863e-06\n","Exploitation Data q-values : 53.87563705444336\n","Exploitation OOD q-values : 55.480308532714844\n","Exploitation CQL Loss : 1.6046744585037231\n","Eval_AverageReturn : -21.909090042114258\n","Eval_StdReturn : 3.7828452587127686\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -32.0\n","Eval_AverageEpLen : 22.90909090909091\n","Buffer size : 48001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -27.010000\n","best mean reward -25.990000\n","running time 1772.313373\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -27.010000228881836\n","Train_BestReturn : -25.989999771118164\n","TimeSinceStart : 1772.3133733272552\n","Exploitation Critic Loss : 3.549637794494629\n","Exploration Critic Loss : 0.07310223579406738\n","Exploration Model Loss : 9.293157927459106e-05\n","Exploitation Data q-values : 54.08130645751953\n","Exploitation OOD q-values : 55.626731872558594\n","Exploitation CQL Loss : 1.5454297065734863\n","Eval_AverageReturn : -21.086956024169922\n","Eval_StdReturn : 3.275907516479492\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -28.0\n","Eval_AverageEpLen : 22.08695652173913\n","Buffer size : 49001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_expl.py --env_name PointmassMedium-v0 --use_rnd \\\n","--num_exploration_steps=20000 --cql_alpha=0.0 --exp_name q3_medium_dqn --seed 10 \\\n","--exploit_rew_shift 1 --exploit_rew_scale 100"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1332069,"status":"ok","timestamp":1669090477589,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"aa8wwg8o4tqG","outputId":"2a0df677-e266-4900-e84a-bdb9b895c3e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_medium_cql_PointmassMedium-v0_22-11-2022_03-52-27 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_medium_cql_PointmassMedium-v0_22-11-2022_03-52-27\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002165\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.002164602279663086\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 8.167962\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 8.167961597442627\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 14.723670\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 14.72367000579834\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 47.676206\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 47.67620635032654\n","Exploitation Critic Loss : 1.5702391862869263\n","Exploration Critic Loss : 0.15648138523101807\n","Exploration Model Loss : 0.05585671588778496\n","Exploitation Data q-values : 0.3534201681613922\n","Exploitation OOD q-values : 1.9070578813552856\n","Exploitation CQL Loss : 1.5536377429962158\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -149.307693\n","best mean reward -inf\n","running time 76.934452\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -149.3076934814453\n","TimeSinceStart : 76.93445229530334\n","Exploitation Critic Loss : 1.7331547737121582\n","Exploration Critic Loss : 0.19606412947177887\n","Exploration Model Loss : 1.9955641031265259\n","Exploitation Data q-values : 0.9065223932266235\n","Exploitation OOD q-values : 2.566342830657959\n","Exploitation CQL Loss : 1.6598206758499146\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -148.696976\n","best mean reward -inf\n","running time 109.710611\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -148.6969757080078\n","TimeSinceStart : 109.71061110496521\n","Exploitation Critic Loss : 1.9159866571426392\n","Exploration Critic Loss : 0.14368195831775665\n","Exploration Model Loss : 0.028493402525782585\n","Exploitation Data q-values : 1.950470209121704\n","Exploitation OOD q-values : 3.5215346813201904\n","Exploitation CQL Loss : 1.5710645914077759\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -148.925003\n","best mean reward -inf\n","running time 141.386948\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -148.9250030517578\n","TimeSinceStart : 141.38694834709167\n","Exploitation Critic Loss : 2.775667667388916\n","Exploration Critic Loss : 0.278961181640625\n","Exploration Model Loss : 0.0014352265279740095\n","Exploitation Data q-values : 2.577711582183838\n","Exploitation OOD q-values : 4.133792400360107\n","Exploitation CQL Loss : 1.5560805797576904\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -149.065216\n","best mean reward -inf\n","running time 173.447551\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -149.06521606445312\n","TimeSinceStart : 173.44755101203918\n","Exploitation Critic Loss : 7.4724273681640625\n","Exploration Critic Loss : 0.3936464190483093\n","Exploration Model Loss : 0.0009071307722479105\n","Exploitation Data q-values : 3.946342706680298\n","Exploitation OOD q-values : 5.692906856536865\n","Exploitation CQL Loss : 1.7465643882751465\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -149.188675\n","best mean reward -inf\n","running time 205.093044\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -149.1886749267578\n","TimeSinceStart : 205.0930438041687\n","Exploitation Critic Loss : 2.5139145851135254\n","Exploration Critic Loss : 0.2231239229440689\n","Exploration Model Loss : 0.0006004393217153847\n","Exploitation Data q-values : 2.3635506629943848\n","Exploitation OOD q-values : 3.7988648414611816\n","Exploitation CQL Loss : 1.4353142976760864\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -149.283340\n","best mean reward -inf\n","running time 233.572811\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -149.28334045410156\n","TimeSinceStart : 233.57281112670898\n","Exploitation Critic Loss : 3.522559404373169\n","Exploration Critic Loss : 0.3502282500267029\n","Exploration Model Loss : 0.0005072320927865803\n","Exploitation Data q-values : 4.1230244636535645\n","Exploitation OOD q-values : 5.700087547302246\n","Exploitation CQL Loss : 1.5770630836486816\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -149.348480\n","best mean reward -inf\n","running time 265.232246\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -149.34848022460938\n","TimeSinceStart : 265.2322459220886\n","Exploitation Critic Loss : 3.5477147102355957\n","Exploration Critic Loss : 0.35885047912597656\n","Exploration Model Loss : 0.0004220384289510548\n","Exploitation Data q-values : 3.704113245010376\n","Exploitation OOD q-values : 5.208071708679199\n","Exploitation CQL Loss : 1.503959059715271\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -149.410965\n","best mean reward -inf\n","running time 293.633241\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -149.4109649658203\n","TimeSinceStart : 293.63324069976807\n","Exploitation Critic Loss : 4.927560806274414\n","Exploration Critic Loss : 0.2645452320575714\n","Exploration Model Loss : 0.00022444040223490447\n","Exploitation Data q-values : 3.833007335662842\n","Exploitation OOD q-values : 5.379205703735352\n","Exploitation CQL Loss : 1.5461992025375366\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -149.462494\n","best mean reward -inf\n","running time 325.298226\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -149.46249389648438\n","TimeSinceStart : 325.29822635650635\n","Exploitation Critic Loss : 2.1612298488616943\n","Exploration Critic Loss : 0.19018757343292236\n","Exploration Model Loss : 0.00015877021360211074\n","Exploitation Data q-values : 3.859795331954956\n","Exploitation OOD q-values : 5.260849952697754\n","Exploitation CQL Loss : 1.4010543823242188\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -149.500000\n","best mean reward -inf\n","running time 356.457415\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -149.5\n","TimeSinceStart : 356.45741510391235\n","Exploitation Critic Loss : 1.7171798944473267\n","Exploration Critic Loss : 0.1591806560754776\n","Exploration Model Loss : 0.00014161832223180681\n","Exploitation Data q-values : 3.3825325965881348\n","Exploitation OOD q-values : 4.841281890869141\n","Exploitation CQL Loss : 1.4587490558624268\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -149.537628\n","best mean reward -inf\n","running time 388.667990\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -149.53762817382812\n","TimeSinceStart : 388.6679904460907\n","Exploitation Critic Loss : 9.64990234375\n","Exploration Critic Loss : 0.9011781215667725\n","Exploration Model Loss : 6.298794323811308e-05\n","Exploitation Data q-values : 4.53490686416626\n","Exploitation OOD q-values : 5.949393272399902\n","Exploitation CQL Loss : 1.4144866466522217\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -149.570007\n","best mean reward -inf\n","running time 420.323962\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -149.57000732421875\n","TimeSinceStart : 420.32396245002747\n","Exploitation Critic Loss : 2.1935362815856934\n","Exploration Critic Loss : 0.5091655850410461\n","Exploration Model Loss : 0.00010573336476227269\n","Exploitation Data q-values : 4.729028224945068\n","Exploitation OOD q-values : 6.164380073547363\n","Exploitation CQL Loss : 1.4353525638580322\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -149.570007\n","best mean reward -149.570007\n","running time 449.746847\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -149.57000732421875\n","Train_BestReturn : -149.57000732421875\n","TimeSinceStart : 449.7468469142914\n","Exploitation Critic Loss : 2.075038433074951\n","Exploration Critic Loss : 0.5274137854576111\n","Exploration Model Loss : 3.588941763155162e-05\n","Exploitation Data q-values : 3.7315597534179688\n","Exploitation OOD q-values : 5.137221336364746\n","Exploitation CQL Loss : 1.405661702156067\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -149.570007\n","best mean reward -149.570007\n","running time 481.392621\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -149.57000732421875\n","Train_BestReturn : -149.57000732421875\n","TimeSinceStart : 481.3926212787628\n","Exploitation Critic Loss : 2.7286343574523926\n","Exploration Critic Loss : 0.23232313990592957\n","Exploration Model Loss : 2.4780227249721065e-05\n","Exploitation Data q-values : 3.6790785789489746\n","Exploitation OOD q-values : 5.083439350128174\n","Exploitation CQL Loss : 1.4043606519699097\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -149.570007\n","best mean reward -149.570007\n","running time 510.521247\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -149.57000732421875\n","Train_BestReturn : -149.57000732421875\n","TimeSinceStart : 510.5212469100952\n","Exploitation Critic Loss : 2.9094200134277344\n","Exploration Critic Loss : 1.7100317478179932\n","Exploration Model Loss : 1.5401081327581778e-05\n","Exploitation Data q-values : 4.408405303955078\n","Exploitation OOD q-values : 5.813554763793945\n","Exploitation CQL Loss : 1.405149221420288\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -149.750000\n","best mean reward -149.570007\n","running time 541.699520\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -149.75\n","Train_BestReturn : -149.57000732421875\n","TimeSinceStart : 541.6995198726654\n","Exploitation Critic Loss : 16.076414108276367\n","Exploration Critic Loss : 2.8322339057922363\n","Exploration Model Loss : 5.093260551802814e-05\n","Exploitation Data q-values : 4.028875350952148\n","Exploitation OOD q-values : 5.445321083068848\n","Exploitation CQL Loss : 1.4164456129074097\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.570007\n","running time 574.085967\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.57000732421875\n","TimeSinceStart : 574.0859668254852\n","Exploitation Critic Loss : 2.2354211807250977\n","Exploration Critic Loss : 2.0351765155792236\n","Exploration Model Loss : 0.0003028198843821883\n","Exploitation Data q-values : 4.096254825592041\n","Exploitation OOD q-values : 5.504961967468262\n","Exploitation CQL Loss : 1.4087070226669312\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -150.000000\n","best mean reward -149.570007\n","running time 606.137358\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -150.0\n","Train_BestReturn : -149.57000732421875\n","TimeSinceStart : 606.1373581886292\n","Exploitation Critic Loss : 2.0337412357330322\n","Exploration Critic Loss : 0.17495764791965485\n","Exploration Model Loss : 2.116553696396295e-05\n","Exploitation Data q-values : 3.4099669456481934\n","Exploitation OOD q-values : 4.810068607330322\n","Exploitation CQL Loss : 1.400101900100708\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -149.070007\n","best mean reward -149.070007\n","running time 638.609038\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -149.07000732421875\n","Train_BestReturn : -149.07000732421875\n","TimeSinceStart : 638.6090381145477\n","Exploitation Critic Loss : 5.815103530883789\n","Exploration Critic Loss : 5.0018310546875\n","Exploration Model Loss : 0.00014710686809848994\n","Exploitation Data q-values : 4.114066123962402\n","Exploitation OOD q-values : 5.521650791168213\n","Exploitation CQL Loss : 1.4075844287872314\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -148.449997\n","best mean reward -148.449997\n","running time 667.142619\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -148.4499969482422\n","Train_BestReturn : -148.4499969482422\n","TimeSinceStart : 667.1426191329956\n","Exploitation Critic Loss : 5.5014543533325195\n","Exploration Critic Loss : 2.2982633113861084\n","Exploration Model Loss : 1.9423609046498314e-05\n","Exploitation Data q-values : 3.9534521102905273\n","Exploitation OOD q-values : 5.406351089477539\n","Exploitation CQL Loss : 1.4528987407684326\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -147.639999\n","best mean reward -147.639999\n","running time 698.625479\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -147.63999938964844\n","Train_BestReturn : -147.63999938964844\n","TimeSinceStart : 698.6254785060883\n","Exploitation Critic Loss : 2.2337589263916016\n","Exploration Critic Loss : 2.0632450580596924\n","Exploration Model Loss : 0.00015119949239306152\n","Exploitation Data q-values : 4.486830711364746\n","Exploitation OOD q-values : 5.932098865509033\n","Exploitation CQL Loss : 1.4452677965164185\n","Eval_AverageReturn : -125.125\n","Eval_StdReturn : 46.22617721557617\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 125.375\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -146.070007\n","best mean reward -146.070007\n","running time 728.228331\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -146.07000732421875\n","Train_BestReturn : -146.07000732421875\n","TimeSinceStart : 728.2283310890198\n","Exploitation Critic Loss : 2.767024517059326\n","Exploration Critic Loss : 5.372244834899902\n","Exploration Model Loss : 9.940294694388285e-05\n","Exploitation Data q-values : 4.878900527954102\n","Exploitation OOD q-values : 6.3698930740356445\n","Exploitation CQL Loss : 1.4909930229187012\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -143.130005\n","best mean reward -143.130005\n","running time 761.654513\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -143.1300048828125\n","Train_BestReturn : -143.1300048828125\n","TimeSinceStart : 761.6545131206512\n","Exploitation Critic Loss : 2.727262496948242\n","Exploration Critic Loss : 2.806413173675537\n","Exploration Model Loss : 0.00015162209456320852\n","Exploitation Data q-values : 5.406550407409668\n","Exploitation OOD q-values : 6.836437225341797\n","Exploitation CQL Loss : 1.4298875331878662\n","Eval_AverageReturn : -145.85714721679688\n","Eval_StdReturn : 6.685348033905029\n","Eval_MaxReturn : -133.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 146.14285714285714\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -141.750000\n","best mean reward -141.750000\n","running time 796.560700\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -141.75\n","Train_BestReturn : -141.75\n","TimeSinceStart : 796.5606999397278\n","Exploitation Critic Loss : 3.000587224960327\n","Exploration Critic Loss : 3.7871012687683105\n","Exploration Model Loss : 0.00010036176536232233\n","Exploitation Data q-values : 6.0188188552856445\n","Exploitation OOD q-values : 7.492372512817383\n","Exploitation CQL Loss : 1.4735538959503174\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -137.449997\n","best mean reward -137.449997\n","running time 827.363330\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -137.4499969482422\n","Train_BestReturn : -137.4499969482422\n","TimeSinceStart : 827.3633303642273\n","Exploitation Critic Loss : 8.117497444152832\n","Exploration Critic Loss : 1.012282371520996\n","Exploration Model Loss : 0.0002382979728281498\n","Exploitation Data q-values : 6.614174842834473\n","Exploitation OOD q-values : 8.148858070373535\n","Exploitation CQL Loss : 1.5346834659576416\n","Eval_AverageReturn : -136.25\n","Eval_StdReturn : 27.132776260375977\n","Eval_MaxReturn : -69.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 136.5\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -126.260002\n","best mean reward -126.260002\n","running time 860.265538\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -126.26000213623047\n","Train_BestReturn : -126.26000213623047\n","TimeSinceStart : 860.2655377388\n","Exploitation Critic Loss : 24.987564086914062\n","Exploration Critic Loss : 15.733488082885742\n","Exploration Model Loss : 0.00025651647592894733\n","Exploitation Data q-values : 10.356773376464844\n","Exploitation OOD q-values : 12.134220123291016\n","Exploitation CQL Loss : 1.7774466276168823\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -119.330002\n","best mean reward -119.330002\n","running time 889.530502\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -119.33000183105469\n","Train_BestReturn : -119.33000183105469\n","TimeSinceStart : 889.5305020809174\n","Exploitation Critic Loss : 6.175841808319092\n","Exploration Critic Loss : 9.274515151977539\n","Exploration Model Loss : 0.00017156479589175433\n","Exploitation Data q-values : 9.124502182006836\n","Exploitation OOD q-values : 10.852640151977539\n","Exploitation CQL Loss : 1.7281392812728882\n","Eval_AverageReturn : -129.0\n","Eval_StdReturn : 39.34463119506836\n","Eval_MaxReturn : -36.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 129.25\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -106.910004\n","best mean reward -106.910004\n","running time 922.676247\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -106.91000366210938\n","Train_BestReturn : -106.91000366210938\n","TimeSinceStart : 922.6762473583221\n","Exploitation Critic Loss : 5.331813812255859\n","Exploration Critic Loss : 5.740935802459717\n","Exploration Model Loss : 1.6004258213797584e-05\n","Exploitation Data q-values : 9.629533767700195\n","Exploitation OOD q-values : 11.352123260498047\n","Exploitation CQL Loss : 1.722590446472168\n","Eval_AverageReturn : -64.875\n","Eval_StdReturn : 39.60725021362305\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 65.75\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -88.540001\n","best mean reward -88.540001\n","running time 957.652663\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -88.54000091552734\n","Train_BestReturn : -88.54000091552734\n","TimeSinceStart : 957.652663230896\n","Exploitation Critic Loss : 9.126825332641602\n","Exploration Critic Loss : 13.992964744567871\n","Exploration Model Loss : 1.7858841601992026e-05\n","Exploitation Data q-values : 12.903631210327148\n","Exploitation OOD q-values : 14.818666458129883\n","Exploitation CQL Loss : 1.9150357246398926\n","Eval_AverageReturn : -52.0\n","Eval_StdReturn : 43.6324348449707\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 52.89473684210526\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -68.610001\n","best mean reward -68.610001\n","running time 993.720735\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -68.61000061035156\n","Train_BestReturn : -68.61000061035156\n","TimeSinceStart : 993.7207345962524\n","Exploitation Critic Loss : 13.175354957580566\n","Exploration Critic Loss : 21.08231544494629\n","Exploration Model Loss : 7.003056089160964e-05\n","Exploitation Data q-values : 15.653916358947754\n","Exploitation OOD q-values : 17.527450561523438\n","Exploitation CQL Loss : 1.8735332489013672\n","Eval_AverageReturn : -34.96428680419922\n","Eval_StdReturn : 13.159684181213379\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -68.0\n","Eval_AverageEpLen : 35.964285714285715\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -54.849998\n","best mean reward -54.849998\n","running time 1030.797438\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -54.849998474121094\n","Train_BestReturn : -54.849998474121094\n","TimeSinceStart : 1030.7974383831024\n","Exploitation Critic Loss : 13.122007369995117\n","Exploration Critic Loss : 13.46375846862793\n","Exploration Model Loss : 0.00035406782990321517\n","Exploitation Data q-values : 16.153213500976562\n","Exploitation OOD q-values : 18.10218048095703\n","Exploitation CQL Loss : 1.9489679336547852\n","Eval_AverageReturn : -32.83333206176758\n","Eval_StdReturn : 12.011337280273438\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -62.0\n","Eval_AverageEpLen : 33.833333333333336\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -40.990002\n","best mean reward -40.990002\n","running time 1069.567031\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -40.9900016784668\n","Train_BestReturn : -40.9900016784668\n","TimeSinceStart : 1069.5670311450958\n","Exploitation Critic Loss : 14.544134140014648\n","Exploration Critic Loss : 10.928844451904297\n","Exploration Model Loss : 0.00020739127648994327\n","Exploitation Data q-values : 20.235885620117188\n","Exploitation OOD q-values : 22.380449295043945\n","Exploitation CQL Loss : 2.1445631980895996\n","Eval_AverageReturn : -35.24137878417969\n","Eval_StdReturn : 13.639838218688965\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -68.0\n","Eval_AverageEpLen : 36.241379310344826\n","Buffer size : 35001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -37.230000\n","best mean reward -37.230000\n","running time 1104.581127\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -37.22999954223633\n","Train_BestReturn : -37.22999954223633\n","TimeSinceStart : 1104.5811269283295\n","Exploitation Critic Loss : 8.207042694091797\n","Exploration Critic Loss : 12.98087215423584\n","Exploration Model Loss : 9.660470645656005e-09\n","Exploitation Data q-values : 23.789627075195312\n","Exploitation OOD q-values : 26.070865631103516\n","Exploitation CQL Loss : 2.2812392711639404\n","Eval_AverageReturn : -34.82143020629883\n","Eval_StdReturn : 12.077291488647461\n","Eval_MaxReturn : -19.0\n","Eval_MinReturn : -70.0\n","Eval_AverageEpLen : 35.82142857142857\n","Buffer size : 36001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -36.509998\n","best mean reward -36.509998\n","running time 1142.472970\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -36.5099983215332\n","Train_BestReturn : -36.5099983215332\n","TimeSinceStart : 1142.472969532013\n","Exploitation Critic Loss : 13.926066398620605\n","Exploration Critic Loss : 31.708049774169922\n","Exploration Model Loss : 0.0030534600373357534\n","Exploitation Data q-values : 27.22298812866211\n","Exploitation OOD q-values : 29.5567569732666\n","Exploitation CQL Loss : 2.3337690830230713\n","Eval_AverageReturn : -28.794116973876953\n","Eval_StdReturn : 8.894081115722656\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -54.0\n","Eval_AverageEpLen : 29.794117647058822\n","Buffer size : 37001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -34.790001\n","best mean reward -34.790001\n","running time 1178.480415\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -34.790000915527344\n","Train_BestReturn : -34.790000915527344\n","TimeSinceStart : 1178.4804153442383\n","Exploitation Critic Loss : 10.324387550354004\n","Exploration Critic Loss : 37.60651397705078\n","Exploration Model Loss : 1.3287134947859158e-07\n","Exploitation Data q-values : 28.22507095336914\n","Exploitation OOD q-values : 30.277236938476562\n","Exploitation CQL Loss : 2.052166223526001\n","Eval_AverageReturn : -28.114286422729492\n","Eval_StdReturn : 7.974141597747803\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -45.0\n","Eval_AverageEpLen : 29.114285714285714\n","Buffer size : 38001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -34.770000\n","best mean reward -34.770000\n","running time 1217.268231\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -34.77000045776367\n","Train_BestReturn : -34.77000045776367\n","TimeSinceStart : 1217.2682313919067\n","Exploitation Critic Loss : 10.329465866088867\n","Exploration Critic Loss : 44.615135192871094\n","Exploration Model Loss : 3.548625318217091e-05\n","Exploitation Data q-values : 30.564273834228516\n","Exploitation OOD q-values : 32.72126007080078\n","Exploitation CQL Loss : 2.1569814682006836\n","Eval_AverageReturn : -28.91176414489746\n","Eval_StdReturn : 7.277554512023926\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 29.91176470588235\n","Buffer size : 39001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -33.740002\n","best mean reward -33.740002\n","running time 1254.425464\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -33.7400016784668\n","Train_BestReturn : -33.7400016784668\n","TimeSinceStart : 1254.4254639148712\n","Exploitation Critic Loss : 10.765130996704102\n","Exploration Critic Loss : 55.55427169799805\n","Exploration Model Loss : 3.950154678022955e-06\n","Exploitation Data q-values : 29.823139190673828\n","Exploitation OOD q-values : 31.879154205322266\n","Exploitation CQL Loss : 2.0560173988342285\n","Eval_AverageReturn : -24.325000762939453\n","Eval_StdReturn : 5.934591770172119\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -39.0\n","Eval_AverageEpLen : 25.325\n","Buffer size : 40001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -33.509998\n","best mean reward -33.509998\n","running time 1290.908510\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -33.5099983215332\n","Train_BestReturn : -33.5099983215332\n","TimeSinceStart : 1290.9085102081299\n","Exploitation Critic Loss : 6.42689847946167\n","Exploration Critic Loss : 102.75463104248047\n","Exploration Model Loss : 5.17346961714793e-07\n","Exploitation Data q-values : 32.07076644897461\n","Exploitation OOD q-values : 34.280670166015625\n","Exploitation CQL Loss : 2.209902763366699\n","Eval_AverageReturn : -24.073171615600586\n","Eval_StdReturn : 5.452188014984131\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -35.0\n","Eval_AverageEpLen : 25.073170731707318\n","Buffer size : 41001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -33.570000\n","best mean reward -33.509998\n","running time 1327.513222\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\", line 75, in _wrapreduction\n","AttributeError: 'bool' object has no attribute 'any'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"cs285/scripts/run_hw5_expl.py\", line 136, in <module>\n","    main()\n","  File \"cs285/scripts/run_hw5_expl.py\", line 132, in main\n","    trainer.run_training_loop()\n","  File \"cs285/scripts/run_hw5_expl.py\", line 39, in run_training_loop\n","    eval_policy = self.rl_trainer.agent.actor,\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py\", line 212, in run_training_loop\n","    self.perform_dqn_logging(all_logs)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer.py\", line 301, in perform_dqn_logging\n","    eval_paths, eval_envsteps_this_batch = utils.sample_trajectories(self.eval_env, self.agent.eval_policy, self.params['eval_batch_size'], self.params['ep_len'])\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/utils.py\", line 99, in sample_trajectories\n","    path = sample_trajectory(env, policy, max_path_length, render, render_mode)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/utils.py\", line 58, in sample_trajectory\n","    ob = env.reset()\n","  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/record_video.py\", line 97, in reset\n","    observations = super().reset(**kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/gym/core.py\", line 415, in reset\n","    return self.env.reset(**kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/gym/core.py\", line 415, in reset\n","    return self.env.reset(**kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/record_episode_statistics.py\", line 100, in reset\n","    observations = super().reset(**kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/gym/core.py\", line 415, in reset\n","    return self.env.reset(**kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/order_enforcing.py\", line 42, in reset\n","    return self.env.reset(**kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/gym/core.py\", line 415, in reset\n","    return self.env.reset(**kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/gym/wrappers/env_checker.py\", line 47, in reset\n","    return self.env.reset(**kwargs)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py\", line 346, in reset\n","    self.last_trajectory = self.plot_trajectory()\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py\", line 493, in plot_trajectory\n","    self.plot_walls()\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/envs/pointmass/pointmass.py\", line 517, in plot_walls\n","    self.plt.fill_between(x, y0, y1, color='grey')\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\", line 2558, in fill_between\n","    return gca().fill_between(\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\", line 879, in gca\n","    return gcf().gca(**kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\", line 1957, in gca\n","    return self.add_subplot(1, 1, 1, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\", line 1419, in add_subplot\n","    a = subplot_class_factory(projection_class)(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_subplots.py\", line 76, in __init__\n","    self._axes_class.__init__(self, fig, self.figbox, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\", line 454, in __init__\n","    self.cla()\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\", line 1080, in cla\n","    self.yaxis.set_clip_path(self.patch)\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\", line 969, in set_clip_path\n","    for child in self.majorTicks + self.minorTicks:\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\", line 703, in __get__\n","    tick = instance._get_tick(major=False)\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\", line 2230, in _get_tick\n","    return YTick(self.axes, 0, '', major=major, **tick_kw)\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\", line 157, in __init__\n","    self.tick1line = self._get_tick1line()\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\", line 584, in _get_tick1line\n","    zorder=self._zorder)\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/lines.py\", line 393, in __init__\n","    self.set_markerfacecolor(markerfacecolor)\n","  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/lines.py\", line 1253, in set_markerfacecolor\n","    if np.any(self._markerfacecolor != fc):\n","  File \"<__array_function__ internals>\", line 6, in any\n","  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\", line 2359, in any\n","    keepdims=keepdims, where=where)\n","  File \"/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\", line 75, in _wrapreduction\n","    reduction = getattr(obj, method)\n","KeyboardInterrupt\n"]}],"source":["!python cs285/scripts/run_hw5_expl.py --env_name PointmassMedium-v0 --use_rnd \\\n"," --num_exploration_steps=20000 --cql_alpha=1.0 --exp_name q3_medium_cql --seed 2 \\\n"," --exploit_rew_shift 1 --exploit_rew_scale 100"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1746048,"status":"ok","timestamp":1669092226519,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"p0vOMGrPKS2c","outputId":"c6f4a2ab-76d6-4fc9-adb4-bfa347bdb100"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_hard_dqn_PointmassHard-v0_22-11-2022_04-14-42 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_hard_dqn_PointmassHard-v0_22-11-2022_04-14-42\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002069\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0020694732666015625\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -100.000000\n","best mean reward -inf\n","running time 8.119896\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -100.0\n","TimeSinceStart : 8.119896411895752\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -100.000000\n","best mean reward -inf\n","running time 16.615401\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -100.0\n","TimeSinceStart : 16.615401029586792\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -100.000000\n","best mean reward -inf\n","running time 47.291809\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -100.0\n","TimeSinceStart : 47.29180884361267\n","Exploitation Critic Loss : 0.000388480955734849\n","Exploration Critic Loss : 0.10792098939418793\n","Exploration Model Loss : 0.02157778851687908\n","Exploitation Data q-values : -0.21746064722537994\n","Exploitation OOD q-values : 1.3920726776123047\n","Exploitation CQL Loss : 1.6095330715179443\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -98.025002\n","best mean reward -inf\n","running time 80.988923\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -98.0250015258789\n","TimeSinceStart : 80.98892331123352\n","Exploitation Critic Loss : 0.15349911153316498\n","Exploration Critic Loss : 0.13216394186019897\n","Exploration Model Loss : 0.183432936668396\n","Exploitation Data q-values : -0.07710089534521103\n","Exploitation OOD q-values : 1.5408716201782227\n","Exploitation CQL Loss : 1.6179726123809814\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -98.419998\n","best mean reward -inf\n","running time 112.784487\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -98.41999816894531\n","TimeSinceStart : 112.78448677062988\n","Exploitation Critic Loss : 0.24585513770580292\n","Exploration Critic Loss : 0.11588682234287262\n","Exploration Model Loss : 0.010582776740193367\n","Exploitation Data q-values : 0.881996750831604\n","Exploitation OOD q-values : 2.551717758178711\n","Exploitation CQL Loss : 1.669721007347107\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -98.683334\n","best mean reward -inf\n","running time 146.396644\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -98.68333435058594\n","TimeSinceStart : 146.39664387702942\n","Exploitation Critic Loss : 16.557144165039062\n","Exploration Critic Loss : 0.2656967341899872\n","Exploration Model Loss : 0.012039652094244957\n","Exploitation Data q-values : 2.39727783203125\n","Exploitation OOD q-values : 4.1086907386779785\n","Exploitation CQL Loss : 1.7114131450653076\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -98.871429\n","best mean reward -inf\n","running time 177.426515\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -98.87142944335938\n","TimeSinceStart : 177.4265148639679\n","Exploitation Critic Loss : 0.2706708014011383\n","Exploration Critic Loss : 0.1833890676498413\n","Exploration Model Loss : 0.0019640224054455757\n","Exploitation Data q-values : 3.1529438495635986\n","Exploitation OOD q-values : 4.852038383483887\n","Exploitation CQL Loss : 1.699094533920288\n","Eval_AverageReturn : -99.7272720336914\n","Eval_StdReturn : 0.8624393939971924\n","Eval_MaxReturn : -97.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 99.81818181818181\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -99.012497\n","best mean reward -inf\n","running time 211.550224\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -99.01249694824219\n","TimeSinceStart : 211.55022430419922\n","Exploitation Critic Loss : 0.8252949714660645\n","Exploration Critic Loss : 0.4278554618358612\n","Exploration Model Loss : 0.0009589134133420885\n","Exploitation Data q-values : 3.766995429992676\n","Exploitation OOD q-values : 5.453446388244629\n","Exploitation CQL Loss : 1.6864509582519531\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -99.122223\n","best mean reward -inf\n","running time 242.063409\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -99.12222290039062\n","TimeSinceStart : 242.06340885162354\n","Exploitation Critic Loss : 0.3264578580856323\n","Exploration Critic Loss : 0.5449850559234619\n","Exploration Model Loss : 0.0010348570067435503\n","Exploitation Data q-values : 4.578727722167969\n","Exploitation OOD q-values : 6.255950450897217\n","Exploitation CQL Loss : 1.6772218942642212\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -99.209999\n","best mean reward -inf\n","running time 275.551656\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -99.20999908447266\n","TimeSinceStart : 275.5516560077667\n","Exploitation Critic Loss : 1.2603895664215088\n","Exploration Critic Loss : 0.7066980600357056\n","Exploration Model Loss : 0.0007003651116974652\n","Exploitation Data q-values : 5.48015832901001\n","Exploitation OOD q-values : 7.148375034332275\n","Exploitation CQL Loss : 1.6682169437408447\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -99.209999\n","best mean reward -99.209999\n","running time 313.157429\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -99.20999908447266\n","Train_BestReturn : -99.20999908447266\n","TimeSinceStart : 313.15742921829224\n","Exploitation Critic Loss : 1.4301334619522095\n","Exploration Critic Loss : 0.5160726308822632\n","Exploration Model Loss : 0.0003541144251357764\n","Exploitation Data q-values : 5.7599196434021\n","Exploitation OOD q-values : 7.519102573394775\n","Exploitation CQL Loss : 1.7591826915740967\n","Eval_AverageReturn : -97.54545593261719\n","Eval_StdReturn : 7.761954307556152\n","Eval_MaxReturn : -73.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 97.63636363636364\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -99.209999\n","best mean reward -99.209999\n","running time 344.825664\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -99.20999908447266\n","Train_BestReturn : -99.20999908447266\n","TimeSinceStart : 344.82566356658936\n","Exploitation Critic Loss : 1.2602460384368896\n","Exploration Critic Loss : 0.45413750410079956\n","Exploration Model Loss : 0.00031129480339586735\n","Exploitation Data q-values : 5.940312385559082\n","Exploitation OOD q-values : 7.575002670288086\n","Exploitation CQL Loss : 1.6346906423568726\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -99.209999\n","best mean reward -99.209999\n","running time 378.456995\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -99.20999908447266\n","Train_BestReturn : -99.20999908447266\n","TimeSinceStart : 378.456995010376\n","Exploitation Critic Loss : 0.7839309573173523\n","Exploration Critic Loss : 0.3098043203353882\n","Exploration Model Loss : 0.0002905742439907044\n","Exploitation Data q-values : 6.821215629577637\n","Exploitation OOD q-values : 8.5065336227417\n","Exploitation CQL Loss : 1.6853179931640625\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -100.000000\n","best mean reward -99.209999\n","running time 412.047177\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -100.0\n","Train_BestReturn : -99.20999908447266\n","TimeSinceStart : 412.04717659950256\n","Exploitation Critic Loss : 1.3104572296142578\n","Exploration Critic Loss : 0.3689981997013092\n","Exploration Model Loss : 9.253866301150993e-05\n","Exploitation Data q-values : 6.4034576416015625\n","Exploitation OOD q-values : 8.0978364944458\n","Exploitation CQL Loss : 1.6943786144256592\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -100.000000\n","best mean reward -99.209999\n","running time 443.463595\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -100.0\n","Train_BestReturn : -99.20999908447266\n","TimeSinceStart : 443.46359491348267\n","Exploitation Critic Loss : 1.0142192840576172\n","Exploration Critic Loss : 0.36012741923332214\n","Exploration Model Loss : 5.266661901259795e-05\n","Exploitation Data q-values : 6.441966533660889\n","Exploitation OOD q-values : 8.190611839294434\n","Exploitation CQL Loss : 1.748645544052124\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -100.000000\n","best mean reward -99.209999\n","running time 477.464164\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -100.0\n","Train_BestReturn : -99.20999908447266\n","TimeSinceStart : 477.46416425704956\n","Exploitation Critic Loss : 0.677813708782196\n","Exploration Critic Loss : 0.10919149219989777\n","Exploration Model Loss : 5.22865702805575e-05\n","Exploitation Data q-values : 7.294735908508301\n","Exploitation OOD q-values : 9.125410079956055\n","Exploitation CQL Loss : 1.830674648284912\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -100.000000\n","best mean reward -99.209999\n","running time 508.698472\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -100.0\n","Train_BestReturn : -99.20999908447266\n","TimeSinceStart : 508.6984715461731\n","Exploitation Critic Loss : 0.37038886547088623\n","Exploration Critic Loss : 0.15497660636901855\n","Exploration Model Loss : 2.9759516110061668e-05\n","Exploitation Data q-values : 7.147162437438965\n","Exploitation OOD q-values : 8.8599214553833\n","Exploitation CQL Loss : 1.7127585411071777\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -100.000000\n","best mean reward -99.209999\n","running time 545.954285\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -100.0\n","Train_BestReturn : -99.20999908447266\n","TimeSinceStart : 545.9542851448059\n","Exploitation Critic Loss : 0.7449826598167419\n","Exploration Critic Loss : 0.1495261937379837\n","Exploration Model Loss : 6.313822814263403e-05\n","Exploitation Data q-values : 7.943177700042725\n","Exploitation OOD q-values : 9.697294235229492\n","Exploitation CQL Loss : 1.7541158199310303\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -100.000000\n","best mean reward -99.209999\n","running time 580.110568\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -100.0\n","Train_BestReturn : -99.20999908447266\n","TimeSinceStart : 580.1105682849884\n","Exploitation Critic Loss : 6.927632808685303\n","Exploration Critic Loss : 0.16391830146312714\n","Exploration Model Loss : 0.0002611015224829316\n","Exploitation Data q-values : 8.422492980957031\n","Exploitation OOD q-values : 10.113101959228516\n","Exploitation CQL Loss : 1.6906088590621948\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -100.000000\n","best mean reward -99.209999\n","running time 611.384577\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -100.0\n","Train_BestReturn : -99.20999908447266\n","TimeSinceStart : 611.3845772743225\n","Exploitation Critic Loss : 1.8257439136505127\n","Exploration Critic Loss : 0.20865148305892944\n","Exploration Model Loss : 0.0002693624119274318\n","Exploitation Data q-values : 9.966623306274414\n","Exploitation OOD q-values : 11.79863166809082\n","Exploitation CQL Loss : 1.8320081233978271\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -97.489998\n","best mean reward -97.489998\n","running time 646.141654\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -97.48999786376953\n","Train_BestReturn : -97.48999786376953\n","TimeSinceStart : 646.1416544914246\n","Exploitation Critic Loss : 1.2858189344406128\n","Exploration Critic Loss : 0.05852315202355385\n","Exploration Model Loss : 0.00018192979041486979\n","Exploitation Data q-values : 10.075368881225586\n","Exploitation OOD q-values : 11.881970405578613\n","Exploitation CQL Loss : 1.8066022396087646\n","Eval_AverageReturn : -98.18181610107422\n","Eval_StdReturn : 5.440891742706299\n","Eval_MaxReturn : -81.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 98.36363636363636\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -95.510002\n","best mean reward -95.510002\n","running time 678.761492\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -95.51000213623047\n","Train_BestReturn : -95.51000213623047\n","TimeSinceStart : 678.7614915370941\n","Exploitation Critic Loss : 0.6769098043441772\n","Exploration Critic Loss : 0.024468302726745605\n","Exploration Model Loss : 0.00014157613622955978\n","Exploitation Data q-values : 11.35044002532959\n","Exploitation OOD q-values : 13.13420295715332\n","Exploitation CQL Loss : 1.7837640047073364\n","Eval_AverageReturn : -77.92308044433594\n","Eval_StdReturn : 19.578094482421875\n","Eval_MaxReturn : -46.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 78.6923076923077\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -92.000000\n","best mean reward -92.000000\n","running time 713.987722\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -92.0\n","Train_BestReturn : -92.0\n","TimeSinceStart : 713.9877223968506\n","Exploitation Critic Loss : 1.308394432067871\n","Exploration Critic Loss : 0.06245366111397743\n","Exploration Model Loss : 0.00011382438242435455\n","Exploitation Data q-values : 12.871657371520996\n","Exploitation OOD q-values : 14.515233993530273\n","Exploitation CQL Loss : 1.6435766220092773\n","Eval_AverageReturn : -76.92308044433594\n","Eval_StdReturn : 23.093326568603516\n","Eval_MaxReturn : -44.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 77.46153846153847\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -88.779999\n","best mean reward -88.779999\n","running time 752.281400\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -88.77999877929688\n","Train_BestReturn : -88.77999877929688\n","TimeSinceStart : 752.2814004421234\n","Exploitation Critic Loss : 3.332202672958374\n","Exploration Critic Loss : 0.11908702552318573\n","Exploration Model Loss : 0.00018171410192735493\n","Exploitation Data q-values : 15.075296401977539\n","Exploitation OOD q-values : 16.77124786376953\n","Exploitation CQL Loss : 1.6959528923034668\n","Eval_AverageReturn : -70.86666870117188\n","Eval_StdReturn : 24.31972312927246\n","Eval_MaxReturn : -33.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 71.6\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -84.269997\n","best mean reward -84.269997\n","running time 786.544730\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -84.2699966430664\n","Train_BestReturn : -84.2699966430664\n","TimeSinceStart : 786.5447297096252\n","Exploitation Critic Loss : 5.372862815856934\n","Exploration Critic Loss : 0.13080307841300964\n","Exploration Model Loss : 6.308048614300787e-05\n","Exploitation Data q-values : 14.89674186706543\n","Exploitation OOD q-values : 16.549034118652344\n","Exploitation CQL Loss : 1.652292013168335\n","Eval_AverageReturn : -66.93333435058594\n","Eval_StdReturn : 25.65012550354004\n","Eval_MaxReturn : -31.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 67.73333333333333\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -78.040001\n","best mean reward -78.040001\n","running time 822.238630\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -78.04000091552734\n","Train_BestReturn : -78.04000091552734\n","TimeSinceStart : 822.2386298179626\n","Exploitation Critic Loss : 7.818079948425293\n","Exploration Critic Loss : 0.027054257690906525\n","Exploration Model Loss : 3.802461287705228e-05\n","Exploitation Data q-values : 14.279273986816406\n","Exploitation OOD q-values : 16.00798797607422\n","Exploitation CQL Loss : 1.7287147045135498\n","Eval_AverageReturn : -58.70588302612305\n","Eval_StdReturn : 30.756662368774414\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 59.35294117647059\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -72.930000\n","best mean reward -72.930000\n","running time 858.288688\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -72.93000030517578\n","Train_BestReturn : -72.93000030517578\n","TimeSinceStart : 858.2886879444122\n","Exploitation Critic Loss : 2.0723483562469482\n","Exploration Critic Loss : 0.13263532519340515\n","Exploration Model Loss : 0.0007167919538915157\n","Exploitation Data q-values : 15.357331275939941\n","Exploitation OOD q-values : 17.048633575439453\n","Exploitation CQL Loss : 1.6913025379180908\n","Eval_AverageReturn : -61.05882263183594\n","Eval_StdReturn : 25.533069610595703\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 61.88235294117647\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -69.690002\n","best mean reward -69.690002\n","running time 892.242646\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -69.69000244140625\n","Train_BestReturn : -69.69000244140625\n","TimeSinceStart : 892.242645740509\n","Exploitation Critic Loss : 6.836831092834473\n","Exploration Critic Loss : 0.12700672447681427\n","Exploration Model Loss : 2.0023499018861912e-05\n","Exploitation Data q-values : 16.31604766845703\n","Exploitation OOD q-values : 17.949426651000977\n","Exploitation CQL Loss : 1.6333799362182617\n","Eval_AverageReturn : -43.173912048339844\n","Eval_StdReturn : 14.16871166229248\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -79.0\n","Eval_AverageEpLen : 44.17391304347826\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -67.029999\n","best mean reward -67.029999\n","running time 929.935967\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -67.02999877929688\n","Train_BestReturn : -67.02999877929688\n","TimeSinceStart : 929.935967206955\n","Exploitation Critic Loss : 22.096500396728516\n","Exploration Critic Loss : 0.11109940707683563\n","Exploration Model Loss : 0.0001326637138845399\n","Exploitation Data q-values : 16.731096267700195\n","Exploitation OOD q-values : 18.30595588684082\n","Exploitation CQL Loss : 1.5748594999313354\n","Eval_AverageReturn : -72.5\n","Eval_StdReturn : 24.05870819091797\n","Eval_MaxReturn : -35.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 73.21428571428571\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -62.419998\n","best mean reward -62.419998\n","running time 968.863882\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -62.41999816894531\n","Train_BestReturn : -62.41999816894531\n","TimeSinceStart : 968.8638815879822\n","Exploitation Critic Loss : 2.4576594829559326\n","Exploration Critic Loss : 0.03864441066980362\n","Exploration Model Loss : 9.510366362519562e-05\n","Exploitation Data q-values : 17.226985931396484\n","Exploitation OOD q-values : 18.787090301513672\n","Exploitation CQL Loss : 1.5601056814193726\n","Eval_AverageReturn : -43.39130401611328\n","Eval_StdReturn : 15.434965133666992\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 44.34782608695652\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -60.970001\n","best mean reward -60.970001\n","running time 1003.764822\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -60.970001220703125\n","Train_BestReturn : -60.970001220703125\n","TimeSinceStart : 1003.7648220062256\n","Exploitation Critic Loss : 2.737109899520874\n","Exploration Critic Loss : 0.04186942055821419\n","Exploration Model Loss : 6.479867806774564e-06\n","Exploitation Data q-values : 18.858243942260742\n","Exploitation OOD q-values : 20.519655227661133\n","Exploitation CQL Loss : 1.6614123582839966\n","Eval_AverageReturn : -46.66666793823242\n","Eval_StdReturn : 19.511493682861328\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 47.61904761904762\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -57.950001\n","best mean reward -57.950001\n","running time 1042.928604\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -57.95000076293945\n","Train_BestReturn : -57.95000076293945\n","TimeSinceStart : 1042.9286036491394\n","Exploitation Critic Loss : 4.3315863609313965\n","Exploration Critic Loss : 0.2627663016319275\n","Exploration Model Loss : 5.755295546805428e-07\n","Exploitation Data q-values : 18.984844207763672\n","Exploitation OOD q-values : 20.517629623413086\n","Exploitation CQL Loss : 1.5327872037887573\n","Eval_AverageReturn : -53.31578826904297\n","Eval_StdReturn : 23.069808959960938\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 54.26315789473684\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -56.560001\n","best mean reward -56.560001\n","running time 1079.733281\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -56.560001373291016\n","Train_BestReturn : -56.560001373291016\n","TimeSinceStart : 1079.7332808971405\n","Exploitation Critic Loss : 9.123947143554688\n","Exploration Critic Loss : 0.25097763538360596\n","Exploration Model Loss : 0.00017007025599014014\n","Exploitation Data q-values : 18.691797256469727\n","Exploitation OOD q-values : 20.389822006225586\n","Exploitation CQL Loss : 1.698024034500122\n","Eval_AverageReturn : -39.2400016784668\n","Eval_StdReturn : 13.264327049255371\n","Eval_MaxReturn : -27.0\n","Eval_MinReturn : -89.0\n","Eval_AverageEpLen : 40.24\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -54.830002\n","best mean reward -54.830002\n","running time 1115.756978\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -54.83000183105469\n","Train_BestReturn : -54.83000183105469\n","TimeSinceStart : 1115.7569777965546\n","Exploitation Critic Loss : 3.2776825428009033\n","Exploration Critic Loss : 0.1743786334991455\n","Exploration Model Loss : 3.22768255500705e-06\n","Exploitation Data q-values : 21.848583221435547\n","Exploitation OOD q-values : 23.391576766967773\n","Exploitation CQL Loss : 1.5429940223693848\n","Eval_AverageReturn : -43.70833206176758\n","Eval_StdReturn : 19.462526321411133\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 44.666666666666664\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -53.470001\n","best mean reward -53.470001\n","running time 1155.566879\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -53.470001220703125\n","Train_BestReturn : -53.470001220703125\n","TimeSinceStart : 1155.5668785572052\n","Exploitation Critic Loss : 5.998412132263184\n","Exploration Critic Loss : 0.10363619774580002\n","Exploration Model Loss : 0.00013790871889796108\n","Exploitation Data q-values : 21.454238891601562\n","Exploitation OOD q-values : 23.003459930419922\n","Exploitation CQL Loss : 1.5492219924926758\n","Eval_AverageReturn : -48.238094329833984\n","Eval_StdReturn : 21.217477798461914\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -98.0\n","Eval_AverageEpLen : 49.23809523809524\n","Buffer size : 35001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -53.200001\n","best mean reward -53.200001\n","running time 1197.081624\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -53.20000076293945\n","Train_BestReturn : -53.20000076293945\n","TimeSinceStart : 1197.0816237926483\n","Exploitation Critic Loss : 4.153319358825684\n","Exploration Critic Loss : 0.15499217808246613\n","Exploration Model Loss : 2.9636844374181237e-06\n","Exploitation Data q-values : 20.97370147705078\n","Exploitation OOD q-values : 22.603473663330078\n","Exploitation CQL Loss : 1.6297712326049805\n","Eval_AverageReturn : -41.58333206176758\n","Eval_StdReturn : 15.261381149291992\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 42.541666666666664\n","Buffer size : 36001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -54.299999\n","best mean reward -53.200001\n","running time 1235.243287\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -54.29999923706055\n","Train_BestReturn : -53.20000076293945\n","TimeSinceStart : 1235.2432873249054\n","Exploitation Critic Loss : 4.8569560050964355\n","Exploration Critic Loss : 0.24908897280693054\n","Exploration Model Loss : 2.2635958885075524e-07\n","Exploitation Data q-values : 22.6510066986084\n","Exploitation OOD q-values : 24.221485137939453\n","Exploitation CQL Loss : 1.570478916168213\n","Eval_AverageReturn : -41.04166793823242\n","Eval_StdReturn : 11.851438522338867\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -67.0\n","Eval_AverageEpLen : 42.041666666666664\n","Buffer size : 37001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -53.340000\n","best mean reward -53.200001\n","running time 1272.627959\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -53.34000015258789\n","Train_BestReturn : -53.20000076293945\n","TimeSinceStart : 1272.6279587745667\n","Exploitation Critic Loss : 2.716864585876465\n","Exploration Critic Loss : 0.16764815151691437\n","Exploration Model Loss : 2.916219636972528e-05\n","Exploitation Data q-values : 19.390653610229492\n","Exploitation OOD q-values : 20.944263458251953\n","Exploitation CQL Loss : 1.5536093711853027\n","Eval_AverageReturn : -42.78260803222656\n","Eval_StdReturn : 19.16054916381836\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 43.73913043478261\n","Buffer size : 38001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -54.360001\n","best mean reward -53.200001\n","running time 1312.041973\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -54.36000061035156\n","Train_BestReturn : -53.20000076293945\n","TimeSinceStart : 1312.0419733524323\n","Exploitation Critic Loss : 5.332387447357178\n","Exploration Critic Loss : 0.15710653364658356\n","Exploration Model Loss : 2.9981961233715992e-06\n","Exploitation Data q-values : 24.765094757080078\n","Exploitation OOD q-values : 26.243587493896484\n","Exploitation CQL Loss : 1.478495478630066\n","Eval_AverageReturn : -44.08333206176758\n","Eval_StdReturn : 17.64444351196289\n","Eval_MaxReturn : -26.0\n","Eval_MinReturn : -89.0\n","Eval_AverageEpLen : 45.083333333333336\n","Buffer size : 39001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -53.770000\n","best mean reward -53.200001\n","running time 1351.287499\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -53.77000045776367\n","Train_BestReturn : -53.20000076293945\n","TimeSinceStart : 1351.2874987125397\n","Exploitation Critic Loss : 4.754278182983398\n","Exploration Critic Loss : 0.12456092238426208\n","Exploration Model Loss : 9.631407010601833e-06\n","Exploitation Data q-values : 22.96907615661621\n","Exploitation OOD q-values : 24.533824920654297\n","Exploitation CQL Loss : 1.5647491216659546\n","Eval_AverageReturn : -40.040000915527344\n","Eval_StdReturn : 15.350518226623535\n","Eval_MaxReturn : -27.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 41.0\n","Buffer size : 40001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -53.990002\n","best mean reward -53.200001\n","running time 1388.300250\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -53.9900016784668\n","Train_BestReturn : -53.20000076293945\n","TimeSinceStart : 1388.3002502918243\n","Exploitation Critic Loss : 2.147392511367798\n","Exploration Critic Loss : 0.061581801623106\n","Exploration Model Loss : 6.983815546846017e-05\n","Exploitation Data q-values : 25.655563354492188\n","Exploitation OOD q-values : 27.044857025146484\n","Exploitation CQL Loss : 1.3892927169799805\n","Eval_AverageReturn : -43.91666793823242\n","Eval_StdReturn : 16.121715545654297\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -88.0\n","Eval_AverageEpLen : 44.916666666666664\n","Buffer size : 41001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -54.029999\n","best mean reward -53.200001\n","running time 1430.813034\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -54.029998779296875\n","Train_BestReturn : -53.20000076293945\n","TimeSinceStart : 1430.8130338191986\n","Exploitation Critic Loss : 5.244950771331787\n","Exploration Critic Loss : 0.10994239896535873\n","Exploration Model Loss : 6.8551403273886535e-06\n","Exploitation Data q-values : 25.52031707763672\n","Exploitation OOD q-values : 27.085853576660156\n","Exploitation CQL Loss : 1.565536379814148\n","Eval_AverageReturn : -45.681819915771484\n","Eval_StdReturn : 18.365942001342773\n","Eval_MaxReturn : -26.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 46.63636363636363\n","Buffer size : 42001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -54.110001\n","best mean reward -53.200001\n","running time 1469.229753\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -54.11000061035156\n","Train_BestReturn : -53.20000076293945\n","TimeSinceStart : 1469.2297530174255\n","Exploitation Critic Loss : 3.414950370788574\n","Exploration Critic Loss : 0.03569973260164261\n","Exploration Model Loss : 5.42664565728046e-05\n","Exploitation Data q-values : 25.453025817871094\n","Exploitation OOD q-values : 27.05665397644043\n","Exploitation CQL Loss : 1.6036275625228882\n","Eval_AverageReturn : -45.04545593261719\n","Eval_StdReturn : 18.101926803588867\n","Eval_MaxReturn : -27.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 46.0\n","Buffer size : 43001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -53.080002\n","best mean reward -53.080002\n","running time 1507.987382\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -53.08000183105469\n","Train_BestReturn : -53.08000183105469\n","TimeSinceStart : 1507.9873819351196\n","Exploitation Critic Loss : 3.9110934734344482\n","Exploration Critic Loss : 0.1186613216996193\n","Exploration Model Loss : 2.6656498448573984e-06\n","Exploitation Data q-values : 27.452728271484375\n","Exploitation OOD q-values : 28.932132720947266\n","Exploitation CQL Loss : 1.479405164718628\n","Eval_AverageReturn : -39.84000015258789\n","Eval_StdReturn : 10.562877655029297\n","Eval_MaxReturn : -26.0\n","Eval_MinReturn : -64.0\n","Eval_AverageEpLen : 40.84\n","Buffer size : 44001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -51.669998\n","best mean reward -51.669998\n","running time 1545.525903\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -51.66999816894531\n","Train_BestReturn : -51.66999816894531\n","TimeSinceStart : 1545.525902748108\n","Exploitation Critic Loss : 3.2563796043395996\n","Exploration Critic Loss : 0.06371640413999557\n","Exploration Model Loss : 1.1203814210603014e-05\n","Exploitation Data q-values : 28.163875579833984\n","Exploitation OOD q-values : 29.820518493652344\n","Exploitation CQL Loss : 1.6566416025161743\n","Eval_AverageReturn : -36.592594146728516\n","Eval_StdReturn : 7.7136688232421875\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -61.0\n","Eval_AverageEpLen : 37.592592592592595\n","Buffer size : 45001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -49.840000\n","best mean reward -49.840000\n","running time 1584.846888\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -49.84000015258789\n","Train_BestReturn : -49.84000015258789\n","TimeSinceStart : 1584.8468880653381\n","Exploitation Critic Loss : 3.503265619277954\n","Exploration Critic Loss : 0.07169685512781143\n","Exploration Model Loss : 0.00022287578030955046\n","Exploitation Data q-values : 30.18129539489746\n","Exploitation OOD q-values : 31.715396881103516\n","Exploitation CQL Loss : 1.5340993404388428\n","Eval_AverageReturn : -39.36000061035156\n","Eval_StdReturn : 10.499066352844238\n","Eval_MaxReturn : -27.0\n","Eval_MinReturn : -76.0\n","Eval_AverageEpLen : 40.36\n","Buffer size : 46001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -48.490002\n","best mean reward -48.490002\n","running time 1625.837128\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -48.4900016784668\n","Train_BestReturn : -48.4900016784668\n","TimeSinceStart : 1625.837128162384\n","Exploitation Critic Loss : 5.155045986175537\n","Exploration Critic Loss : 0.09874150156974792\n","Exploration Model Loss : 1.0388744158262853e-05\n","Exploitation Data q-values : 28.038246154785156\n","Exploitation OOD q-values : 29.616731643676758\n","Exploitation CQL Loss : 1.5784876346588135\n","Eval_AverageReturn : -37.80769348144531\n","Eval_StdReturn : 4.906974792480469\n","Eval_MaxReturn : -30.0\n","Eval_MinReturn : -48.0\n","Eval_AverageEpLen : 38.80769230769231\n","Buffer size : 47001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -48.299999\n","best mean reward -48.299999\n","running time 1664.280541\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -48.29999923706055\n","Train_BestReturn : -48.29999923706055\n","TimeSinceStart : 1664.280541419983\n","Exploitation Critic Loss : 6.380801200866699\n","Exploration Critic Loss : 0.23251931369304657\n","Exploration Model Loss : 0.0007428704993799329\n","Exploitation Data q-values : 30.013748168945312\n","Exploitation OOD q-values : 31.521881103515625\n","Exploitation CQL Loss : 1.508131980895996\n","Eval_AverageReturn : -40.400001525878906\n","Eval_StdReturn : 8.265591621398926\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -55.0\n","Eval_AverageEpLen : 41.4\n","Buffer size : 48001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -47.930000\n","best mean reward -47.930000\n","running time 1703.597095\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -47.93000030517578\n","Train_BestReturn : -47.93000030517578\n","TimeSinceStart : 1703.597095489502\n","Exploitation Critic Loss : 3.5728869438171387\n","Exploration Critic Loss : 0.07580086588859558\n","Exploration Model Loss : 1.0606183423078619e-05\n","Exploitation Data q-values : 33.889957427978516\n","Exploitation OOD q-values : 35.36127471923828\n","Exploitation CQL Loss : 1.4713134765625\n","Eval_AverageReturn : -41.75\n","Eval_StdReturn : 12.507497787475586\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -82.0\n","Eval_AverageEpLen : 42.75\n","Buffer size : 49001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_expl.py --env_name PointmassHard-v0 --use_rnd \\\n"," --num_exploration_steps=20000 --cql_alpha=0.0 --exp_name q3_hard_dqn --seed 10 \\\n"," --exploit_rew_shift 1 --exploit_rew_scale 100"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1723774,"status":"ok","timestamp":1669093950280,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"kMSnyxhAKZWi","outputId":"f1241e62-f46f-4692-88dd-9c2ebffeebb4"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_hard_cql_PointmassHard-v0_22-11-2022_04-43-48 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q3_hard_cql_PointmassHard-v0_22-11-2022_04-43-48\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002007\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.00200653076171875\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -100.000000\n","best mean reward -inf\n","running time 8.067546\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -100.0\n","TimeSinceStart : 8.067545890808105\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -100.000000\n","best mean reward -inf\n","running time 16.510534\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -100.0\n","TimeSinceStart : 16.510534048080444\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -100.000000\n","best mean reward -inf\n","running time 50.741830\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -100.0\n","TimeSinceStart : 50.74182963371277\n","Exploitation Critic Loss : 1.5603983402252197\n","Exploration Critic Loss : 0.08761035650968552\n","Exploration Model Loss : 0.5045261979103088\n","Exploitation Data q-values : 0.21280410885810852\n","Exploitation OOD q-values : 1.7511898279190063\n","Exploitation CQL Loss : 1.5383857488632202\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -98.550003\n","best mean reward -inf\n","running time 81.387122\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -98.55000305175781\n","TimeSinceStart : 81.38712191581726\n","Exploitation Critic Loss : 1.5784912109375\n","Exploration Critic Loss : 0.12038340419530869\n","Exploration Model Loss : 0.13677576184272766\n","Exploitation Data q-values : 0.7895557284355164\n","Exploitation OOD q-values : 2.2352664470672607\n","Exploitation CQL Loss : 1.4457107782363892\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -98.839996\n","best mean reward -inf\n","running time 122.305999\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -98.83999633789062\n","TimeSinceStart : 122.30599904060364\n","Exploitation Critic Loss : 1.5294169187545776\n","Exploration Critic Loss : 0.1810779869556427\n","Exploration Model Loss : 0.011451983824372292\n","Exploitation Data q-values : 1.1638675928115845\n","Exploitation OOD q-values : 2.5636825561523438\n","Exploitation CQL Loss : 1.3998149633407593\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -99.033333\n","best mean reward -inf\n","running time 153.426811\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -99.03333282470703\n","TimeSinceStart : 153.42681074142456\n","Exploitation Critic Loss : 1.560056209564209\n","Exploration Critic Loss : 0.18812738358974457\n","Exploration Model Loss : 0.003634166903793812\n","Exploitation Data q-values : 2.296989917755127\n","Exploitation OOD q-values : 3.6378164291381836\n","Exploitation CQL Loss : 1.340826153755188\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -98.814285\n","best mean reward -inf\n","running time 186.488279\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -98.81428527832031\n","TimeSinceStart : 186.4882788658142\n","Exploitation Critic Loss : 1.7275755405426025\n","Exploration Critic Loss : 0.29784664511680603\n","Exploration Model Loss : 0.002409728243947029\n","Exploitation Data q-values : 3.0282363891601562\n","Exploitation OOD q-values : 4.403952121734619\n","Exploitation CQL Loss : 1.3757156133651733\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -98.962502\n","best mean reward -inf\n","running time 220.079999\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -98.9625015258789\n","TimeSinceStart : 220.0799994468689\n","Exploitation Critic Loss : 1.940179467201233\n","Exploration Critic Loss : 0.24215617775917053\n","Exploration Model Loss : 0.0009614418377168477\n","Exploitation Data q-values : 3.415269613265991\n","Exploitation OOD q-values : 4.804798126220703\n","Exploitation CQL Loss : 1.3895288705825806\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -99.077774\n","best mean reward -inf\n","running time 251.031149\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -99.07777404785156\n","TimeSinceStart : 251.03114891052246\n","Exploitation Critic Loss : 2.026609182357788\n","Exploration Critic Loss : 0.3018574118614197\n","Exploration Model Loss : 0.0006327165756374598\n","Exploitation Data q-values : 4.061013698577881\n","Exploitation OOD q-values : 5.438357353210449\n","Exploitation CQL Loss : 1.3773432970046997\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -99.169998\n","best mean reward -inf\n","running time 285.277457\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -99.16999816894531\n","TimeSinceStart : 285.27745723724365\n","Exploitation Critic Loss : 1.771797776222229\n","Exploration Critic Loss : 0.30108946561813354\n","Exploration Model Loss : 0.0002740301424637437\n","Exploitation Data q-values : 4.063547134399414\n","Exploitation OOD q-values : 5.403151035308838\n","Exploitation CQL Loss : 1.3396034240722656\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -99.169998\n","best mean reward -99.169998\n","running time 318.605061\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -99.16999816894531\n","Train_BestReturn : -99.16999816894531\n","TimeSinceStart : 318.6050605773926\n","Exploitation Critic Loss : 4.863317012786865\n","Exploration Critic Loss : 0.2813372015953064\n","Exploration Model Loss : 0.00025464006466791034\n","Exploitation Data q-values : 3.732853651046753\n","Exploitation OOD q-values : 5.082101821899414\n","Exploitation CQL Loss : 1.3492481708526611\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -99.169998\n","best mean reward -99.169998\n","running time 352.265005\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -99.16999816894531\n","Train_BestReturn : -99.16999816894531\n","TimeSinceStart : 352.2650053501129\n","Exploitation Critic Loss : 15.855649948120117\n","Exploration Critic Loss : 0.27074816823005676\n","Exploration Model Loss : 0.0001509814610471949\n","Exploitation Data q-values : 4.282029628753662\n","Exploitation OOD q-values : 5.6382060050964355\n","Exploitation CQL Loss : 1.3561768531799316\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -99.169998\n","best mean reward -99.169998\n","running time 386.266201\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -99.16999816894531\n","Train_BestReturn : -99.16999816894531\n","TimeSinceStart : 386.2662012577057\n","Exploitation Critic Loss : 2.21636700630188\n","Exploration Critic Loss : 0.37346094846725464\n","Exploration Model Loss : 0.000131076158140786\n","Exploitation Data q-values : 4.547361373901367\n","Exploitation OOD q-values : 5.914656639099121\n","Exploitation CQL Loss : 1.3672946691513062\n","Eval_AverageReturn : -96.54545593261719\n","Eval_StdReturn : 8.70071792602539\n","Eval_MaxReturn : -70.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 96.72727272727273\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -99.750000\n","best mean reward -99.169998\n","running time 418.719190\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -99.75\n","Train_BestReturn : -99.16999816894531\n","TimeSinceStart : 418.71918964385986\n","Exploitation Critic Loss : 2.3862948417663574\n","Exploration Critic Loss : 0.3230379521846771\n","Exploration Model Loss : 9.896986739477143e-05\n","Exploitation Data q-values : 4.539948463439941\n","Exploitation OOD q-values : 5.877235412597656\n","Exploitation CQL Loss : 1.3372870683670044\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -99.750000\n","best mean reward -99.169998\n","running time 451.939917\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -99.75\n","Train_BestReturn : -99.16999816894531\n","TimeSinceStart : 451.9399173259735\n","Exploitation Critic Loss : 1.925304889678955\n","Exploration Critic Loss : 0.3781266510486603\n","Exploration Model Loss : 6.16285324213095e-05\n","Exploitation Data q-values : 4.373137950897217\n","Exploitation OOD q-values : 5.765430450439453\n","Exploitation CQL Loss : 1.3922924995422363\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -99.750000\n","best mean reward -99.169998\n","running time 483.066660\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -99.75\n","Train_BestReturn : -99.16999816894531\n","TimeSinceStart : 483.0666596889496\n","Exploitation Critic Loss : 2.815603017807007\n","Exploration Critic Loss : 0.26492372155189514\n","Exploration Model Loss : 5.623262040899135e-05\n","Exploitation Data q-values : 4.354372024536133\n","Exploitation OOD q-values : 5.7524614334106445\n","Exploitation CQL Loss : 1.3980900049209595\n","Eval_AverageReturn : -90.63636016845703\n","Eval_StdReturn : 15.58713150024414\n","Eval_MaxReturn : -58.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 90.9090909090909\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -100.000000\n","best mean reward -99.169998\n","running time 518.470079\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -100.0\n","Train_BestReturn : -99.16999816894531\n","TimeSinceStart : 518.4700787067413\n","Exploitation Critic Loss : 2.0564303398132324\n","Exploration Critic Loss : 0.26096072793006897\n","Exploration Model Loss : 2.3379066988127306e-05\n","Exploitation Data q-values : 4.240103721618652\n","Exploitation OOD q-values : 5.600922584533691\n","Exploitation CQL Loss : 1.36081862449646\n","Eval_AverageReturn : -98.18181610107422\n","Eval_StdReturn : 4.217241287231445\n","Eval_MaxReturn : -86.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 98.36363636363636\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -100.000000\n","best mean reward -99.169998\n","running time 555.007841\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -100.0\n","Train_BestReturn : -99.16999816894531\n","TimeSinceStart : 555.0078413486481\n","Exploitation Critic Loss : 4.484826564788818\n","Exploration Critic Loss : 0.144332617521286\n","Exploration Model Loss : 3.205103348591365e-05\n","Exploitation Data q-values : 4.854920387268066\n","Exploitation OOD q-values : 6.236902236938477\n","Exploitation CQL Loss : 1.3819811344146729\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -100.000000\n","best mean reward -99.169998\n","running time 587.226704\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -100.0\n","Train_BestReturn : -99.16999816894531\n","TimeSinceStart : 587.226704120636\n","Exploitation Critic Loss : 2.035369396209717\n","Exploration Critic Loss : 0.20982277393341064\n","Exploration Model Loss : 4.9222355301026255e-05\n","Exploitation Data q-values : 4.718410491943359\n","Exploitation OOD q-values : 6.0438690185546875\n","Exploitation CQL Loss : 1.3254581689834595\n","Eval_AverageReturn : -98.54545593261719\n","Eval_StdReturn : 4.599676609039307\n","Eval_MaxReturn : -84.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 98.63636363636364\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -100.000000\n","best mean reward -99.169998\n","running time 622.080184\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -100.0\n","Train_BestReturn : -99.16999816894531\n","TimeSinceStart : 622.0801837444305\n","Exploitation Critic Loss : 2.120896816253662\n","Exploration Critic Loss : 0.14657272398471832\n","Exploration Model Loss : 1.7553124052938074e-05\n","Exploitation Data q-values : 4.514260292053223\n","Exploitation OOD q-values : 5.94240140914917\n","Exploitation CQL Loss : 1.4281411170959473\n","Eval_AverageReturn : -92.7272720336914\n","Eval_StdReturn : 15.433675765991211\n","Eval_MaxReturn : -59.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 92.9090909090909\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -99.290001\n","best mean reward -99.169998\n","running time 656.040353\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -99.29000091552734\n","Train_BestReturn : -99.16999816894531\n","TimeSinceStart : 656.0403532981873\n","Exploitation Critic Loss : 1.9598323106765747\n","Exploration Critic Loss : 0.13351817429065704\n","Exploration Model Loss : 9.372730710310861e-05\n","Exploitation Data q-values : 4.623491287231445\n","Exploitation OOD q-values : 6.022400856018066\n","Exploitation CQL Loss : 1.3989100456237793\n","Eval_AverageReturn : -96.0\n","Eval_StdReturn : 10.625868797302246\n","Eval_MaxReturn : -63.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 96.18181818181819\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -97.709999\n","best mean reward -97.709999\n","running time 688.304407\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -97.70999908447266\n","Train_BestReturn : -97.70999908447266\n","TimeSinceStart : 688.304407119751\n","Exploitation Critic Loss : 1.9077510833740234\n","Exploration Critic Loss : 0.13167515397071838\n","Exploration Model Loss : 0.00019120649085380137\n","Exploitation Data q-values : 4.662864685058594\n","Exploitation OOD q-values : 6.064039707183838\n","Exploitation CQL Loss : 1.4011751413345337\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -96.379997\n","best mean reward -96.379997\n","running time 722.052847\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -96.37999725341797\n","Train_BestReturn : -96.37999725341797\n","TimeSinceStart : 722.0528471469879\n","Exploitation Critic Loss : 11.637250900268555\n","Exploration Critic Loss : 0.09973295032978058\n","Exploration Model Loss : 0.00040084574720822275\n","Exploitation Data q-values : 5.960208892822266\n","Exploitation OOD q-values : 7.347856521606445\n","Exploitation CQL Loss : 1.3876476287841797\n","Eval_AverageReturn : -100.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -100.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 100.0\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -95.209999\n","best mean reward -95.209999\n","running time 755.397995\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -95.20999908447266\n","Train_BestReturn : -95.20999908447266\n","TimeSinceStart : 755.3979952335358\n","Exploitation Critic Loss : 2.6771066188812256\n","Exploration Critic Loss : 0.2613574266433716\n","Exploration Model Loss : 3.5541928809834644e-05\n","Exploitation Data q-values : 4.411172866821289\n","Exploitation OOD q-values : 5.816203594207764\n","Exploitation CQL Loss : 1.4050307273864746\n","Eval_AverageReturn : -94.36363983154297\n","Eval_StdReturn : 17.823745727539062\n","Eval_MaxReturn : -38.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 94.45454545454545\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -92.349998\n","best mean reward -92.349998\n","running time 792.666845\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -92.3499984741211\n","Train_BestReturn : -92.3499984741211\n","TimeSinceStart : 792.6668448448181\n","Exploitation Critic Loss : 2.297936201095581\n","Exploration Critic Loss : 0.1831890046596527\n","Exploration Model Loss : 0.0005504037835635245\n","Exploitation Data q-values : 5.609189987182617\n","Exploitation OOD q-values : 6.997108459472656\n","Exploitation CQL Loss : 1.3879188299179077\n","Eval_AverageReturn : -92.45454406738281\n","Eval_StdReturn : 10.731124877929688\n","Eval_MaxReturn : -67.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 92.9090909090909\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -89.980003\n","best mean reward -89.980003\n","running time 827.301114\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -89.9800033569336\n","Train_BestReturn : -89.9800033569336\n","TimeSinceStart : 827.3011140823364\n","Exploitation Critic Loss : 34.56016159057617\n","Exploration Critic Loss : 0.21331652998924255\n","Exploration Model Loss : 9.182887151837349e-05\n","Exploitation Data q-values : 7.487732887268066\n","Exploitation OOD q-values : 8.941338539123535\n","Exploitation CQL Loss : 1.4536058902740479\n","Eval_AverageReturn : -81.76923370361328\n","Eval_StdReturn : 22.12605094909668\n","Eval_MaxReturn : -37.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 82.23076923076923\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -87.199997\n","best mean reward -87.199997\n","running time 860.138452\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -87.19999694824219\n","Train_BestReturn : -87.19999694824219\n","TimeSinceStart : 860.1384522914886\n","Exploitation Critic Loss : 4.130139350891113\n","Exploration Critic Loss : 0.3073149025440216\n","Exploration Model Loss : 7.45330034988001e-05\n","Exploitation Data q-values : 5.414140701293945\n","Exploitation OOD q-values : 6.874882698059082\n","Exploitation CQL Loss : 1.4607422351837158\n","Eval_AverageReturn : -91.2727279663086\n","Eval_StdReturn : 17.76662254333496\n","Eval_MaxReturn : -41.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 91.54545454545455\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -84.690002\n","best mean reward -84.690002\n","running time 895.129125\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -84.69000244140625\n","Train_BestReturn : -84.69000244140625\n","TimeSinceStart : 895.1291246414185\n","Exploitation Critic Loss : 3.554964065551758\n","Exploration Critic Loss : 0.11425603926181793\n","Exploration Model Loss : 0.00037162331864237785\n","Exploitation Data q-values : 7.1163482666015625\n","Exploitation OOD q-values : 8.639884948730469\n","Exploitation CQL Loss : 1.5235371589660645\n","Eval_AverageReturn : -77.92308044433594\n","Eval_StdReturn : 27.485553741455078\n","Eval_MaxReturn : -31.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 78.46153846153847\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -82.879997\n","best mean reward -82.879997\n","running time 927.480841\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -82.87999725341797\n","Train_BestReturn : -82.87999725341797\n","TimeSinceStart : 927.4808411598206\n","Exploitation Critic Loss : 3.0173540115356445\n","Exploration Critic Loss : 0.10777199268341064\n","Exploration Model Loss : 0.000245550530962646\n","Exploitation Data q-values : 8.709364891052246\n","Exploitation OOD q-values : 9.994831085205078\n","Exploitation CQL Loss : 1.2854663133621216\n","Eval_AverageReturn : -56.83333206176758\n","Eval_StdReturn : 19.163768768310547\n","Eval_MaxReturn : -30.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 57.77777777777778\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -82.300003\n","best mean reward -82.300003\n","running time 963.328848\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -82.30000305175781\n","Train_BestReturn : -82.30000305175781\n","TimeSinceStart : 963.3288476467133\n","Exploitation Critic Loss : 23.878620147705078\n","Exploration Critic Loss : 0.029112135991454124\n","Exploration Model Loss : 3.4134453017031774e-05\n","Exploitation Data q-values : 9.034866333007812\n","Exploitation OOD q-values : 10.503606796264648\n","Exploitation CQL Loss : 1.4687411785125732\n","Eval_AverageReturn : -54.55555725097656\n","Eval_StdReturn : 19.25046157836914\n","Eval_MaxReturn : -30.0\n","Eval_MinReturn : -98.0\n","Eval_AverageEpLen : 55.55555555555556\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -80.660004\n","best mean reward -80.660004\n","running time 1003.028996\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -80.66000366210938\n","Train_BestReturn : -80.66000366210938\n","TimeSinceStart : 1003.028995513916\n","Exploitation Critic Loss : 7.96083402633667\n","Exploration Critic Loss : 0.12289837002754211\n","Exploration Model Loss : 2.8431653845473193e-05\n","Exploitation Data q-values : 9.567407608032227\n","Exploitation OOD q-values : 10.963024139404297\n","Exploitation CQL Loss : 1.395616054534912\n","Eval_AverageReturn : -83.76923370361328\n","Eval_StdReturn : 19.215993881225586\n","Eval_MaxReturn : -45.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 84.3076923076923\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -75.800003\n","best mean reward -75.800003\n","running time 1037.438688\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -75.80000305175781\n","Train_BestReturn : -75.80000305175781\n","TimeSinceStart : 1037.4386880397797\n","Exploitation Critic Loss : 7.742524147033691\n","Exploration Critic Loss : 0.3785138428211212\n","Exploration Model Loss : 0.00010214277426712215\n","Exploitation Data q-values : 12.205087661743164\n","Exploitation OOD q-values : 13.539087295532227\n","Exploitation CQL Loss : 1.333999752998352\n","Eval_AverageReturn : -63.117645263671875\n","Eval_StdReturn : 23.6018009185791\n","Eval_MaxReturn : -38.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 63.94117647058823\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -73.339996\n","best mean reward -73.339996\n","running time 1074.262010\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -73.33999633789062\n","Train_BestReturn : -73.33999633789062\n","TimeSinceStart : 1074.2620096206665\n","Exploitation Critic Loss : 3.3627243041992188\n","Exploration Critic Loss : 0.06551386415958405\n","Exploration Model Loss : 0.00016016350127756596\n","Exploitation Data q-values : 11.267922401428223\n","Exploitation OOD q-values : 12.741321563720703\n","Exploitation CQL Loss : 1.4733999967575073\n","Eval_AverageReturn : -87.66666412353516\n","Eval_StdReturn : 18.8870906829834\n","Eval_MaxReturn : -46.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 88.0\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -71.889999\n","best mean reward -71.889999\n","running time 1109.941166\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -71.88999938964844\n","Train_BestReturn : -71.88999938964844\n","TimeSinceStart : 1109.9411659240723\n","Exploitation Critic Loss : 3.368678092956543\n","Exploration Critic Loss : 0.08066295087337494\n","Exploration Model Loss : 2.5619866050874407e-07\n","Exploitation Data q-values : 11.254693984985352\n","Exploitation OOD q-values : 12.56503963470459\n","Exploitation CQL Loss : 1.3103452920913696\n","Eval_AverageReturn : -58.588233947753906\n","Eval_StdReturn : 20.736608505249023\n","Eval_MaxReturn : -30.0\n","Eval_MinReturn : -98.0\n","Eval_AverageEpLen : 59.588235294117645\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -69.449997\n","best mean reward -69.449997\n","running time 1145.287272\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -69.44999694824219\n","Train_BestReturn : -69.44999694824219\n","TimeSinceStart : 1145.287271976471\n","Exploitation Critic Loss : 5.024330139160156\n","Exploration Critic Loss : 0.13472865521907806\n","Exploration Model Loss : 1.1367501429049298e-07\n","Exploitation Data q-values : 12.938882827758789\n","Exploitation OOD q-values : 14.330793380737305\n","Exploitation CQL Loss : 1.391911506652832\n","Eval_AverageReturn : -50.150001525878906\n","Eval_StdReturn : 19.560867309570312\n","Eval_MaxReturn : -30.0\n","Eval_MinReturn : -94.0\n","Eval_AverageEpLen : 51.15\n","Buffer size : 35001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -65.699997\n","best mean reward -65.699997\n","running time 1182.486905\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -65.69999694824219\n","Train_BestReturn : -65.69999694824219\n","TimeSinceStart : 1182.4869050979614\n","Exploitation Critic Loss : 4.335667133331299\n","Exploration Critic Loss : 0.09441256523132324\n","Exploration Model Loss : 0.00014318220200948417\n","Exploitation Data q-values : 12.64892292022705\n","Exploitation OOD q-values : 13.926064491271973\n","Exploitation CQL Loss : 1.2771422863006592\n","Eval_AverageReturn : -47.761905670166016\n","Eval_StdReturn : 15.476263999938965\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 48.714285714285715\n","Buffer size : 36001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -62.009998\n","best mean reward -62.009998\n","running time 1223.133318\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -62.0099983215332\n","Train_BestReturn : -62.0099983215332\n","TimeSinceStart : 1223.1333177089691\n","Exploitation Critic Loss : 5.855130672454834\n","Exploration Critic Loss : 0.18301962316036224\n","Exploration Model Loss : 1.4456997632805724e-05\n","Exploitation Data q-values : 15.456084251403809\n","Exploitation OOD q-values : 16.744007110595703\n","Exploitation CQL Loss : 1.287923812866211\n","Eval_AverageReturn : -47.904762268066406\n","Eval_StdReturn : 17.137828826904297\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 48.857142857142854\n","Buffer size : 37001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -60.040001\n","best mean reward -60.040001\n","running time 1259.779514\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -60.040000915527344\n","Train_BestReturn : -60.040000915527344\n","TimeSinceStart : 1259.7795135974884\n","Exploitation Critic Loss : 7.28593111038208\n","Exploration Critic Loss : 0.2030697762966156\n","Exploration Model Loss : 1.7638836652622558e-05\n","Exploitation Data q-values : 17.056596755981445\n","Exploitation OOD q-values : 18.63709259033203\n","Exploitation CQL Loss : 1.5804953575134277\n","Eval_AverageReturn : -49.04999923706055\n","Eval_StdReturn : 16.656757354736328\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -96.0\n","Eval_AverageEpLen : 50.05\n","Buffer size : 38001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -59.380001\n","best mean reward -59.380001\n","running time 1297.607105\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -59.380001068115234\n","Train_BestReturn : -59.380001068115234\n","TimeSinceStart : 1297.607105255127\n","Exploitation Critic Loss : 10.29840087890625\n","Exploration Critic Loss : 0.15216924250125885\n","Exploration Model Loss : 1.3806038623442873e-05\n","Exploitation Data q-values : 18.194927215576172\n","Exploitation OOD q-values : 19.633197784423828\n","Exploitation CQL Loss : 1.4382684230804443\n","Eval_AverageReturn : -48.0\n","Eval_StdReturn : 19.801395416259766\n","Eval_MaxReturn : -27.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 48.904761904761905\n","Buffer size : 39001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -58.660000\n","best mean reward -58.660000\n","running time 1333.388374\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -58.65999984741211\n","Train_BestReturn : -58.65999984741211\n","TimeSinceStart : 1333.3883743286133\n","Exploitation Critic Loss : 9.677346229553223\n","Exploration Critic Loss : 0.09111315757036209\n","Exploration Model Loss : 0.000757476023864001\n","Exploitation Data q-values : 16.128028869628906\n","Exploitation OOD q-values : 17.724945068359375\n","Exploitation CQL Loss : 1.5969146490097046\n","Eval_AverageReturn : -42.956520080566406\n","Eval_StdReturn : 12.719305038452148\n","Eval_MaxReturn : -27.0\n","Eval_MinReturn : -75.0\n","Eval_AverageEpLen : 43.95652173913044\n","Buffer size : 40001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -58.650002\n","best mean reward -58.650002\n","running time 1371.465896\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -58.650001525878906\n","Train_BestReturn : -58.650001525878906\n","TimeSinceStart : 1371.4658961296082\n","Exploitation Critic Loss : 8.520444869995117\n","Exploration Critic Loss : 0.15799129009246826\n","Exploration Model Loss : 1.1481948604341596e-05\n","Exploitation Data q-values : 17.152965545654297\n","Exploitation OOD q-values : 18.565155029296875\n","Exploitation CQL Loss : 1.4121907949447632\n","Eval_AverageReturn : -45.272727966308594\n","Eval_StdReturn : 10.55798625946045\n","Eval_MaxReturn : -31.0\n","Eval_MinReturn : -70.0\n","Eval_AverageEpLen : 46.27272727272727\n","Buffer size : 41001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -56.200001\n","best mean reward -56.200001\n","running time 1410.617738\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -56.20000076293945\n","Train_BestReturn : -56.20000076293945\n","TimeSinceStart : 1410.6177382469177\n","Exploitation Critic Loss : 14.040291786193848\n","Exploration Critic Loss : 0.16919545829296112\n","Exploration Model Loss : 0.000242582056671381\n","Exploitation Data q-values : 19.982433319091797\n","Exploitation OOD q-values : 21.447107315063477\n","Exploitation CQL Loss : 1.4646742343902588\n","Eval_AverageReturn : -47.14285659790039\n","Eval_StdReturn : 19.248464584350586\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 48.04761904761905\n","Buffer size : 42001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -55.470001\n","best mean reward -55.470001\n","running time 1451.885502\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -55.470001220703125\n","Train_BestReturn : -55.470001220703125\n","TimeSinceStart : 1451.8855018615723\n","Exploitation Critic Loss : 8.607239723205566\n","Exploration Critic Loss : 0.13899677991867065\n","Exploration Model Loss : 0.0006327133160084486\n","Exploitation Data q-values : 21.705963134765625\n","Exploitation OOD q-values : 23.145408630371094\n","Exploitation CQL Loss : 1.4394463300704956\n","Eval_AverageReturn : -50.04999923706055\n","Eval_StdReturn : 14.065828323364258\n","Eval_MaxReturn : -32.0\n","Eval_MinReturn : -77.0\n","Eval_AverageEpLen : 51.05\n","Buffer size : 43001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -54.220001\n","best mean reward -54.220001\n","running time 1488.010623\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -54.220001220703125\n","Train_BestReturn : -54.220001220703125\n","TimeSinceStart : 1488.0106227397919\n","Exploitation Critic Loss : 7.111392021179199\n","Exploration Critic Loss : 0.3221697509288788\n","Exploration Model Loss : 5.840906669618562e-05\n","Exploitation Data q-values : 19.252525329589844\n","Exploitation OOD q-values : 20.554981231689453\n","Exploitation CQL Loss : 1.3024566173553467\n","Eval_AverageReturn : -46.619049072265625\n","Eval_StdReturn : 15.92385482788086\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -85.0\n","Eval_AverageEpLen : 47.61904761904762\n","Buffer size : 44001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -53.540001\n","best mean reward -53.540001\n","running time 1526.639960\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -53.540000915527344\n","Train_BestReturn : -53.540000915527344\n","TimeSinceStart : 1526.6399600505829\n","Exploitation Critic Loss : 6.942109107971191\n","Exploration Critic Loss : 0.05791885778307915\n","Exploration Model Loss : 5.715405131923035e-05\n","Exploitation Data q-values : 20.70245361328125\n","Exploitation OOD q-values : 22.064929962158203\n","Exploitation CQL Loss : 1.3624780178070068\n","Eval_AverageReturn : -39.959999084472656\n","Eval_StdReturn : 9.672558784484863\n","Eval_MaxReturn : -27.0\n","Eval_MinReturn : -78.0\n","Eval_AverageEpLen : 40.96\n","Buffer size : 45001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -51.040001\n","best mean reward -51.040001\n","running time 1565.958384\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -51.040000915527344\n","Train_BestReturn : -51.040000915527344\n","TimeSinceStart : 1565.9583840370178\n","Exploitation Critic Loss : 35.17299270629883\n","Exploration Critic Loss : 0.19040045142173767\n","Exploration Model Loss : 9.022016342896677e-07\n","Exploitation Data q-values : 24.012117385864258\n","Exploitation OOD q-values : 25.385379791259766\n","Exploitation CQL Loss : 1.3732625246047974\n","Eval_AverageReturn : -45.54545593261719\n","Eval_StdReturn : 18.860076904296875\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 46.45454545454545\n","Buffer size : 46001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -50.709999\n","best mean reward -50.709999\n","running time 1602.656640\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -50.709999084472656\n","Train_BestReturn : -50.709999084472656\n","TimeSinceStart : 1602.656640291214\n","Exploitation Critic Loss : 11.60607624053955\n","Exploration Critic Loss : 0.09348893910646439\n","Exploration Model Loss : 0.00017722426855470985\n","Exploitation Data q-values : 21.854965209960938\n","Exploitation OOD q-values : 23.29136848449707\n","Exploitation CQL Loss : 1.4364014863967896\n","Eval_AverageReturn : -40.75\n","Eval_StdReturn : 8.69506549835205\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -59.0\n","Eval_AverageEpLen : 41.75\n","Buffer size : 47001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -51.049999\n","best mean reward -50.709999\n","running time 1644.526364\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -51.04999923706055\n","Train_BestReturn : -50.709999084472656\n","TimeSinceStart : 1644.52636384964\n","Exploitation Critic Loss : 6.041213035583496\n","Exploration Critic Loss : 0.1380460560321808\n","Exploration Model Loss : 4.0547369280830026e-05\n","Exploitation Data q-values : 26.51703643798828\n","Exploitation OOD q-values : 27.896053314208984\n","Exploitation CQL Loss : 1.3790178298950195\n","Eval_AverageReturn : -43.65217208862305\n","Eval_StdReturn : 15.130184173583984\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 44.608695652173914\n","Buffer size : 48001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -50.270000\n","best mean reward -50.270000\n","running time 1683.283404\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -50.27000045776367\n","Train_BestReturn : -50.27000045776367\n","TimeSinceStart : 1683.2834038734436\n","Exploitation Critic Loss : 42.47727966308594\n","Exploration Critic Loss : 0.17647989094257355\n","Exploration Model Loss : 7.602632194902981e-07\n","Exploitation Data q-values : 24.177776336669922\n","Exploitation OOD q-values : 25.537887573242188\n","Exploitation CQL Loss : 1.3601117134094238\n","Eval_AverageReturn : -38.88461685180664\n","Eval_StdReturn : 7.196831226348877\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -61.0\n","Eval_AverageEpLen : 39.88461538461539\n","Buffer size : 49001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_expl.py --env_name PointmassHard-v0 --use_rnd \\\n"," --num_exploration_steps=20000 --cql_alpha=1.0 --exp_name q3_hard_cql --seed 10 \\\n"," --exploit_rew_shift 1 --exploit_rew_scale 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sazfa_1xKdqX"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Vak_qLvvwW1e"},"source":["## 4"]},{"cell_type":"markdown","metadata":{"id":"eeAyuSznsga5"},"source":["#### we will test lambda=0.1, 2, and 20"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1238309,"status":"ok","timestamp":1669126573661,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"dW7Gyra7wbwg","outputId":"ede6f25e-06ab-413c-c644-dba85f7a9965"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_unsupervised_lam20_PointmassEasy-v0_22-11-2022_13-55-37 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_easy_unsupervised_lam20_PointmassEasy-v0_22-11-2022_13-55-37\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002234\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.002233743667602539\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -50.000000\n","best mean reward -inf\n","running time 12.956441\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -50.0\n","TimeSinceStart : 12.956441402435303\n","Eval_AverageReturn : -49.19047546386719\n","Eval_StdReturn : 3.620300769805908\n","Eval_MaxReturn : -33.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 49.23809523809524\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -50.000000\n","best mean reward -inf\n","running time 26.720239\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -50.0\n","TimeSinceStart : 26.72023892402649\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -50.000000\n","best mean reward -inf\n","running time 135.694657\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -50.0\n","TimeSinceStart : 135.69465684890747\n","Exploration Critic Loss : 0.13461904227733612\n","Exploitation Critic Loss : 0.06650183349847794\n","Exploration Model Loss : 0.07352981716394424\n","Actor Loss : 1.625351905822754\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -49.612499\n","best mean reward -inf\n","running time 249.692736\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -49.61249923706055\n","TimeSinceStart : 249.69273614883423\n","Exploration Critic Loss : 0.6503855586051941\n","Exploitation Critic Loss : 0.11696405708789825\n","Exploration Model Loss : 0.3080402910709381\n","Actor Loss : 1.471327781677246\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -46.340000\n","best mean reward -46.340000\n","running time 351.032917\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -46.34000015258789\n","Train_BestReturn : -46.34000015258789\n","TimeSinceStart : 351.03291749954224\n","Exploration Critic Loss : 1.316635012626648\n","Exploitation Critic Loss : 0.07270950824022293\n","Exploration Model Loss : 0.020154595375061035\n","Actor Loss : 1.4986240863800049\n","Eval_AverageReturn : -49.619049072265625\n","Eval_StdReturn : 1.7036707401275635\n","Eval_MaxReturn : -42.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 49.666666666666664\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -36.910000\n","best mean reward -36.910000\n","running time 473.597919\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -36.90999984741211\n","Train_BestReturn : -36.90999984741211\n","TimeSinceStart : 473.5979187488556\n","Exploration Critic Loss : 5.54080057144165\n","Exploitation Critic Loss : 0.07525765895843506\n","Exploration Model Loss : 0.002284516580402851\n","Actor Loss : 1.4081870317459106\n","Eval_AverageReturn : -46.6363639831543\n","Eval_StdReturn : 5.629026889801025\n","Eval_MaxReturn : -30.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 47.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -25.320000\n","best mean reward -25.320000\n","running time 585.417339\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -25.31999969482422\n","Train_BestReturn : -25.31999969482422\n","TimeSinceStart : 585.417338848114\n","Exploration Critic Loss : 6.422232627868652\n","Exploitation Critic Loss : 0.167036771774292\n","Exploration Model Loss : 0.0005641999887302518\n","Actor Loss : 1.3316071033477783\n","Eval_AverageReturn : -40.47999954223633\n","Eval_StdReturn : 9.5921630859375\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 41.04\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -19.270000\n","best mean reward -19.270000\n","running time 699.727744\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -19.270000457763672\n","Train_BestReturn : -19.270000457763672\n","TimeSinceStart : 699.7277436256409\n","Exploration Critic Loss : 8.114123344421387\n","Exploitation Critic Loss : 0.17764034867286682\n","Exploration Model Loss : 0.0008888683514669538\n","Actor Loss : 1.3994035720825195\n","Eval_AverageReturn : -36.0\n","Eval_StdReturn : 11.177145004272461\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 36.785714285714285\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -17.209999\n","best mean reward -17.209999\n","running time 819.645268\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -17.209999084472656\n","Train_BestReturn : -17.209999084472656\n","TimeSinceStart : 819.645268201828\n","Exploration Critic Loss : 10.663900375366211\n","Exploitation Critic Loss : 0.19277983903884888\n","Exploration Model Loss : 0.0005199320730753243\n","Actor Loss : 1.361531376838684\n","Eval_AverageReturn : -32.032257080078125\n","Eval_StdReturn : 11.414432525634766\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 32.903225806451616\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -18.090000\n","best mean reward -17.209999\n","running time 940.496538\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -18.09000015258789\n","Train_BestReturn : -17.209999084472656\n","TimeSinceStart : 940.4965381622314\n","Exploration Critic Loss : 8.097373962402344\n","Exploitation Critic Loss : 0.12459880858659744\n","Exploration Model Loss : 0.0002429307933198288\n","Actor Loss : 1.258484125137329\n","Eval_AverageReturn : -32.67741775512695\n","Eval_StdReturn : 9.562652587890625\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 33.61290322580645\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -17.969999\n","best mean reward -17.209999\n","running time 1068.413598\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -17.969999313354492\n","Train_BestReturn : -17.209999084472656\n","TimeSinceStart : 1068.4135975837708\n","Exploration Critic Loss : 11.121363639831543\n","Exploitation Critic Loss : 0.1066131740808487\n","Exploration Model Loss : 0.0001985785784199834\n","Actor Loss : 1.1623106002807617\n","Eval_AverageReturn : -28.314285278320312\n","Eval_StdReturn : 8.811588287353516\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 29.257142857142856\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -17.500000\n","best mean reward -17.209999\n","running time 1192.070560\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -17.5\n","Train_BestReturn : -17.209999084472656\n","TimeSinceStart : 1192.0705604553223\n","Exploration Critic Loss : 9.365662574768066\n","Exploitation Critic Loss : 0.1261691451072693\n","Exploration Model Loss : 0.0002725119993556291\n","Actor Loss : 1.1669983863830566\n","Eval_AverageReturn : -25.28205108642578\n","Eval_StdReturn : 7.755721569061279\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -46.0\n","Eval_AverageEpLen : 26.28205128205128\n","Buffer size : 12001\n","Done logging...\n","\n","\n","Traceback (most recent call last):\n","  File \"cs285/scripts/run_hw5_awac.py\", line 142, in <module>\n","    main()\n","  File \"cs285/scripts/run_hw5_awac.py\", line 138, in main\n","    trainer.run_training_loop()\n","  File \"cs285/scripts/run_hw5_awac.py\", line 39, in run_training_loop\n","    eval_policy = self.rl_trainer.agent.actor,\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py\", line 201, in run_training_loop\n","    all_logs = self.train_agent()\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py\", line 260, in train_agent\n","    train_log = self.agent.train(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/agents/awac_agent.py\", line 129, in train\n","    exploration_critic_loss = self.exploration_critic.update(ob_no, ac_na, next_ob_no, mixed_reward, terminal_n)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/critics/dqn_critic.py\", line 81, in update\n","    loss.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 396, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 175, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n"]}],"source":["!python cs285/scripts/run_hw5_awac.py --env_name PointmassEasy-v0 \\\n","--exp_name q4_awac_easy_unsupervised_lam20 --use_rnd --num_exploration_steps=20000 \\\n","--unsupervised_exploration --awac_lambda=20 --seed 10"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2826246,"status":"ok","timestamp":1669258717181,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"OvujhfagtieH","outputId":"45c3139b-5cac-43e3-8404-4df5d2ff9011"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_supervised_lam2_PointmassMedium-v0_24-11-2022_02-11-32 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q4_awac_medium_supervised_lam2_PointmassMedium-v0_24-11-2022_02-11-32\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002200\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0022003650665283203\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -149.666672\n","best mean reward -inf\n","running time 6.786539\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -149.6666717529297\n","TimeSinceStart : 6.786538600921631\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -149.846161\n","best mean reward -inf\n","running time 14.002633\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -149.84616088867188\n","TimeSinceStart : 14.002632856369019\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -149.899994\n","best mean reward -inf\n","running time 113.694162\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -149.89999389648438\n","TimeSinceStart : 113.69416213035583\n","Exploration Critic Loss : 0.25906309485435486\n","Exploitation Critic Loss : 0.008178920485079288\n","Exploration Model Loss : 0.34431201219558716\n","Actor Loss : 1.5713207721710205\n","Eval_AverageReturn : -143.7142791748047\n","Eval_StdReturn : 15.3967924118042\n","Eval_MaxReturn : -106.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 143.85714285714286\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -149.923080\n","best mean reward -inf\n","running time 213.438867\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -149.92308044433594\n","TimeSinceStart : 213.43886709213257\n","Exploration Critic Loss : 0.2263258695602417\n","Exploitation Critic Loss : 0.08566693216562271\n","Exploration Model Loss : 0.0009025012841448188\n","Actor Loss : 1.5499745607376099\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -149.939392\n","best mean reward -inf\n","running time 308.937961\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -149.93939208984375\n","TimeSinceStart : 308.93796133995056\n","Exploration Critic Loss : 0.19440858066082\n","Exploitation Critic Loss : 0.05008966848254204\n","Exploration Model Loss : 0.0007143347756937146\n","Actor Loss : 1.3679232597351074\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -142.975616\n","best mean reward -inf\n","running time 405.351553\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -142.97561645507812\n","TimeSinceStart : 405.35155296325684\n","Exploration Critic Loss : 0.1322735846042633\n","Exploitation Critic Loss : 0.10653729736804962\n","Exploration Model Loss : 0.07469980418682098\n","Actor Loss : 1.4435943365097046\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -138.699997\n","best mean reward -inf\n","running time 502.935875\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -138.6999969482422\n","TimeSinceStart : 502.9358751773834\n","Exploration Critic Loss : 0.20392124354839325\n","Exploitation Critic Loss : 0.07918518036603928\n","Exploration Model Loss : 0.0003736130893230438\n","Actor Loss : 1.4676189422607422\n","Eval_AverageReturn : -136.75\n","Eval_StdReturn : 18.71329689025879\n","Eval_MaxReturn : -97.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 137.25\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -134.830505\n","best mean reward -inf\n","running time 601.161856\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -134.83050537109375\n","TimeSinceStart : 601.1618556976318\n","Exploration Critic Loss : 0.1701170653104782\n","Exploitation Critic Loss : 0.09151286631822586\n","Exploration Model Loss : 0.0012517553986981511\n","Actor Loss : 1.4081676006317139\n","Eval_AverageReturn : -139.625\n","Eval_StdReturn : 25.970836639404297\n","Eval_MaxReturn : -71.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 139.875\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -126.842857\n","best mean reward -inf\n","running time 700.329121\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -126.84285736083984\n","TimeSinceStart : 700.3291206359863\n","Exploration Critic Loss : 0.25229284167289734\n","Exploitation Critic Loss : 0.1362331062555313\n","Exploration Model Loss : 0.0009745637071318924\n","Actor Loss : 1.4748643636703491\n","Eval_AverageReturn : -131.75\n","Eval_StdReturn : 18.032955169677734\n","Eval_MaxReturn : -94.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 132.375\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -122.888885\n","best mean reward -inf\n","running time 796.826865\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -122.88888549804688\n","TimeSinceStart : 796.8268649578094\n","Exploration Critic Loss : 0.5435625910758972\n","Exploitation Critic Loss : 0.13657115399837494\n","Exploration Model Loss : 0.0002467406156938523\n","Actor Loss : 1.407819151878357\n","Eval_AverageReturn : -132.375\n","Eval_StdReturn : 31.264745712280273\n","Eval_MaxReturn : -66.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 132.625\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -113.458336\n","best mean reward -inf\n","running time 894.388177\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -113.45833587646484\n","TimeSinceStart : 894.3881766796112\n","Exploration Critic Loss : 0.3931478261947632\n","Exploitation Critic Loss : 0.12681129574775696\n","Exploration Model Loss : 0.0002862739493139088\n","Actor Loss : 1.4548234939575195\n","Eval_AverageReturn : -109.30000305175781\n","Eval_StdReturn : 33.83504104614258\n","Eval_MaxReturn : -40.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 110.1\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -97.809998\n","best mean reward -97.809998\n","running time 988.410161\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -97.80999755859375\n","Train_BestReturn : -97.80999755859375\n","TimeSinceStart : 988.4101614952087\n","Exploration Critic Loss : 0.8701336979866028\n","Exploitation Critic Loss : 0.1399904489517212\n","Exploration Model Loss : 0.00023176326067186892\n","Actor Loss : 1.4342827796936035\n","Eval_AverageReturn : -90.2727279663086\n","Eval_StdReturn : 37.545345306396484\n","Eval_MaxReturn : -35.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 91.18181818181819\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -83.180000\n","best mean reward -83.180000\n","running time 1089.132967\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -83.18000030517578\n","Train_BestReturn : -83.18000030517578\n","TimeSinceStart : 1089.132966518402\n","Exploration Critic Loss : 0.8137537837028503\n","Exploitation Critic Loss : 0.17259615659713745\n","Exploration Model Loss : 0.0001169162496807985\n","Actor Loss : 1.4691964387893677\n","Eval_AverageReturn : -104.0\n","Eval_StdReturn : 35.193748474121094\n","Eval_MaxReturn : -48.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 104.7\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -67.639999\n","best mean reward -67.639999\n","running time 1188.439543\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -67.63999938964844\n","Train_BestReturn : -67.63999938964844\n","TimeSinceStart : 1188.4395430088043\n","Exploration Critic Loss : 0.9031175971031189\n","Exploitation Critic Loss : 0.19749373197555542\n","Exploration Model Loss : 8.118126424960792e-05\n","Actor Loss : 1.3944733142852783\n","Eval_AverageReturn : -83.58333587646484\n","Eval_StdReturn : 28.825504302978516\n","Eval_MaxReturn : -48.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 84.5\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -52.119999\n","best mean reward -52.119999\n","running time 1289.776650\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -52.119998931884766\n","Train_BestReturn : -52.119998931884766\n","TimeSinceStart : 1289.7766497135162\n","Exploration Critic Loss : 1.4926847219467163\n","Exploitation Critic Loss : 0.15486733615398407\n","Exploration Model Loss : 7.183520210674033e-05\n","Actor Loss : 1.322660207748413\n","Eval_AverageReturn : -105.80000305175781\n","Eval_StdReturn : 39.79648208618164\n","Eval_MaxReturn : -46.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 106.5\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -43.990002\n","best mean reward -43.990002\n","running time 1386.174770\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -43.9900016784668\n","Train_BestReturn : -43.9900016784668\n","TimeSinceStart : 1386.174770116806\n","Exploration Critic Loss : 1.5322985649108887\n","Exploitation Critic Loss : 0.24530337750911713\n","Exploration Model Loss : 2.6967807571054436e-05\n","Actor Loss : 1.3017133474349976\n","Eval_AverageReturn : -73.92857360839844\n","Eval_StdReturn : 31.031007766723633\n","Eval_MaxReturn : -33.0\n","Eval_MinReturn : -138.0\n","Eval_AverageEpLen : 74.92857142857143\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -37.660000\n","best mean reward -37.660000\n","running time 1485.362432\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -37.65999984741211\n","Train_BestReturn : -37.65999984741211\n","TimeSinceStart : 1485.362431526184\n","Exploration Critic Loss : 1.9506089687347412\n","Exploitation Critic Loss : 0.10591711848974228\n","Exploration Model Loss : 3.1811126973479986e-05\n","Actor Loss : 1.3405922651290894\n","Eval_AverageReturn : -91.36363983154297\n","Eval_StdReturn : 40.315284729003906\n","Eval_MaxReturn : -32.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 92.27272727272727\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -35.820000\n","best mean reward -35.820000\n","running time 1579.111144\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -35.81999969482422\n","Train_BestReturn : -35.81999969482422\n","TimeSinceStart : 1579.1111443042755\n","Exploration Critic Loss : 2.7260901927948\n","Exploitation Critic Loss : 0.1910666674375534\n","Exploration Model Loss : 1.9663995772134513e-05\n","Actor Loss : 1.3532618284225464\n","Eval_AverageReturn : -55.16666793823242\n","Eval_StdReturn : 17.02041244506836\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -98.0\n","Eval_AverageEpLen : 56.166666666666664\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -36.209999\n","best mean reward -35.820000\n","running time 1681.516159\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -36.209999084472656\n","Train_BestReturn : -35.81999969482422\n","TimeSinceStart : 1681.51615858078\n","Exploration Critic Loss : 3.5166802406311035\n","Exploitation Critic Loss : 0.15852582454681396\n","Exploration Model Loss : 7.192056364146993e-05\n","Actor Loss : 1.4041240215301514\n","Eval_AverageReturn : -62.1875\n","Eval_StdReturn : 19.44292449951172\n","Eval_MaxReturn : -39.0\n","Eval_MinReturn : -100.0\n","Eval_AverageEpLen : 63.1875\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -35.369999\n","best mean reward -35.369999\n","running time 1782.008193\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -35.369998931884766\n","Train_BestReturn : -35.369998931884766\n","TimeSinceStart : 1782.008192539215\n","Exploration Critic Loss : 4.72425651550293\n","Exploitation Critic Loss : 0.0996096283197403\n","Exploration Model Loss : 4.7090117732295766e-05\n","Actor Loss : 1.2688653469085693\n","Eval_AverageReturn : -53.157894134521484\n","Eval_StdReturn : 21.636381149291992\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -116.0\n","Eval_AverageEpLen : 54.1578947368421\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -34.990002\n","best mean reward -34.990002\n","running time 1884.649282\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -34.9900016784668\n","Train_BestReturn : -34.9900016784668\n","TimeSinceStart : 1884.6492817401886\n","Exploration Critic Loss : 10.662262916564941\n","Exploitation Critic Loss : 0.14222298562526703\n","Exploration Model Loss : 4.913906013825908e-05\n","Actor Loss : 1.1845158338546753\n","Eval_AverageReturn : -67.19999694824219\n","Eval_StdReturn : 32.60919952392578\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -141.0\n","Eval_AverageEpLen : 68.2\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -32.310001\n","best mean reward -32.310001\n","running time 1984.426187\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -32.310001373291016\n","Train_BestReturn : -32.310001373291016\n","TimeSinceStart : 1984.426186800003\n","Exploration Critic Loss : 16.848588943481445\n","Exploitation Critic Loss : 0.14203013479709625\n","Exploration Model Loss : 0.00011600874859141186\n","Actor Loss : 1.2443548440933228\n","Eval_AverageReturn : -49.04999923706055\n","Eval_StdReturn : 19.306669235229492\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -94.0\n","Eval_AverageEpLen : 50.05\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -31.170000\n","best mean reward -31.170000\n","running time 2086.091646\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -31.170000076293945\n","Train_BestReturn : -31.170000076293945\n","TimeSinceStart : 2086.091645717621\n","Exploration Critic Loss : 13.097210884094238\n","Exploitation Critic Loss : 0.09814231842756271\n","Exploration Model Loss : 0.00011142833682242781\n","Actor Loss : 1.2498611211776733\n","Eval_AverageReturn : -76.76923370361328\n","Eval_StdReturn : 32.81452178955078\n","Eval_MaxReturn : -35.0\n","Eval_MinReturn : -143.0\n","Eval_AverageEpLen : 77.76923076923077\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -29.030001\n","best mean reward -29.030001\n","running time 2183.055255\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -29.030000686645508\n","Train_BestReturn : -29.030000686645508\n","TimeSinceStart : 2183.055255174637\n","Exploration Critic Loss : 17.382278442382812\n","Exploitation Critic Loss : 0.18862825632095337\n","Exploration Model Loss : 0.00023056031204760075\n","Actor Loss : 1.216738224029541\n","Eval_AverageReturn : -58.411766052246094\n","Eval_StdReturn : 20.364498138427734\n","Eval_MaxReturn : -33.0\n","Eval_MinReturn : -110.0\n","Eval_AverageEpLen : 59.411764705882355\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -28.020000\n","best mean reward -28.020000\n","running time 2286.834255\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -28.020000457763672\n","Train_BestReturn : -28.020000457763672\n","TimeSinceStart : 2286.8342549800873\n","Exploration Critic Loss : 28.21112060546875\n","Exploitation Critic Loss : 0.14345243573188782\n","Exploration Model Loss : 2.9986600566189736e-05\n","Actor Loss : 1.2941865921020508\n","Eval_AverageReturn : -58.0\n","Eval_StdReturn : 29.78814125061035\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -138.0\n","Eval_AverageEpLen : 59.0\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -27.410000\n","best mean reward -27.410000\n","running time 2392.445215\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -27.40999984741211\n","Train_BestReturn : -27.40999984741211\n","TimeSinceStart : 2392.4452147483826\n","Exploration Critic Loss : 36.86017990112305\n","Exploitation Critic Loss : 0.1108194887638092\n","Exploration Model Loss : 6.406813918147236e-05\n","Actor Loss : 1.2225557565689087\n","Eval_AverageReturn : -54.31578826904297\n","Eval_StdReturn : 23.44097328186035\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -104.0\n","Eval_AverageEpLen : 55.31578947368421\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -27.450001\n","best mean reward -27.410000\n","running time 2494.598102\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -27.450000762939453\n","Train_BestReturn : -27.40999984741211\n","TimeSinceStart : 2494.5981016159058\n","Exploration Critic Loss : 37.703487396240234\n","Exploitation Critic Loss : 0.09536603093147278\n","Exploration Model Loss : 0.0003618004557210952\n","Actor Loss : 1.1795034408569336\n","Eval_AverageReturn : -48.19047546386719\n","Eval_StdReturn : 14.853631973266602\n","Eval_MaxReturn : -26.0\n","Eval_MinReturn : -79.0\n","Eval_AverageEpLen : 49.19047619047619\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -27.049999\n","best mean reward -27.049999\n","running time 2598.032308\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -27.049999237060547\n","Train_BestReturn : -27.049999237060547\n","TimeSinceStart : 2598.032308101654\n","Exploration Critic Loss : 41.646942138671875\n","Exploitation Critic Loss : 0.10816474258899689\n","Exploration Model Loss : 0.00020009397121611983\n","Actor Loss : 1.1778067350387573\n","Eval_AverageReturn : -54.83333206176758\n","Eval_StdReturn : 28.03420639038086\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 55.77777777777778\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -27.170000\n","best mean reward -27.049999\n","running time 2697.661901\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -27.170000076293945\n","Train_BestReturn : -27.049999237060547\n","TimeSinceStart : 2697.6619005203247\n","Exploration Critic Loss : 38.908206939697266\n","Exploitation Critic Loss : 0.10817635804414749\n","Exploration Model Loss : 0.0003563613863661885\n","Actor Loss : 1.1284301280975342\n","Eval_AverageReturn : -47.09090805053711\n","Eval_StdReturn : 16.32318115234375\n","Eval_MaxReturn : -21.0\n","Eval_MinReturn : -78.0\n","Eval_AverageEpLen : 48.09090909090909\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -27.059999\n","best mean reward -27.049999\n","running time 2798.713084\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -27.059999465942383\n","Train_BestReturn : -27.049999237060547\n","TimeSinceStart : 2798.7130835056305\n","Exploration Critic Loss : 57.038841247558594\n","Exploitation Critic Loss : 0.1492927074432373\n","Exploration Model Loss : 2.042798041657079e-05\n","Actor Loss : 1.1933104991912842\n","Eval_AverageReturn : -38.07692337036133\n","Eval_StdReturn : 10.321460723876953\n","Eval_MaxReturn : -21.0\n","Eval_MinReturn : -59.0\n","Eval_AverageEpLen : 39.07692307692308\n","Buffer size : 30001\n","Done logging...\n","\n","\n","Traceback (most recent call last):\n","  File \"cs285/scripts/run_hw5_awac.py\", line 142, in <module>\n","    main()\n","  File \"cs285/scripts/run_hw5_awac.py\", line 138, in main\n","    trainer.run_training_loop()\n","  File \"cs285/scripts/run_hw5_awac.py\", line 39, in run_training_loop\n","    eval_policy = self.rl_trainer.agent.actor,\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py\", line 201, in run_training_loop\n","    all_logs = self.train_agent()\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py\", line 260, in train_agent\n","    train_log = self.agent.train(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/agents/awac_agent.py\", line 136, in train\n","    actor_loss = self.awac_actor.update(ob_no, ac_na, advantage)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/policies/MLP_policy.py\", line 177, in update\n","    actor_loss.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 396, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 175, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n"]}],"source":["!python cs285/scripts/run_hw5_awac.py --env_name PointmassMedium-v0 --use_rnd \\\n"," --num_exploration_steps=20000 --awac_lambda=2 \\\n"," --exp_name q4_awac_medium_supervised_lam2"]},{"cell_type":"markdown","metadata":{"id":"HsLRMLpo1I6O"},"source":["## 5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2142257,"status":"ok","timestamp":1669136999259,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"PdUwsWWcswXB","outputId":"24a269bf-474c-4a41-e723-a8d3a4b77c58"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_easy_supervised_lam2_tau0.7_PointmassEasy-v0_22-11-2022_16-34-18 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_easy_supervised_lam2_tau0.7_PointmassEasy-v0_22-11-2022_16-34-18\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.001821\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.001821279525756836\n","Eval_AverageReturn : -49.52381134033203\n","Eval_StdReturn : 2.1295883655548096\n","Eval_MaxReturn : -40.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 49.57142857142857\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -50.000000\n","best mean reward -inf\n","running time 12.180299\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -50.0\n","TimeSinceStart : 12.180298805236816\n","Eval_AverageReturn : -49.85714340209961\n","Eval_StdReturn : 0.6388765573501587\n","Eval_MaxReturn : -47.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 49.904761904761905\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -50.000000\n","best mean reward -inf\n","running time 27.031078\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -50.0\n","TimeSinceStart : 27.031078100204468\n","Eval_AverageReturn : -49.0\n","Eval_StdReturn : 4.4721360206604\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 49.04761904761905\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -50.000000\n","best mean reward -inf\n","running time 98.717557\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -50.0\n","TimeSinceStart : 98.7175567150116\n","Exploration Critic Loss : 0.14720404148101807\n","Exploitation Critic Q Loss : 0.00032105096033774316\n","Exploitation Critic V Loss : 4.999988595955074e-05\n","Exploration Model Loss : 0.010279985144734383\n","Actor Loss : 1.5779913663864136\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -49.900002\n","best mean reward -inf\n","running time 168.167138\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -49.900001525878906\n","TimeSinceStart : 168.167138338089\n","Exploration Critic Loss : 0.2143275886774063\n","Exploitation Critic Q Loss : 9.520260209683329e-05\n","Exploitation Critic V Loss : 2.5993926101364195e-05\n","Exploration Model Loss : 0.0012151787523180246\n","Actor Loss : 1.4933956861495972\n","Eval_AverageReturn : -48.47618865966797\n","Eval_StdReturn : 3.7495272159576416\n","Eval_MaxReturn : -38.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 48.61904761904762\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -49.810001\n","best mean reward -inf\n","running time 241.793630\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -49.810001373291016\n","TimeSinceStart : 241.79362988471985\n","Exploration Critic Loss : 0.21245086193084717\n","Exploitation Critic Q Loss : 0.003119682427495718\n","Exploitation Critic V Loss : 2.3946751753101125e-05\n","Exploration Model Loss : 0.0011449296725913882\n","Actor Loss : 1.409279227256775\n","Eval_AverageReturn : -48.52381134033203\n","Eval_StdReturn : 4.97248649597168\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 48.61904761904762\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -49.430000\n","best mean reward -49.430000\n","running time 313.946297\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -49.43000030517578\n","Train_BestReturn : -49.43000030517578\n","TimeSinceStart : 313.9462971687317\n","Exploration Critic Loss : 0.220185786485672\n","Exploitation Critic Q Loss : 7.484015077352524e-05\n","Exploitation Critic V Loss : 2.352238880121149e-05\n","Exploration Model Loss : 0.0030873229261487722\n","Actor Loss : 1.4171504974365234\n","Eval_AverageReturn : -49.0\n","Eval_StdReturn : 2.7945525646209717\n","Eval_MaxReturn : -38.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 49.142857142857146\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -46.580002\n","best mean reward -46.580002\n","running time 385.427570\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -46.58000183105469\n","Train_BestReturn : -46.58000183105469\n","TimeSinceStart : 385.4275698661804\n","Exploration Critic Loss : 0.14977216720581055\n","Exploitation Critic Q Loss : 0.001048662350513041\n","Exploitation Critic V Loss : 7.255777018144727e-05\n","Exploration Model Loss : 0.000587749236728996\n","Actor Loss : 1.47971773147583\n","Eval_AverageReturn : -48.52381134033203\n","Eval_StdReturn : 3.935419797897339\n","Eval_MaxReturn : -35.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 48.666666666666664\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -44.950001\n","best mean reward -44.950001\n","running time 458.029681\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -44.95000076293945\n","Train_BestReturn : -44.95000076293945\n","TimeSinceStart : 458.0296812057495\n","Exploration Critic Loss : 0.26369911432266235\n","Exploitation Critic Q Loss : 0.003843629965558648\n","Exploitation Critic V Loss : 0.0003805723099503666\n","Exploration Model Loss : 0.0011687591904774308\n","Actor Loss : 1.3329761028289795\n","Eval_AverageReturn : -48.80952453613281\n","Eval_StdReturn : 4.6966962814331055\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 48.904761904761905\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -36.119999\n","best mean reward -36.119999\n","running time 534.141927\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -36.119998931884766\n","Train_BestReturn : -36.119998931884766\n","TimeSinceStart : 534.141927242279\n","Exploration Critic Loss : 0.3447950482368469\n","Exploitation Critic Q Loss : 0.0037515561562031507\n","Exploitation Critic V Loss : 0.0003977739834226668\n","Exploration Model Loss : 0.0006505728233605623\n","Actor Loss : 1.3416643142700195\n","Eval_AverageReturn : -47.80952453613281\n","Eval_StdReturn : 5.720631122589111\n","Eval_MaxReturn : -27.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 48.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -27.389999\n","best mean reward -27.389999\n","running time 611.450584\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -27.389999389648438\n","Train_BestReturn : -27.389999389648438\n","TimeSinceStart : 611.4505839347839\n","Exploration Critic Loss : 0.28538697957992554\n","Exploitation Critic Q Loss : 0.001984360394999385\n","Exploitation Critic V Loss : 0.0004176411312073469\n","Exploration Model Loss : 0.0003264530096203089\n","Actor Loss : 1.4250915050506592\n","Eval_AverageReturn : -43.434783935546875\n","Eval_StdReturn : 10.171683311462402\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 43.78260869565217\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -20.250000\n","best mean reward -20.250000\n","running time 688.028223\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -20.25\n","Train_BestReturn : -20.25\n","TimeSinceStart : 688.0282230377197\n","Exploration Critic Loss : 0.44476938247680664\n","Exploitation Critic Q Loss : 0.0032291219104081392\n","Exploitation Critic V Loss : 0.0007664988515898585\n","Exploration Model Loss : 0.0005211883690208197\n","Actor Loss : 1.3913440704345703\n","Eval_AverageReturn : -39.68000030517578\n","Eval_StdReturn : 8.748577117919922\n","Eval_MaxReturn : -21.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 40.4\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -18.940001\n","best mean reward -18.940001\n","running time 765.194559\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -18.940000534057617\n","Train_BestReturn : -18.940000534057617\n","TimeSinceStart : 765.19455909729\n","Exploration Critic Loss : 0.4379768967628479\n","Exploitation Critic Q Loss : 0.006206105463206768\n","Exploitation Critic V Loss : 0.0007038734620437026\n","Exploration Model Loss : 0.0001670675992500037\n","Actor Loss : 1.3418818712234497\n","Eval_AverageReturn : -43.869564056396484\n","Eval_StdReturn : 7.930702209472656\n","Eval_MaxReturn : -26.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 44.34782608695652\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -18.950001\n","best mean reward -18.940001\n","running time 840.297427\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -18.950000762939453\n","Train_BestReturn : -18.940000534057617\n","TimeSinceStart : 840.2974274158478\n","Exploration Critic Loss : 0.3948233723640442\n","Exploitation Critic Q Loss : 0.004333973862230778\n","Exploitation Critic V Loss : 0.0008445312269032001\n","Exploration Model Loss : 0.00015024600725155324\n","Actor Loss : 1.2691859006881714\n","Eval_AverageReturn : -40.279998779296875\n","Eval_StdReturn : 8.806906700134277\n","Eval_MaxReturn : -26.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 40.96\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -18.580000\n","best mean reward -18.580000\n","running time 921.879580\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -18.579999923706055\n","Train_BestReturn : -18.579999923706055\n","TimeSinceStart : 921.8795804977417\n","Exploration Critic Loss : 0.22664642333984375\n","Exploitation Critic Q Loss : 0.005703733302652836\n","Exploitation Critic V Loss : 0.0007338182185776532\n","Exploration Model Loss : 4.815051215700805e-05\n","Actor Loss : 1.201032280921936\n","Eval_AverageReturn : -34.75862121582031\n","Eval_StdReturn : 11.106553077697754\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 35.58620689655172\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -18.820000\n","best mean reward -18.580000\n","running time 998.237872\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -18.81999969482422\n","Train_BestReturn : -18.579999923706055\n","TimeSinceStart : 998.2378721237183\n","Exploration Critic Loss : 0.13543736934661865\n","Exploitation Critic Q Loss : 0.0023695293348282576\n","Exploitation Critic V Loss : 0.0004884855588898063\n","Exploration Model Loss : 3.6969719076296315e-05\n","Actor Loss : 1.1913480758666992\n","Eval_AverageReturn : -31.1875\n","Eval_StdReturn : 8.228902816772461\n","Eval_MaxReturn : -20.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 32.15625\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -18.309999\n","best mean reward -18.309999\n","running time 1075.751590\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -18.309999465942383\n","Train_BestReturn : -18.309999465942383\n","TimeSinceStart : 1075.7515904903412\n","Exploration Critic Loss : 0.09450408816337585\n","Exploitation Critic Q Loss : 0.0019221606198698282\n","Exploitation Critic V Loss : 0.0006002477020956576\n","Exploration Model Loss : 2.7912135919905268e-05\n","Actor Loss : 1.1518752574920654\n","Eval_AverageReturn : -35.60714340209961\n","Eval_StdReturn : 9.76999568939209\n","Eval_MaxReturn : -17.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 36.5\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -18.209999\n","best mean reward -18.209999\n","running time 1156.052119\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -18.209999084472656\n","Train_BestReturn : -18.209999084472656\n","TimeSinceStart : 1156.0521194934845\n","Exploration Critic Loss : 0.10042375326156616\n","Exploitation Critic Q Loss : 0.003407460870221257\n","Exploitation Critic V Loss : 0.0006335044745355844\n","Exploration Model Loss : 1.2996770237805322e-05\n","Actor Loss : 1.273614525794983\n","Eval_AverageReturn : -32.32258224487305\n","Eval_StdReturn : 10.458479881286621\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 33.193548387096776\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -18.299999\n","best mean reward -18.209999\n","running time 1236.627132\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -18.299999237060547\n","Train_BestReturn : -18.209999084472656\n","TimeSinceStart : 1236.6271324157715\n","Exploration Critic Loss : 0.15211941301822662\n","Exploitation Critic Q Loss : 0.002659811871126294\n","Exploitation Critic V Loss : 0.0005721579655073583\n","Exploration Model Loss : 1.591896034369711e-05\n","Actor Loss : 1.1786707639694214\n","Eval_AverageReturn : -33.29999923706055\n","Eval_StdReturn : 11.225418090820312\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 34.06666666666667\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -17.690001\n","best mean reward -17.690001\n","running time 1316.192922\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -17.690000534057617\n","Train_BestReturn : -17.690000534057617\n","TimeSinceStart : 1316.1929223537445\n","Exploration Critic Loss : 0.2818457782268524\n","Exploitation Critic Q Loss : 0.004158482886850834\n","Exploitation Critic V Loss : 0.0008110746857710183\n","Exploration Model Loss : 2.8657483198912814e-05\n","Actor Loss : 1.0908961296081543\n","Eval_AverageReturn : -30.5625\n","Eval_StdReturn : 10.14561939239502\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 31.4375\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -18.250000\n","best mean reward -17.690001\n","running time 1393.626772\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -18.25\n","Train_BestReturn : -17.690000534057617\n","TimeSinceStart : 1393.6267716884613\n","Exploration Critic Loss : 0.2382987141609192\n","Exploitation Critic Q Loss : 0.0036395464558154345\n","Exploitation Critic V Loss : 0.0008543537696823478\n","Exploration Model Loss : 3.071504761464894e-05\n","Actor Loss : 1.2167232036590576\n","Eval_AverageReturn : -27.742856979370117\n","Eval_StdReturn : 8.422224998474121\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -48.0\n","Eval_AverageEpLen : 28.742857142857144\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -18.150000\n","best mean reward -17.690001\n","running time 1477.381607\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -18.149999618530273\n","Train_BestReturn : -17.690000534057617\n","TimeSinceStart : 1477.3816068172455\n","Exploration Critic Loss : 0.09168218821287155\n","Exploitation Critic Q Loss : 0.0012837063986808062\n","Exploitation Critic V Loss : 0.000552599027287215\n","Exploration Model Loss : 4.608639937941916e-05\n","Actor Loss : 1.136716365814209\n","Eval_AverageReturn : -27.742856979370117\n","Eval_StdReturn : 8.553504943847656\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -49.0\n","Eval_AverageEpLen : 28.742857142857144\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -17.780001\n","best mean reward -17.690001\n","running time 1556.486864\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -17.780000686645508\n","Train_BestReturn : -17.690000534057617\n","TimeSinceStart : 1556.4868640899658\n","Exploration Critic Loss : 0.06652993708848953\n","Exploitation Critic Q Loss : 0.00678818067535758\n","Exploitation Critic V Loss : 0.0007989206933416426\n","Exploration Model Loss : 1.4208183529262897e-05\n","Actor Loss : 1.0793722867965698\n","Eval_AverageReturn : -23.14285659790039\n","Eval_StdReturn : 7.701488494873047\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -46.0\n","Eval_AverageEpLen : 24.142857142857142\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -17.730000\n","best mean reward -17.690001\n","running time 1640.256715\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -17.729999542236328\n","Train_BestReturn : -17.690000534057617\n","TimeSinceStart : 1640.2567145824432\n","Exploration Critic Loss : 0.09783745557069778\n","Exploitation Critic Q Loss : 0.002912630792707205\n","Exploitation Critic V Loss : 0.0005115554085932672\n","Exploration Model Loss : 0.000198669484234415\n","Actor Loss : 1.1450285911560059\n","Eval_AverageReturn : -26.216217041015625\n","Eval_StdReturn : 9.031881332397461\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 27.16216216216216\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -18.040001\n","best mean reward -17.690001\n","running time 1720.166232\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -18.040000915527344\n","Train_BestReturn : -17.690000534057617\n","TimeSinceStart : 1720.1662318706512\n","Exploration Critic Loss : 0.08095480501651764\n","Exploitation Critic Q Loss : 0.0018006745958700776\n","Exploitation Critic V Loss : 0.00038490528822876513\n","Exploration Model Loss : 5.398091525421478e-05\n","Actor Loss : 1.074652910232544\n","Eval_AverageReturn : -26.29729652404785\n","Eval_StdReturn : 10.213570594787598\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 27.243243243243242\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -18.250000\n","best mean reward -17.690001\n","running time 1798.812730\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -18.25\n","Train_BestReturn : -17.690000534057617\n","TimeSinceStart : 1798.8127300739288\n","Exploration Critic Loss : 0.09607860445976257\n","Exploitation Critic Q Loss : 0.00137005012948066\n","Exploitation Critic V Loss : 0.00036468025064095855\n","Exploration Model Loss : 0.0001537671487312764\n","Actor Loss : 1.023630976676941\n","Eval_AverageReturn : -24.897436141967773\n","Eval_StdReturn : 10.453303337097168\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 25.82051282051282\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -18.650000\n","best mean reward -17.690001\n","running time 1880.506586\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -18.649999618530273\n","Train_BestReturn : -17.690000534057617\n","TimeSinceStart : 1880.506585597992\n","Exploration Critic Loss : 0.06027783825993538\n","Exploitation Critic Q Loss : 0.0015677341725677252\n","Exploitation Critic V Loss : 0.000588598195463419\n","Exploration Model Loss : 4.458124749362469e-05\n","Actor Loss : 1.036111831665039\n","Eval_AverageReturn : -23.0\n","Eval_StdReturn : 8.67673397064209\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 23.976190476190474\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -17.900000\n","best mean reward -17.690001\n","running time 1960.506716\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -17.899999618530273\n","Train_BestReturn : -17.690000534057617\n","TimeSinceStart : 1960.5067155361176\n","Exploration Critic Loss : 0.13970120251178741\n","Exploitation Critic Q Loss : 0.0013065548846498132\n","Exploitation Critic V Loss : 0.0006864379392936826\n","Exploration Model Loss : 8.156751573551446e-05\n","Actor Loss : 1.0869176387786865\n","Eval_AverageReturn : -29.636363983154297\n","Eval_StdReturn : 9.011165618896484\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -48.0\n","Eval_AverageEpLen : 30.636363636363637\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -16.969999\n","best mean reward -16.969999\n","running time 2038.521914\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -16.969999313354492\n","Train_BestReturn : -16.969999313354492\n","TimeSinceStart : 2038.5219142436981\n","Exploration Critic Loss : 0.0832071304321289\n","Exploitation Critic Q Loss : 0.0016713663935661316\n","Exploitation Critic V Loss : 0.0007374355918727815\n","Exploration Model Loss : 0.00018281667144037783\n","Actor Loss : 1.067557692527771\n","Eval_AverageReturn : -25.205127716064453\n","Eval_StdReturn : 6.4497246742248535\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -42.0\n","Eval_AverageEpLen : 26.205128205128204\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -17.700001\n","best mean reward -16.969999\n","running time 2120.729047\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -17.700000762939453\n","Train_BestReturn : -16.969999313354492\n","TimeSinceStart : 2120.7290472984314\n","Exploration Critic Loss : 0.0951385498046875\n","Exploitation Critic Q Loss : 0.0017647637287154794\n","Exploitation Critic V Loss : 0.0004842098569497466\n","Exploration Model Loss : 0.00024855276569724083\n","Actor Loss : 1.064701795578003\n","Eval_AverageReturn : -25.5\n","Eval_StdReturn : 8.110260009765625\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -47.0\n","Eval_AverageEpLen : 26.5\n","Buffer size : 29001\n","Done logging...\n","\n","\n","Traceback (most recent call last):\n","  File \"cs285/scripts/run_hw5_iql.py\", line 143, in <module>\n","    main()\n","  File \"cs285/scripts/run_hw5_iql.py\", line 139, in main\n","    trainer.run_training_loop()\n","  File \"cs285/scripts/run_hw5_iql.py\", line 39, in run_training_loop\n","    eval_policy = self.rl_trainer.agent.actor,\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py\", line 201, in run_training_loop\n","    all_logs = self.train_agent()\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/infrastructure/rl_trainer_awac.py\", line 260, in train_agent\n","    train_log = self.agent.train(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/agents/iql_agent.py\", line 123, in train\n","    actor_loss = self.awac_actor.update(ob_no, ac_na, advantage)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/policies/MLP_policy.py\", line 173, in update\n","    dist = self(observations)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/policies/MLP_policy.py\", line 112, in forward\n","    logits = self.logits_na(observation)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\", line 139, in forward\n","    input = module(input)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","KeyboardInterrupt\n"]}],"source":["!python cs285/scripts/run_hw5_iql.py --env_name PointmassEasy-v0 \\\n"," --exp_name q5_iql_easy_supervised_lam2_tau0.7 --use_rnd \\\n"," --num_exploration_steps=20000 --awac_lambda=2 --iql_expectile=0.7 --seed 2"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"RoIT8bvCK1SM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669255665924,"user_tz":480,"elapsed":4023942,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"}},"outputId":"9426c2a0-47d2-4602-bb34-676d3444bc92"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_easy_unsupervised_lam2_tau0.7_PointmassEasy-v0_24-11-2022_01-00-43 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_easy_unsupervised_lam2_tau0.7_PointmassEasy-v0_24-11-2022_01-00-43\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002158\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0021581649780273438\n","Eval_AverageReturn : -49.42856979370117\n","Eval_StdReturn : 2.555506467819214\n","Eval_MaxReturn : -38.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 49.476190476190474\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -50.000000\n","best mean reward -inf\n","running time 12.533667\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -50.0\n","TimeSinceStart : 12.533667087554932\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -49.924999\n","best mean reward -inf\n","running time 25.182382\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -49.92499923706055\n","TimeSinceStart : 25.182382345199585\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -48.426231\n","best mean reward -inf\n","running time 98.578808\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -48.426231384277344\n","TimeSinceStart : 98.57880759239197\n","Exploration Critic Loss : 0.003396740648895502\n","Exploitation Critic Q Loss : 0.11763066798448563\n","Exploitation Critic V Loss : 0.000784897361882031\n","Exploration Model Loss : 0.04387068748474121\n","Actor Loss : 1.5605978965759277\n","Eval_AverageReturn : -50.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 50.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -45.712643\n","best mean reward -inf\n","running time 168.147397\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -45.712642669677734\n","TimeSinceStart : 168.14739727973938\n","Exploration Critic Loss : 0.03442136570811272\n","Exploitation Critic Q Loss : 0.47407034039497375\n","Exploitation Critic V Loss : 0.002793688327074051\n","Exploration Model Loss : 0.0006004972383379936\n","Actor Loss : 1.4531761407852173\n","Eval_AverageReturn : -47.318180084228516\n","Eval_StdReturn : 6.511432647705078\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 47.5\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -44.130001\n","best mean reward -44.130001\n","running time 240.370506\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -44.130001068115234\n","Train_BestReturn : -44.130001068115234\n","TimeSinceStart : 240.37050557136536\n","Exploration Critic Loss : 0.04845474660396576\n","Exploitation Critic Q Loss : 1.2200695276260376\n","Exploitation Critic V Loss : 0.013446973636746407\n","Exploration Model Loss : 0.0034113596193492413\n","Actor Loss : 1.3575432300567627\n","Eval_AverageReturn : -48.238094329833984\n","Eval_StdReturn : 5.353279113769531\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 48.38095238095238\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -39.849998\n","best mean reward -39.849998\n","running time 314.000097\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -39.849998474121094\n","Train_BestReturn : -39.849998474121094\n","TimeSinceStart : 314.0000967979431\n","Exploration Critic Loss : 0.0836370661854744\n","Exploitation Critic Q Loss : 0.8039623498916626\n","Exploitation Critic V Loss : 0.02645731531083584\n","Exploration Model Loss : 0.004186104517430067\n","Actor Loss : 1.3888026475906372\n","Eval_AverageReturn : -42.5\n","Eval_StdReturn : 8.986100196838379\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 43.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -31.459999\n","best mean reward -31.459999\n","running time 388.031122\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -31.459999084472656\n","Train_BestReturn : -31.459999084472656\n","TimeSinceStart : 388.0311222076416\n","Exploration Critic Loss : 0.11658491939306259\n","Exploitation Critic Q Loss : 0.5041370391845703\n","Exploitation Critic V Loss : 0.030865618959069252\n","Exploration Model Loss : 0.001386406016536057\n","Actor Loss : 1.2753190994262695\n","Eval_AverageReturn : -43.34782791137695\n","Eval_StdReturn : 8.28098201751709\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 43.82608695652174\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -22.410000\n","best mean reward -22.410000\n","running time 467.862254\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -22.40999984741211\n","Train_BestReturn : -22.40999984741211\n","TimeSinceStart : 467.8622543811798\n","Exploration Critic Loss : 0.11359328031539917\n","Exploitation Critic Q Loss : 0.9519721865653992\n","Exploitation Critic V Loss : 0.02366570383310318\n","Exploration Model Loss : 0.0010495345341041684\n","Actor Loss : 1.2986161708831787\n","Eval_AverageReturn : -39.400001525878906\n","Eval_StdReturn : 11.513470649719238\n","Eval_MaxReturn : -15.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 40.08\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -18.230000\n","best mean reward -18.230000\n","running time 543.850389\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -18.229999542236328\n","Train_BestReturn : -18.229999542236328\n","TimeSinceStart : 543.8503885269165\n","Exploration Critic Loss : 0.1868966668844223\n","Exploitation Critic Q Loss : 2.1574583053588867\n","Exploitation Critic V Loss : 0.07347913831472397\n","Exploration Model Loss : 0.0007197203231044114\n","Actor Loss : 1.3071235418319702\n","Eval_AverageReturn : -36.296295166015625\n","Eval_StdReturn : 11.197932243347168\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 37.111111111111114\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -18.010000\n","best mean reward -18.010000\n","running time 626.328235\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -18.010000228881836\n","Train_BestReturn : -18.010000228881836\n","TimeSinceStart : 626.3282351493835\n","Exploration Critic Loss : 0.1761631816625595\n","Exploitation Critic Q Loss : 0.9814746379852295\n","Exploitation Critic V Loss : 0.07185083627700806\n","Exploration Model Loss : 0.00040769169572740793\n","Actor Loss : 1.2543848752975464\n","Eval_AverageReturn : -35.21428680419922\n","Eval_StdReturn : 10.7582426071167\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 36.107142857142854\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -17.870001\n","best mean reward -17.870001\n","running time 706.406585\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -17.8700008392334\n","Train_BestReturn : -17.8700008392334\n","TimeSinceStart : 706.4065849781036\n","Exploration Critic Loss : 0.18122942745685577\n","Exploitation Critic Q Loss : 0.2747853398323059\n","Exploitation Critic V Loss : 0.07884863764047623\n","Exploration Model Loss : 0.0002795552718453109\n","Actor Loss : 1.1824630498886108\n","Eval_AverageReturn : -29.147058486938477\n","Eval_StdReturn : 10.31313419342041\n","Eval_MaxReturn : -16.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 30.058823529411764\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -17.200001\n","best mean reward -17.200001\n","running time 787.738065\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -17.200000762939453\n","Train_BestReturn : -17.200000762939453\n","TimeSinceStart : 787.7380645275116\n","Exploration Critic Loss : 0.2604328691959381\n","Exploitation Critic Q Loss : 1.2562251091003418\n","Exploitation Critic V Loss : 0.08445021510124207\n","Exploration Model Loss : 0.00013938592746853828\n","Actor Loss : 1.2092818021774292\n","Eval_AverageReturn : -28.941177368164062\n","Eval_StdReturn : 10.948922157287598\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 29.852941176470587\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -17.510000\n","best mean reward -17.200001\n","running time 869.727902\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -17.510000228881836\n","Train_BestReturn : -17.200000762939453\n","TimeSinceStart : 869.7279016971588\n","Exploration Critic Loss : 0.30834534764289856\n","Exploitation Critic Q Loss : 1.5333844423294067\n","Exploitation Critic V Loss : 0.07600963860750198\n","Exploration Model Loss : 0.00016548905114177614\n","Actor Loss : 1.1100456714630127\n","Eval_AverageReturn : -28.285715103149414\n","Eval_StdReturn : 9.352070808410645\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 29.228571428571428\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -17.639999\n","best mean reward -17.200001\n","running time 954.006000\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -17.639999389648438\n","Train_BestReturn : -17.200000762939453\n","TimeSinceStart : 954.0059995651245\n","Exploration Critic Loss : 0.2591315805912018\n","Exploitation Critic Q Loss : 1.159537672996521\n","Exploitation Critic V Loss : 0.0688076913356781\n","Exploration Model Loss : 0.00010942210792563856\n","Actor Loss : 1.1247254610061646\n","Eval_AverageReturn : -27.55555534362793\n","Eval_StdReturn : 11.12415885925293\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 28.444444444444443\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -18.090000\n","best mean reward -17.200001\n","running time 1032.849591\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -18.09000015258789\n","Train_BestReturn : -17.200000762939453\n","TimeSinceStart : 1032.8495905399323\n","Exploration Critic Loss : 0.31842654943466187\n","Exploitation Critic Q Loss : 1.2330094575881958\n","Exploitation Critic V Loss : 0.0700482577085495\n","Exploration Model Loss : 5.692099148291163e-05\n","Actor Loss : 1.1235483884811401\n","Eval_AverageReturn : -23.238094329833984\n","Eval_StdReturn : 9.012715339660645\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 24.19047619047619\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -17.870001\n","best mean reward -17.200001\n","running time 1117.831155\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -17.8700008392334\n","Train_BestReturn : -17.200000762939453\n","TimeSinceStart : 1117.8311545848846\n","Exploration Critic Loss : 0.2243659794330597\n","Exploitation Critic Q Loss : 1.0182056427001953\n","Exploitation Critic V Loss : 0.03789651393890381\n","Exploration Model Loss : 4.551859819912352e-05\n","Actor Loss : 1.0156536102294922\n","Eval_AverageReturn : -25.36842155456543\n","Eval_StdReturn : 7.603396415710449\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 26.342105263157894\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -17.730000\n","best mean reward -17.200001\n","running time 1200.362734\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -17.729999542236328\n","Train_BestReturn : -17.200000762939453\n","TimeSinceStart : 1200.3627338409424\n","Exploration Critic Loss : 0.19119006395339966\n","Exploitation Critic Q Loss : 0.22072461247444153\n","Exploitation Critic V Loss : 0.06845372915267944\n","Exploration Model Loss : 3.010324144270271e-05\n","Actor Loss : 1.0263619422912598\n","Eval_AverageReturn : -23.80487823486328\n","Eval_StdReturn : 9.507941246032715\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 24.78048780487805\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -17.490000\n","best mean reward -17.200001\n","running time 1284.551896\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -17.489999771118164\n","Train_BestReturn : -17.200000762939453\n","TimeSinceStart : 1284.5518963336945\n","Exploration Critic Loss : 0.17193830013275146\n","Exploitation Critic Q Loss : 0.5154513120651245\n","Exploitation Critic V Loss : 0.07588107138872147\n","Exploration Model Loss : 2.097435026371386e-05\n","Actor Loss : 1.0007095336914062\n","Eval_AverageReturn : -25.657894134521484\n","Eval_StdReturn : 10.863450050354004\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 26.57894736842105\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -17.090000\n","best mean reward -17.090000\n","running time 1364.904360\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -17.09000015258789\n","Train_BestReturn : -17.09000015258789\n","TimeSinceStart : 1364.9043595790863\n","Exploration Critic Loss : 0.24797505140304565\n","Exploitation Critic Q Loss : 0.2594224810600281\n","Exploitation Critic V Loss : 0.06290636956691742\n","Exploration Model Loss : 0.0002745245292317122\n","Actor Loss : 1.0068037509918213\n","Eval_AverageReturn : -21.511110305786133\n","Eval_StdReturn : 6.920572280883789\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -43.0\n","Eval_AverageEpLen : 22.511111111111113\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -17.100000\n","best mean reward -17.090000\n","running time 1447.456080\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -17.100000381469727\n","Train_BestReturn : -17.09000015258789\n","TimeSinceStart : 1447.4560804367065\n","Exploration Critic Loss : 0.14771459996700287\n","Exploitation Critic Q Loss : 0.7536932826042175\n","Exploitation Critic V Loss : 0.05510133504867554\n","Exploration Model Loss : 0.0001552281028125435\n","Actor Loss : 1.0777831077575684\n","Eval_AverageReturn : -25.30769157409668\n","Eval_StdReturn : 9.492652893066406\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 26.28205128205128\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -17.420000\n","best mean reward -17.090000\n","running time 1530.802910\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -17.420000076293945\n","Train_BestReturn : -17.09000015258789\n","TimeSinceStart : 1530.8029100894928\n","Exploration Critic Loss : 0.16301371157169342\n","Exploitation Critic Q Loss : 0.2107626348733902\n","Exploitation Critic V Loss : 0.0679880902171135\n","Exploration Model Loss : 0.00010563606338109821\n","Actor Loss : 1.0446254014968872\n","Eval_AverageReturn : -26.37837791442871\n","Eval_StdReturn : 9.774072647094727\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -44.0\n","Eval_AverageEpLen : 27.37837837837838\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -17.180000\n","best mean reward -17.090000\n","running time 1609.559314\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -17.18000030517578\n","Train_BestReturn : -17.09000015258789\n","TimeSinceStart : 1609.5593135356903\n","Exploration Critic Loss : 0.10292495787143707\n","Exploitation Critic Q Loss : 0.2630826234817505\n","Exploitation Critic V Loss : 0.05440155044198036\n","Exploration Model Loss : 0.00014543204451911151\n","Actor Loss : 1.0025951862335205\n","Eval_AverageReturn : -23.309524536132812\n","Eval_StdReturn : 7.216014385223389\n","Eval_MaxReturn : -14.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 24.285714285714285\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -17.090000\n","best mean reward -17.090000\n","running time 1694.787018\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -17.09000015258789\n","Train_BestReturn : -17.09000015258789\n","TimeSinceStart : 1694.7870178222656\n","Exploration Critic Loss : 0.10028927028179169\n","Exploitation Critic Q Loss : 0.22174301743507385\n","Exploitation Critic V Loss : 0.06531979888677597\n","Exploration Model Loss : 0.0003556642332114279\n","Actor Loss : 0.9299870133399963\n","Eval_AverageReturn : -21.10869598388672\n","Eval_StdReturn : 6.290877819061279\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -40.0\n","Eval_AverageEpLen : 22.108695652173914\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -16.620001\n","best mean reward -16.620001\n","running time 1778.350795\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -16.6200008392334\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 1778.3507952690125\n","Exploration Critic Loss : 0.1693490743637085\n","Exploitation Critic Q Loss : 1.5191627740859985\n","Exploitation Critic V Loss : 0.07221375405788422\n","Exploration Model Loss : 0.00013942016812507063\n","Actor Loss : 1.1142752170562744\n","Eval_AverageReturn : -22.674419403076172\n","Eval_StdReturn : 6.947380065917969\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 23.651162790697676\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -16.990000\n","best mean reward -16.620001\n","running time 1861.866458\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -16.989999771118164\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 1861.8664577007294\n","Exploration Critic Loss : 0.12429440766572952\n","Exploitation Critic Q Loss : 0.8053376078605652\n","Exploitation Critic V Loss : 0.05999423563480377\n","Exploration Model Loss : 0.00018411276687402278\n","Actor Loss : 0.9570344090461731\n","Eval_AverageReturn : -22.65116310119629\n","Eval_StdReturn : 7.9822001457214355\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -46.0\n","Eval_AverageEpLen : 23.651162790697676\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -16.850000\n","best mean reward -16.620001\n","running time 1945.818537\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -16.850000381469727\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 1945.8185365200043\n","Exploration Critic Loss : 0.1130756139755249\n","Exploitation Critic Q Loss : 0.17927062511444092\n","Exploitation Critic V Loss : 0.06330272555351257\n","Exploration Model Loss : 3.50974514731206e-05\n","Actor Loss : 0.9516533017158508\n","Eval_AverageReturn : -20.826086044311523\n","Eval_StdReturn : 5.753276348114014\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -44.0\n","Eval_AverageEpLen : 21.82608695652174\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -17.480000\n","best mean reward -16.620001\n","running time 2033.142712\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -17.479999542236328\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 2033.1427121162415\n","Exploration Critic Loss : 0.11634034663438797\n","Exploitation Critic Q Loss : 0.2565675973892212\n","Exploitation Critic V Loss : 0.0618104450404644\n","Exploration Model Loss : 0.00029549439204856753\n","Actor Loss : 1.0165925025939941\n","Eval_AverageReturn : -22.090909957885742\n","Eval_StdReturn : 7.13766622543335\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -44.0\n","Eval_AverageEpLen : 23.09090909090909\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -17.400000\n","best mean reward -16.620001\n","running time 2115.370482\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -17.399999618530273\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 2115.370482444763\n","Exploration Critic Loss : 0.07746534049510956\n","Exploitation Critic Q Loss : 0.6468843817710876\n","Exploitation Critic V Loss : 0.05025259032845497\n","Exploration Model Loss : 5.0480099162086844e-05\n","Actor Loss : 0.955557644367218\n","Eval_AverageReturn : -21.311111450195312\n","Eval_StdReturn : 6.568687438964844\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -44.0\n","Eval_AverageEpLen : 22.31111111111111\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -17.440001\n","best mean reward -16.620001\n","running time 2199.524420\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -17.440000534057617\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 2199.5244204998016\n","Exploration Critic Loss : 0.04175308346748352\n","Exploitation Critic Q Loss : 0.22710533440113068\n","Exploitation Critic V Loss : 0.062154337763786316\n","Exploration Model Loss : 0.00013295987446326762\n","Actor Loss : 0.844401478767395\n","Eval_AverageReturn : -23.536584854125977\n","Eval_StdReturn : 9.899735450744629\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -50.0\n","Eval_AverageEpLen : 24.51219512195122\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -16.969999\n","best mean reward -16.620001\n","running time 2283.791381\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -16.969999313354492\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 2283.7913806438446\n","Exploration Critic Loss : 0.063821941614151\n","Exploitation Critic Q Loss : 0.9520887136459351\n","Exploitation Critic V Loss : 0.05014960840344429\n","Exploration Model Loss : 3.901537638739683e-06\n","Actor Loss : 0.8945996761322021\n","Eval_AverageReturn : -21.173913955688477\n","Eval_StdReturn : 6.8597893714904785\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -42.0\n","Eval_AverageEpLen : 22.17391304347826\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -17.240000\n","best mean reward -16.620001\n","running time 2375.933450\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -17.239999771118164\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 2375.933450460434\n","Exploration Critic Loss : 0.030581679195165634\n","Exploitation Critic Q Loss : 0.20498661696910858\n","Exploitation Critic V Loss : 0.07151051610708237\n","Exploration Model Loss : 9.341398254036903e-05\n","Actor Loss : 0.9754958748817444\n","Eval_AverageReturn : -22.395349502563477\n","Eval_StdReturn : 6.972169399261475\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -39.0\n","Eval_AverageEpLen : 23.3953488372093\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -17.219999\n","best mean reward -16.620001\n","running time 2458.794636\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -17.219999313354492\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 2458.7946360111237\n","Exploration Critic Loss : 0.038933463394641876\n","Exploitation Critic Q Loss : 0.20286458730697632\n","Exploitation Critic V Loss : 0.05763738229870796\n","Exploration Model Loss : 0.00011157421249663457\n","Actor Loss : 0.9071038961410522\n","Eval_AverageReturn : -20.36170196533203\n","Eval_StdReturn : 5.199940204620361\n","Eval_MaxReturn : -13.0\n","Eval_MinReturn : -37.0\n","Eval_AverageEpLen : 21.361702127659573\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -17.280001\n","best mean reward -16.620001\n","running time 2547.896189\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -17.280000686645508\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 2547.896189212799\n","Exploration Critic Loss : 0.05420718342065811\n","Exploitation Critic Q Loss : 0.2960836887359619\n","Exploitation Critic V Loss : 0.051146868616342545\n","Exploration Model Loss : 0.00011420722876209766\n","Actor Loss : 0.901939332485199\n","Eval_AverageReturn : -21.81818199157715\n","Eval_StdReturn : 6.942205429077148\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -42.0\n","Eval_AverageEpLen : 22.818181818181817\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -17.160000\n","best mean reward -16.620001\n","running time 2633.134218\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -17.15999984741211\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 2633.1342175006866\n","Exploration Critic Loss : 0.030338553711771965\n","Exploitation Critic Q Loss : 0.22803030908107758\n","Exploitation Critic V Loss : 0.06565862149000168\n","Exploration Model Loss : 0.0018019691342487931\n","Actor Loss : 0.8776898980140686\n","Eval_AverageReturn : -18.901960372924805\n","Eval_StdReturn : 5.0264201164245605\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -34.0\n","Eval_AverageEpLen : 19.901960784313726\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -17.000000\n","best mean reward -16.620001\n","running time 2720.803282\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -17.0\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 2720.803282022476\n","Exploration Critic Loss : 0.024315902963280678\n","Exploitation Critic Q Loss : 0.2006375789642334\n","Exploitation Critic V Loss : 0.05472143366932869\n","Exploration Model Loss : 5.676412001776043e-06\n","Actor Loss : 0.907503068447113\n","Eval_AverageReturn : -20.782608032226562\n","Eval_StdReturn : 6.192256927490234\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -39.0\n","Eval_AverageEpLen : 21.782608695652176\n","Buffer size : 35001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -17.520000\n","best mean reward -16.620001\n","running time 2804.860175\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -17.520000457763672\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 2804.860174894333\n","Exploration Critic Loss : 0.03515513613820076\n","Exploitation Critic Q Loss : 0.2113177329301834\n","Exploitation Critic V Loss : 0.05528169870376587\n","Exploration Model Loss : 8.050016731431242e-06\n","Actor Loss : 0.8665546178817749\n","Eval_AverageReturn : -19.360000610351562\n","Eval_StdReturn : 5.245035648345947\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -32.0\n","Eval_AverageEpLen : 20.36\n","Buffer size : 36001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -17.889999\n","best mean reward -16.620001\n","running time 2891.384946\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -17.889999389648438\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 2891.384945631027\n","Exploration Critic Loss : 0.03707604855298996\n","Exploitation Critic Q Loss : 0.5447789430618286\n","Exploitation Critic V Loss : 0.03455302119255066\n","Exploration Model Loss : 2.064850377792027e-05\n","Actor Loss : 0.7122809290885925\n","Eval_AverageReturn : -19.89583396911621\n","Eval_StdReturn : 5.300312519073486\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -35.0\n","Eval_AverageEpLen : 20.895833333333332\n","Buffer size : 37001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -17.600000\n","best mean reward -16.620001\n","running time 2979.973278\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -17.600000381469727\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 2979.973278284073\n","Exploration Critic Loss : 0.02911367639899254\n","Exploitation Critic Q Loss : 0.21677929162979126\n","Exploitation Critic V Loss : 0.06107857823371887\n","Exploration Model Loss : 5.026543476560619e-06\n","Actor Loss : 0.9895325899124146\n","Eval_AverageReturn : -17.796297073364258\n","Eval_StdReturn : 4.05690860748291\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -30.0\n","Eval_AverageEpLen : 18.796296296296298\n","Buffer size : 38001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -17.450001\n","best mean reward -16.620001\n","running time 3064.423996\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -17.450000762939453\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 3064.4239962100983\n","Exploration Critic Loss : 0.013532690703868866\n","Exploitation Critic Q Loss : 0.2123977243900299\n","Exploitation Critic V Loss : 0.050752196460962296\n","Exploration Model Loss : 0.0001766976056387648\n","Actor Loss : 0.9338894486427307\n","Eval_AverageReturn : -19.40816307067871\n","Eval_StdReturn : 5.760238170623779\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -36.0\n","Eval_AverageEpLen : 20.408163265306122\n","Buffer size : 39001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -17.620001\n","best mean reward -16.620001\n","running time 3153.110482\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -17.6200008392334\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 3153.1104822158813\n","Exploration Critic Loss : 0.0215713232755661\n","Exploitation Critic Q Loss : 0.23237010836601257\n","Exploitation Critic V Loss : 0.06642121821641922\n","Exploration Model Loss : 1.4868894027131319e-07\n","Actor Loss : 1.0097129344940186\n","Eval_AverageReturn : -19.489795684814453\n","Eval_StdReturn : 5.288116455078125\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -40.0\n","Eval_AverageEpLen : 20.489795918367346\n","Buffer size : 40001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -17.629999\n","best mean reward -16.620001\n","running time 3239.498558\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -17.6299991607666\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 3239.498557806015\n","Exploration Critic Loss : 0.011708779260516167\n","Exploitation Critic Q Loss : 0.20298293232917786\n","Exploitation Critic V Loss : 0.05976684391498566\n","Exploration Model Loss : 1.7339215219180915e-06\n","Actor Loss : 0.9137739539146423\n","Eval_AverageReturn : -19.200000762939453\n","Eval_StdReturn : 5.858327388763428\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -41.0\n","Eval_AverageEpLen : 20.2\n","Buffer size : 41001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -17.850000\n","best mean reward -16.620001\n","running time 3327.556222\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -17.850000381469727\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 3327.5562217235565\n","Exploration Critic Loss : 0.00796076562255621\n","Exploitation Critic Q Loss : 0.45663776993751526\n","Exploitation Critic V Loss : 0.038446344435214996\n","Exploration Model Loss : 4.1982514176197583e-07\n","Actor Loss : 0.8031201362609863\n","Eval_AverageReturn : -19.89583396911621\n","Eval_StdReturn : 5.602230548858643\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -45.0\n","Eval_AverageEpLen : 20.895833333333332\n","Buffer size : 42001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -17.719999\n","best mean reward -16.620001\n","running time 3410.570329\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -17.719999313354492\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 3410.570329427719\n","Exploration Critic Loss : 0.005880112759768963\n","Exploitation Critic Q Loss : 0.20867608487606049\n","Exploitation Critic V Loss : 0.05762863904237747\n","Exploration Model Loss : 0.0009205762180499732\n","Actor Loss : 0.8220740556716919\n","Eval_AverageReturn : -20.10416603088379\n","Eval_StdReturn : 5.335570335388184\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -41.0\n","Eval_AverageEpLen : 21.104166666666668\n","Buffer size : 43001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -17.700001\n","best mean reward -16.620001\n","running time 3499.758689\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -17.700000762939453\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 3499.7586891651154\n","Exploration Critic Loss : 0.0050737252458930016\n","Exploitation Critic Q Loss : 0.1949964165687561\n","Exploitation Critic V Loss : 0.06055591255426407\n","Exploration Model Loss : 1.168667495221598e-06\n","Actor Loss : 0.882998526096344\n","Eval_AverageReturn : -20.1875\n","Eval_StdReturn : 5.768651962280273\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -42.0\n","Eval_AverageEpLen : 21.1875\n","Buffer size : 44001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -16.990000\n","best mean reward -16.620001\n","running time 3585.631592\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -16.989999771118164\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 3585.631591796875\n","Exploration Critic Loss : 0.004583073779940605\n","Exploitation Critic Q Loss : 0.49416953325271606\n","Exploitation Critic V Loss : 0.04119178280234337\n","Exploration Model Loss : 4.1877498802023183e-07\n","Actor Loss : 0.7939272522926331\n","Eval_AverageReturn : -21.622222900390625\n","Eval_StdReturn : 6.357110023498535\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -36.0\n","Eval_AverageEpLen : 22.622222222222224\n","Buffer size : 45001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -16.940001\n","best mean reward -16.620001\n","running time 3672.700715\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -16.940000534057617\n","Train_BestReturn : -16.6200008392334\n","TimeSinceStart : 3672.7007145881653\n","Exploration Critic Loss : 0.007509083952754736\n","Exploitation Critic Q Loss : 0.354536235332489\n","Exploitation Critic V Loss : 0.05068032443523407\n","Exploration Model Loss : 0.00030524993781000376\n","Actor Loss : 0.8944481015205383\n","Eval_AverageReturn : -19.836734771728516\n","Eval_StdReturn : 5.219066143035889\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -32.0\n","Eval_AverageEpLen : 20.836734693877553\n","Buffer size : 46001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -16.559999\n","best mean reward -16.559999\n","running time 3758.620102\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -16.559999465942383\n","Train_BestReturn : -16.559999465942383\n","TimeSinceStart : 3758.620101928711\n","Exploration Critic Loss : 0.0018354932544752955\n","Exploitation Critic Q Loss : 0.20063433051109314\n","Exploitation Critic V Loss : 0.07381414622068405\n","Exploration Model Loss : 0.0006270379526540637\n","Actor Loss : 0.9486784934997559\n","Eval_AverageReturn : -19.571428298950195\n","Eval_StdReturn : 5.948760986328125\n","Eval_MaxReturn : -12.0\n","Eval_MinReturn : -42.0\n","Eval_AverageEpLen : 20.571428571428573\n","Buffer size : 47001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -16.780001\n","best mean reward -16.559999\n","running time 3847.948528\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -16.780000686645508\n","Train_BestReturn : -16.559999465942383\n","TimeSinceStart : 3847.948527574539\n","Exploration Critic Loss : 0.0016998602077364922\n","Exploitation Critic Q Loss : 0.18796926736831665\n","Exploitation Critic V Loss : 0.04324648156762123\n","Exploration Model Loss : 1.186729332403047e-05\n","Actor Loss : 0.7982092499732971\n","Eval_AverageReturn : -19.420000076293945\n","Eval_StdReturn : 5.028280258178711\n","Eval_MaxReturn : -11.0\n","Eval_MinReturn : -34.0\n","Eval_AverageEpLen : 20.42\n","Buffer size : 48001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -16.700001\n","best mean reward -16.559999\n","running time 3934.212924\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -16.700000762939453\n","Train_BestReturn : -16.559999465942383\n","TimeSinceStart : 3934.2129237651825\n","Exploration Critic Loss : 0.002219569170847535\n","Exploitation Critic Q Loss : 0.21038487553596497\n","Exploitation Critic V Loss : 0.04656992480158806\n","Exploration Model Loss : 4.2479496187297627e-07\n","Actor Loss : 0.8439709544181824\n","Eval_AverageReturn : -19.693878173828125\n","Eval_StdReturn : 6.031225204467773\n","Eval_MaxReturn : -10.0\n","Eval_MinReturn : -42.0\n","Eval_AverageEpLen : 20.693877551020407\n","Buffer size : 49001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_iql.py --env_name PointmassEasy-v0 \\\n","--exp_name q5_easy_unsupervised_lam2_tau0.7 --use_rnd \\\n","--unsupervised_exploration \\\n","--num_exploration_steps=20000 \\\n","--awac_lambda=2 \\\n","--iql_expectile=0.7 \\\n","--seed 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3289103,"status":"ok","timestamp":1669200637997,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"vsotN-YNHk4U","outputId":"07724627-7229-4be0-ad50-ff252cb82dbc"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/My Drive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_medium_supervised_lam0.5_tau0.5_PointmassMedium-v0_23-11-2022_09-55-51 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/My Drive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_medium_supervised_lam0.5_tau0.5_PointmassMedium-v0_23-11-2022_09-55-51\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002116\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0021164417266845703\n","Eval_AverageReturn : -133.375\n","Eval_StdReturn : 32.353275299072266\n","Eval_MaxReturn : -54.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 133.625\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 6.734739\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 6.73473858833313\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 13.847452\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 13.84745168685913\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -148.750000\n","best mean reward -inf\n","running time 81.181973\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -148.75\n","TimeSinceStart : 81.18197321891785\n","Exploration Critic Loss : 0.025944199413061142\n","Exploitation Critic Q Loss : 0.05855872482061386\n","Exploitation Critic V Loss : 0.00023224472533911467\n","Exploration Model Loss : 0.015384295955300331\n","Actor Loss : 1.5400896072387695\n","Eval_AverageReturn : -146.2857208251953\n","Eval_StdReturn : 9.098104476928711\n","Eval_MaxReturn : -124.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 146.42857142857142\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -149.038467\n","best mean reward -inf\n","running time 147.973884\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -149.03846740722656\n","TimeSinceStart : 147.9738838672638\n","Exploration Critic Loss : 0.023801717907190323\n","Exploitation Critic Q Loss : 0.09411422908306122\n","Exploitation Critic V Loss : 0.000616788340266794\n","Exploration Model Loss : 0.0017161417054012418\n","Actor Loss : 1.397784948348999\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -146.470581\n","best mean reward -inf\n","running time 212.504774\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -146.4705810546875\n","TimeSinceStart : 212.5047743320465\n","Exploration Critic Loss : 0.03274146839976311\n","Exploitation Critic Q Loss : 0.00885716825723648\n","Exploitation Critic V Loss : 0.0008858399232849479\n","Exploration Model Loss : 0.08851434290409088\n","Actor Loss : 1.3190799951553345\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -146.899994\n","best mean reward -inf\n","running time 278.577497\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -146.89999389648438\n","TimeSinceStart : 278.5774972438812\n","Exploration Critic Loss : 0.089579276740551\n","Exploitation Critic Q Loss : 0.6343997716903687\n","Exploitation Critic V Loss : 0.001426346367225051\n","Exploration Model Loss : 0.0010216811206191778\n","Actor Loss : 1.278088927268982\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -147.361710\n","best mean reward -inf\n","running time 344.118695\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -147.36170959472656\n","TimeSinceStart : 344.11869525909424\n","Exploration Critic Loss : 0.1925169676542282\n","Exploitation Critic Q Loss : 0.39085453748703003\n","Exploitation Critic V Loss : 0.0033190588001161814\n","Exploration Model Loss : 0.009561304934322834\n","Actor Loss : 1.256569504737854\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -147.703705\n","best mean reward -inf\n","running time 410.387244\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -147.70370483398438\n","TimeSinceStart : 410.3872437477112\n","Exploration Critic Loss : 0.23439021408557892\n","Exploitation Critic Q Loss : 1.0390077829360962\n","Exploitation Critic V Loss : 0.01050687674432993\n","Exploration Model Loss : 0.008452478796243668\n","Actor Loss : 1.2678418159484863\n","Eval_AverageReturn : -146.42857360839844\n","Eval_StdReturn : 8.748177528381348\n","Eval_MaxReturn : -125.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 146.57142857142858\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -147.766663\n","best mean reward -inf\n","running time 475.601106\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -147.76666259765625\n","TimeSinceStart : 475.6011064052582\n","Exploration Critic Loss : 0.01859491877257824\n","Exploitation Critic Q Loss : 0.018216855823993683\n","Exploitation Critic V Loss : 0.0037909536622464657\n","Exploration Model Loss : 0.00031246914295479655\n","Actor Loss : 1.2319014072418213\n","Eval_AverageReturn : -142.0\n","Eval_StdReturn : 18.661457061767578\n","Eval_MaxReturn : -93.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 142.25\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -148.000000\n","best mean reward -inf\n","running time 542.394393\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -148.0\n","TimeSinceStart : 542.3943934440613\n","Exploration Critic Loss : 0.18478061258792877\n","Exploitation Critic Q Loss : 0.738684356212616\n","Exploitation Critic V Loss : 0.00428539514541626\n","Exploration Model Loss : 0.0003579031035769731\n","Actor Loss : 1.2782540321350098\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -144.539474\n","best mean reward -inf\n","running time 607.286195\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -144.5394744873047\n","TimeSinceStart : 607.286194562912\n","Exploration Critic Loss : 0.23890888690948486\n","Exploitation Critic Q Loss : 1.386129379272461\n","Exploitation Critic V Loss : 0.015712739899754524\n","Exploration Model Loss : 0.00027173463604412973\n","Actor Loss : 1.4113339185714722\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -144.433731\n","best mean reward -inf\n","running time 674.270500\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -144.43373107910156\n","TimeSinceStart : 674.2704997062683\n","Exploration Critic Loss : 0.25285792350769043\n","Exploitation Critic Q Loss : 2.2782340049743652\n","Exploitation Critic V Loss : 0.015110642649233341\n","Exploration Model Loss : 0.0002192374668084085\n","Actor Loss : 1.282047986984253\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -141.175827\n","best mean reward -inf\n","running time 739.652818\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -141.1758270263672\n","TimeSinceStart : 739.652818441391\n","Exploration Critic Loss : 0.29069268703460693\n","Exploitation Critic Q Loss : 2.420788288116455\n","Exploitation Critic V Loss : 0.0048579266294837\n","Exploration Model Loss : 0.0001454474258935079\n","Actor Loss : 1.2790626287460327\n","Eval_AverageReturn : -144.7142791748047\n","Eval_StdReturn : 10.699990272521973\n","Eval_MaxReturn : -119.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 145.0\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -140.555557\n","best mean reward -inf\n","running time 808.082885\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -140.55555725097656\n","TimeSinceStart : 808.0828845500946\n","Exploration Critic Loss : 0.322088360786438\n","Exploitation Critic Q Loss : 0.7501468062400818\n","Exploitation Critic V Loss : 0.03276005759835243\n","Exploration Model Loss : 9.646092803450301e-05\n","Actor Loss : 1.3039175271987915\n","Eval_AverageReturn : -143.5\n","Eval_StdReturn : 17.197383880615234\n","Eval_MaxReturn : -98.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 143.625\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -139.130005\n","best mean reward -139.130005\n","running time 874.501062\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -139.1300048828125\n","Train_BestReturn : -139.1300048828125\n","TimeSinceStart : 874.5010619163513\n","Exploration Critic Loss : 0.314688503742218\n","Exploitation Critic Q Loss : 0.8796737194061279\n","Exploitation Critic V Loss : 0.028479715809226036\n","Exploration Model Loss : 5.8566773077473044e-05\n","Actor Loss : 1.3042972087860107\n","Eval_AverageReturn : -148.2857208251953\n","Eval_StdReturn : 4.199125289916992\n","Eval_MaxReturn : -138.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.42857142857142\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -137.259995\n","best mean reward -137.259995\n","running time 939.367305\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -137.25999450683594\n","Train_BestReturn : -137.25999450683594\n","TimeSinceStart : 939.3673052787781\n","Exploration Critic Loss : 0.5665508508682251\n","Exploitation Critic Q Loss : 0.9274387359619141\n","Exploitation Critic V Loss : 0.03622198849916458\n","Exploration Model Loss : 3.215200194972567e-05\n","Actor Loss : 1.3275127410888672\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -134.309998\n","best mean reward -134.309998\n","running time 1006.844923\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -134.30999755859375\n","Train_BestReturn : -134.30999755859375\n","TimeSinceStart : 1006.8449234962463\n","Exploration Critic Loss : 0.9142111539840698\n","Exploitation Critic Q Loss : 0.08480268716812134\n","Exploitation Critic V Loss : 0.012794539332389832\n","Exploration Model Loss : 1.8494085452402942e-05\n","Actor Loss : 1.2881182432174683\n","Eval_AverageReturn : -146.2857208251953\n","Eval_StdReturn : 9.098104476928711\n","Eval_MaxReturn : -124.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 146.42857142857142\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -130.869995\n","best mean reward -130.869995\n","running time 1071.828758\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -130.8699951171875\n","Train_BestReturn : -130.8699951171875\n","TimeSinceStart : 1071.8287575244904\n","Exploration Critic Loss : 1.0551358461380005\n","Exploitation Critic Q Loss : 1.9849472045898438\n","Exploitation Critic V Loss : 0.020479651167988777\n","Exploration Model Loss : 2.0854678950854577e-05\n","Actor Loss : 1.3413203954696655\n","Eval_AverageReturn : -148.14285278320312\n","Eval_StdReturn : 4.5490522384643555\n","Eval_MaxReturn : -137.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.28571428571428\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -129.270004\n","best mean reward -129.270004\n","running time 1143.211475\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -129.27000427246094\n","Train_BestReturn : -129.27000427246094\n","TimeSinceStart : 1143.2114748954773\n","Exploration Critic Loss : 0.6016142964363098\n","Exploitation Critic Q Loss : 0.7638189792633057\n","Exploitation Critic V Loss : 0.018042990937829018\n","Exploration Model Loss : 0.00012105773930670694\n","Actor Loss : 1.3430044651031494\n","Eval_AverageReturn : -144.7142791748047\n","Eval_StdReturn : 12.94730281829834\n","Eval_MaxReturn : -113.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 144.85714285714286\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -123.500000\n","best mean reward -123.500000\n","running time 1209.028095\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -123.5\n","Train_BestReturn : -123.5\n","TimeSinceStart : 1209.0280947685242\n","Exploration Critic Loss : 1.0600546598434448\n","Exploitation Critic Q Loss : 3.45131778717041\n","Exploitation Critic V Loss : 0.027850450947880745\n","Exploration Model Loss : 0.00016098268679343164\n","Actor Loss : 1.1898562908172607\n","Eval_AverageReturn : -122.11111450195312\n","Eval_StdReturn : 33.104026794433594\n","Eval_MaxReturn : -67.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 122.66666666666667\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -115.470001\n","best mean reward -115.470001\n","running time 1277.247330\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -115.47000122070312\n","Train_BestReturn : -115.47000122070312\n","TimeSinceStart : 1277.247329711914\n","Exploration Critic Loss : 0.998123824596405\n","Exploitation Critic Q Loss : 0.27981311082839966\n","Exploitation Critic V Loss : 0.04337549954652786\n","Exploration Model Loss : 3.687565549626015e-05\n","Actor Loss : 1.2710570096969604\n","Eval_AverageReturn : -136.375\n","Eval_StdReturn : 30.54479217529297\n","Eval_MaxReturn : -56.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 136.75\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -111.629997\n","best mean reward -111.629997\n","running time 1344.232075\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -111.62999725341797\n","Train_BestReturn : -111.62999725341797\n","TimeSinceStart : 1344.2320754528046\n","Exploration Critic Loss : 1.2817702293395996\n","Exploitation Critic Q Loss : 0.280426561832428\n","Exploitation Critic V Loss : 0.028890324756503105\n","Exploration Model Loss : 0.00012718232756014913\n","Actor Loss : 1.4059698581695557\n","Eval_AverageReturn : -139.25\n","Eval_StdReturn : 19.891895294189453\n","Eval_MaxReturn : -93.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 139.5\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -108.010002\n","best mean reward -108.010002\n","running time 1412.427439\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -108.01000213623047\n","Train_BestReturn : -108.01000213623047\n","TimeSinceStart : 1412.4274394512177\n","Exploration Critic Loss : 2.7158162593841553\n","Exploitation Critic Q Loss : 1.4572759866714478\n","Exploitation Critic V Loss : 0.06203187257051468\n","Exploration Model Loss : 0.00010609503806335852\n","Actor Loss : 1.2699997425079346\n","Eval_AverageReturn : -131.375\n","Eval_StdReturn : 30.203218460083008\n","Eval_MaxReturn : -68.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 131.75\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -102.940002\n","best mean reward -102.940002\n","running time 1479.814571\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -102.94000244140625\n","Train_BestReturn : -102.94000244140625\n","TimeSinceStart : 1479.814570903778\n","Exploration Critic Loss : 1.9005146026611328\n","Exploitation Critic Q Loss : 0.2554108202457428\n","Exploitation Critic V Loss : 0.018753236159682274\n","Exploration Model Loss : 3.327647937112488e-05\n","Actor Loss : 1.3304940462112427\n","Eval_AverageReturn : -113.19999694824219\n","Eval_StdReturn : 41.01658248901367\n","Eval_MaxReturn : -46.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 113.7\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -97.139999\n","best mean reward -97.139999\n","running time 1544.683510\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -97.13999938964844\n","Train_BestReturn : -97.13999938964844\n","TimeSinceStart : 1544.6835098266602\n","Exploration Critic Loss : 1.7153372764587402\n","Exploitation Critic Q Loss : 0.18393471837043762\n","Exploitation Critic V Loss : 0.040750809013843536\n","Exploration Model Loss : 0.00029813230503350496\n","Actor Loss : 1.3252227306365967\n","Eval_AverageReturn : -142.7142791748047\n","Eval_StdReturn : 14.508267402648926\n","Eval_MaxReturn : -108.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 143.0\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -90.709999\n","best mean reward -90.709999\n","running time 1612.386193\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -90.70999908447266\n","Train_BestReturn : -90.70999908447266\n","TimeSinceStart : 1612.386193037033\n","Exploration Critic Loss : 2.3870432376861572\n","Exploitation Critic Q Loss : 0.2630636692047119\n","Exploitation Critic V Loss : 0.04056257754564285\n","Exploration Model Loss : 3.615352761698887e-05\n","Actor Loss : 1.33403480052948\n","Eval_AverageReturn : -106.5\n","Eval_StdReturn : 32.392127990722656\n","Eval_MaxReturn : -70.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 107.2\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -82.860001\n","best mean reward -82.860001\n","running time 1679.205281\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -82.86000061035156\n","Train_BestReturn : -82.86000061035156\n","TimeSinceStart : 1679.205281496048\n","Exploration Critic Loss : 1.815331220626831\n","Exploitation Critic Q Loss : 0.22486597299575806\n","Exploitation Critic V Loss : 0.05062055215239525\n","Exploration Model Loss : 6.087023848522222e-06\n","Actor Loss : 1.3474888801574707\n","Eval_AverageReturn : -112.69999694824219\n","Eval_StdReturn : 36.0833740234375\n","Eval_MaxReturn : -55.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 113.4\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -74.940002\n","best mean reward -74.940002\n","running time 1748.822785\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -74.94000244140625\n","Train_BestReturn : -74.94000244140625\n","TimeSinceStart : 1748.8227846622467\n","Exploration Critic Loss : 1.1598964929580688\n","Exploitation Critic Q Loss : 0.8413995504379272\n","Exploitation Critic V Loss : 0.057373203337192535\n","Exploration Model Loss : 0.00038764943019486964\n","Actor Loss : 1.3645484447479248\n","Eval_AverageReturn : -103.9000015258789\n","Eval_StdReturn : 33.134422302246094\n","Eval_MaxReturn : -54.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 104.8\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -68.519997\n","best mean reward -68.519997\n","running time 1815.767702\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -68.5199966430664\n","Train_BestReturn : -68.5199966430664\n","TimeSinceStart : 1815.7677023410797\n","Exploration Critic Loss : 2.246974468231201\n","Exploitation Critic Q Loss : 1.0860488414764404\n","Exploitation Critic V Loss : 0.03412740305066109\n","Exploration Model Loss : 8.258645539171994e-05\n","Actor Loss : 1.2318042516708374\n","Eval_AverageReturn : -104.80000305175781\n","Eval_StdReturn : 35.78211975097656\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 105.5\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -63.220001\n","best mean reward -63.220001\n","running time 1885.916359\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -63.220001220703125\n","Train_BestReturn : -63.220001220703125\n","TimeSinceStart : 1885.9163591861725\n","Exploration Critic Loss : 3.110161304473877\n","Exploitation Critic Q Loss : 0.3348018229007721\n","Exploitation Critic V Loss : 0.06484276056289673\n","Exploration Model Loss : 0.0005654986598528922\n","Actor Loss : 1.470658779144287\n","Eval_AverageReturn : -107.0999984741211\n","Eval_StdReturn : 44.54761505126953\n","Eval_MaxReturn : -43.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 107.8\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -57.639999\n","best mean reward -57.639999\n","running time 1954.905804\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -57.63999938964844\n","Train_BestReturn : -57.63999938964844\n","TimeSinceStart : 1954.9058043956757\n","Exploration Critic Loss : 2.11232852935791\n","Exploitation Critic Q Loss : 2.270874500274658\n","Exploitation Critic V Loss : 0.0364513099193573\n","Exploration Model Loss : 2.634028896864038e-05\n","Actor Loss : 1.3266409635543823\n","Eval_AverageReturn : -127.5\n","Eval_StdReturn : 33.060550689697266\n","Eval_MaxReturn : -60.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 128.0\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -51.450001\n","best mean reward -51.450001\n","running time 2021.766332\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -51.45000076293945\n","Train_BestReturn : -51.45000076293945\n","TimeSinceStart : 2021.7663316726685\n","Exploration Critic Loss : 2.5824742317199707\n","Exploitation Critic Q Loss : 2.5645041465759277\n","Exploitation Critic V Loss : 0.09275412559509277\n","Exploration Model Loss : 3.067059992645227e-07\n","Actor Loss : 1.3463765382766724\n","Eval_AverageReturn : -103.19999694824219\n","Eval_StdReturn : 36.646419525146484\n","Eval_MaxReturn : -48.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 104.0\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -51.549999\n","best mean reward -51.450001\n","running time 2090.346576\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -51.54999923706055\n","Train_BestReturn : -51.45000076293945\n","TimeSinceStart : 2090.3465757369995\n","Exploration Critic Loss : 2.649592876434326\n","Exploitation Critic Q Loss : 1.6250704526901245\n","Exploitation Critic V Loss : 0.06808950752019882\n","Exploration Model Loss : 3.403598384466022e-05\n","Actor Loss : 1.2630951404571533\n","Eval_AverageReturn : -93.09091186523438\n","Eval_StdReturn : 30.10724639892578\n","Eval_MaxReturn : -63.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 93.9090909090909\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -48.279999\n","best mean reward -48.279999\n","running time 2158.763608\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -48.279998779296875\n","Train_BestReturn : -48.279998779296875\n","TimeSinceStart : 2158.763607740402\n","Exploration Critic Loss : 1.8003904819488525\n","Exploitation Critic Q Loss : 0.40501850843429565\n","Exploitation Critic V Loss : 0.0636255219578743\n","Exploration Model Loss : 1.692351325743857e-08\n","Actor Loss : 1.2609974145889282\n","Eval_AverageReturn : -101.0\n","Eval_StdReturn : 36.07819747924805\n","Eval_MaxReturn : -59.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 101.81818181818181\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -44.630001\n","best mean reward -44.630001\n","running time 2228.930606\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -44.630001068115234\n","Train_BestReturn : -44.630001068115234\n","TimeSinceStart : 2228.9306058883667\n","Exploration Critic Loss : 2.43072247505188\n","Exploitation Critic Q Loss : 2.4524784088134766\n","Exploitation Critic V Loss : 0.061466265469789505\n","Exploration Model Loss : 2.029805000347551e-05\n","Actor Loss : 1.2795590162277222\n","Eval_AverageReturn : -84.66666412353516\n","Eval_StdReturn : 34.8911018371582\n","Eval_MaxReturn : -41.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 85.5\n","Buffer size : 35001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -44.419998\n","best mean reward -44.419998\n","running time 2297.729584\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -44.41999816894531\n","Train_BestReturn : -44.41999816894531\n","TimeSinceStart : 2297.729583501816\n","Exploration Critic Loss : 1.3340191841125488\n","Exploitation Critic Q Loss : 0.32333633303642273\n","Exploitation Critic V Loss : 0.05756852403283119\n","Exploration Model Loss : 0.000103928214230109\n","Actor Loss : 1.2171568870544434\n","Eval_AverageReturn : -70.46666717529297\n","Eval_StdReturn : 20.694175720214844\n","Eval_MaxReturn : -33.0\n","Eval_MinReturn : -112.0\n","Eval_AverageEpLen : 71.46666666666667\n","Buffer size : 36001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -43.150002\n","best mean reward -43.150002\n","running time 2369.206289\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -43.150001525878906\n","Train_BestReturn : -43.150001525878906\n","TimeSinceStart : 2369.206289291382\n","Exploration Critic Loss : 1.687172770500183\n","Exploitation Critic Q Loss : 0.279331773519516\n","Exploitation Critic V Loss : 0.08192543685436249\n","Exploration Model Loss : 6.645565008511767e-06\n","Actor Loss : 1.3119443655014038\n","Eval_AverageReturn : -94.54545593261719\n","Eval_StdReturn : 29.638315200805664\n","Eval_MaxReturn : -56.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 95.45454545454545\n","Buffer size : 37001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -42.910000\n","best mean reward -42.910000\n","running time 2438.356068\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -42.90999984741211\n","Train_BestReturn : -42.90999984741211\n","TimeSinceStart : 2438.3560683727264\n","Exploration Critic Loss : 1.043447732925415\n","Exploitation Critic Q Loss : 0.2653547525405884\n","Exploitation Critic V Loss : 0.0701533704996109\n","Exploration Model Loss : 6.3129523368843365e-06\n","Actor Loss : 1.2893844842910767\n","Eval_AverageReturn : -95.54545593261719\n","Eval_StdReturn : 34.82257080078125\n","Eval_MaxReturn : -44.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 96.45454545454545\n","Buffer size : 38001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -42.240002\n","best mean reward -42.240002\n","running time 2506.377027\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -42.2400016784668\n","Train_BestReturn : -42.2400016784668\n","TimeSinceStart : 2506.377027273178\n","Exploration Critic Loss : 0.8624000549316406\n","Exploitation Critic Q Loss : 2.6396138668060303\n","Exploitation Critic V Loss : 0.06715858727693558\n","Exploration Model Loss : 2.654485342645785e-06\n","Actor Loss : 1.290129542350769\n","Eval_AverageReturn : -78.46154022216797\n","Eval_StdReturn : 36.470462799072266\n","Eval_MaxReturn : -44.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 79.38461538461539\n","Buffer size : 39001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -42.040001\n","best mean reward -42.040001\n","running time 2577.672172\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -42.040000915527344\n","Train_BestReturn : -42.040000915527344\n","TimeSinceStart : 2577.672172307968\n","Exploration Critic Loss : 0.9220461249351501\n","Exploitation Critic Q Loss : 0.28251364827156067\n","Exploitation Critic V Loss : 0.07362432032823563\n","Exploration Model Loss : 0.00034794045495800674\n","Actor Loss : 1.2115375995635986\n","Eval_AverageReturn : -84.33333587646484\n","Eval_StdReturn : 32.85405349731445\n","Eval_MaxReturn : -40.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 85.25\n","Buffer size : 40001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -40.750000\n","best mean reward -40.750000\n","running time 2646.053795\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -40.75\n","Train_BestReturn : -40.75\n","TimeSinceStart : 2646.0537950992584\n","Exploration Critic Loss : 0.5117263793945312\n","Exploitation Critic Q Loss : 0.6564847826957703\n","Exploitation Critic V Loss : 0.08383239060640335\n","Exploration Model Loss : 0.00028646402643062174\n","Actor Loss : 1.2778699398040771\n","Eval_AverageReturn : -95.0\n","Eval_StdReturn : 34.30080795288086\n","Eval_MaxReturn : -49.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 95.9090909090909\n","Buffer size : 41001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -41.689999\n","best mean reward -40.750000\n","running time 2717.801257\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -41.689998626708984\n","Train_BestReturn : -40.75\n","TimeSinceStart : 2717.8012568950653\n","Exploration Critic Loss : 0.4497140944004059\n","Exploitation Critic Q Loss : 0.17851538956165314\n","Exploitation Critic V Loss : 0.06708011031150818\n","Exploration Model Loss : 1.5578236798319267e-06\n","Actor Loss : 1.2213213443756104\n","Eval_AverageReturn : -73.71428680419922\n","Eval_StdReturn : 35.97306442260742\n","Eval_MaxReturn : -35.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 74.64285714285714\n","Buffer size : 42001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -42.070000\n","best mean reward -40.750000\n","running time 2786.455483\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -42.06999969482422\n","Train_BestReturn : -40.75\n","TimeSinceStart : 2786.455483198166\n","Exploration Critic Loss : 0.365749716758728\n","Exploitation Critic Q Loss : 1.570288896560669\n","Exploitation Critic V Loss : 0.08420322090387344\n","Exploration Model Loss : 0.0004296525730751455\n","Actor Loss : 1.2385724782943726\n","Eval_AverageReturn : -79.84615325927734\n","Eval_StdReturn : 31.958738327026367\n","Eval_MaxReturn : -36.0\n","Eval_MinReturn : -138.0\n","Eval_AverageEpLen : 80.84615384615384\n","Buffer size : 43001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -41.529999\n","best mean reward -40.750000\n","running time 2858.424867\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -41.529998779296875\n","Train_BestReturn : -40.75\n","TimeSinceStart : 2858.4248671531677\n","Exploration Critic Loss : 0.19295714795589447\n","Exploitation Critic Q Loss : 0.6091084480285645\n","Exploitation Critic V Loss : 0.1087217703461647\n","Exploration Model Loss : 0.0001957480562850833\n","Actor Loss : 1.2640550136566162\n","Eval_AverageReturn : -72.64286041259766\n","Eval_StdReturn : 26.666210174560547\n","Eval_MaxReturn : -34.0\n","Eval_MinReturn : -117.0\n","Eval_AverageEpLen : 73.64285714285714\n","Buffer size : 44001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -40.320000\n","best mean reward -40.320000\n","running time 2930.120291\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -40.31999969482422\n","Train_BestReturn : -40.31999969482422\n","TimeSinceStart : 2930.1202914714813\n","Exploration Critic Loss : 0.06622359156608582\n","Exploitation Critic Q Loss : 2.2885236740112305\n","Exploitation Critic V Loss : 0.07415212690830231\n","Exploration Model Loss : 1.0363366527599283e-05\n","Actor Loss : 1.2848494052886963\n","Eval_AverageReturn : -83.0\n","Eval_StdReturn : 34.529808044433594\n","Eval_MaxReturn : -38.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 83.92307692307692\n","Buffer size : 45001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -39.779999\n","best mean reward -39.779999\n","running time 2999.809365\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -39.779998779296875\n","Train_BestReturn : -39.779998779296875\n","TimeSinceStart : 2999.809365272522\n","Exploration Critic Loss : 0.03639388829469681\n","Exploitation Critic Q Loss : 0.8350346684455872\n","Exploitation Critic V Loss : 0.061048489063978195\n","Exploration Model Loss : 0.00021345877030398697\n","Actor Loss : 1.2700306177139282\n","Eval_AverageReturn : -79.76923370361328\n","Eval_StdReturn : 23.281400680541992\n","Eval_MaxReturn : -32.0\n","Eval_MinReturn : -120.0\n","Eval_AverageEpLen : 80.76923076923077\n","Buffer size : 46001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -38.160000\n","best mean reward -38.160000\n","running time 3071.747614\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -38.15999984741211\n","Train_BestReturn : -38.15999984741211\n","TimeSinceStart : 3071.747614145279\n","Exploration Critic Loss : 0.02996867336332798\n","Exploitation Critic Q Loss : 1.897733211517334\n","Exploitation Critic V Loss : 0.06629883497953415\n","Exploration Model Loss : 2.1841563011548715e-06\n","Actor Loss : 1.285170555114746\n","Eval_AverageReturn : -77.23076629638672\n","Eval_StdReturn : 38.55895233154297\n","Eval_MaxReturn : -32.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 78.15384615384616\n","Buffer size : 47001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -38.070000\n","best mean reward -38.070000\n","running time 3143.311141\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -38.06999969482422\n","Train_BestReturn : -38.06999969482422\n","TimeSinceStart : 3143.311141014099\n","Exploration Critic Loss : 0.04089925438165665\n","Exploitation Critic Q Loss : 0.7947783470153809\n","Exploitation Critic V Loss : 0.0929398238658905\n","Exploration Model Loss : 0.00012244436948094517\n","Actor Loss : 1.1578314304351807\n","Eval_AverageReturn : -85.5\n","Eval_StdReturn : 33.382877349853516\n","Eval_MaxReturn : -43.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 86.41666666666667\n","Buffer size : 48001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -38.000000\n","best mean reward -38.000000\n","running time 3215.646592\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -38.0\n","Train_BestReturn : -38.0\n","TimeSinceStart : 3215.6465916633606\n","Exploration Critic Loss : 0.043473899364471436\n","Exploitation Critic Q Loss : 0.6110449433326721\n","Exploitation Critic V Loss : 0.11675059795379639\n","Exploration Model Loss : 5.317387149261776e-06\n","Actor Loss : 1.2332069873809814\n","Eval_AverageReturn : -76.30769348144531\n","Eval_StdReturn : 22.07115936279297\n","Eval_MaxReturn : -40.0\n","Eval_MinReturn : -120.0\n","Eval_AverageEpLen : 77.3076923076923\n","Buffer size : 49001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_iql.py --env_name PointmassMedium-v0 \\\n"," --exp_name q5_iql_medium_supervised_lam10_tau0.5 --use_rnd \\\n"," --num_exploration_steps=20000 \\\n"," --awac_lambda=10 \\\n"," --iql_expectile=0.5 \\\n"," --seed 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1607420,"status":"ok","timestamp":1669207508374,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"0lqzysB7VNDT","outputId":"63c8ab0b-04df-48b7-eeb4-80c2b4738ea8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_medium_supervised_lam10_tau0.95_PointmassMedium-v0_23-11-2022_11-47-19 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_medium_supervised_lam10_tau0.95_PointmassMedium-v0_23-11-2022_11-47-19\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002028\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.002027750015258789\n","Eval_AverageReturn : -133.375\n","Eval_StdReturn : 32.353275299072266\n","Eval_MaxReturn : -54.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 133.625\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -142.833328\n","best mean reward -inf\n","running time 6.783995\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -142.8333282470703\n","TimeSinceStart : 6.783995151519775\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -146.692307\n","best mean reward -inf\n","running time 14.025183\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -146.6923065185547\n","TimeSinceStart : 14.02518343925476\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -147.850006\n","best mean reward -inf\n","running time 80.731528\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -147.85000610351562\n","TimeSinceStart : 80.73152756690979\n","Exploration Critic Loss : 0.002912016585469246\n","Exploitation Critic Q Loss : 0.001052230829373002\n","Exploitation Critic V Loss : 5.5682219681330025e-05\n","Exploration Model Loss : 0.0008064330322667956\n","Actor Loss : 1.513861060142517\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -148.000000\n","best mean reward -inf\n","running time 147.500276\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -148.0\n","TimeSinceStart : 147.50027632713318\n","Exploration Critic Loss : 0.03938813507556915\n","Exploitation Critic Q Loss : 0.0874100998044014\n","Exploitation Critic V Loss : 0.00022881902987137437\n","Exploration Model Loss : 0.0009626809624023736\n","Actor Loss : 1.5087995529174805\n","Eval_AverageReturn : -145.42857360839844\n","Eval_StdReturn : 11.197668075561523\n","Eval_MaxReturn : -118.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 145.57142857142858\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -146.088242\n","best mean reward -inf\n","running time 215.999420\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -146.08824157714844\n","TimeSinceStart : 215.9994204044342\n","Exploration Critic Loss : 0.10014138370752335\n","Exploitation Critic Q Loss : 0.5030863285064697\n","Exploitation Critic V Loss : 0.00036085525061935186\n","Exploration Model Loss : 0.15856876969337463\n","Actor Loss : 1.4253617525100708\n","Eval_AverageReturn : -147.57142639160156\n","Eval_StdReturn : 5.948760032653809\n","Eval_MaxReturn : -133.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 147.71428571428572\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -130.622223\n","best mean reward -inf\n","running time 281.828701\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -130.62222290039062\n","TimeSinceStart : 281.8287010192871\n","Exploration Critic Loss : 0.11194238066673279\n","Exploitation Critic Q Loss : 0.21327950060367584\n","Exploitation Critic V Loss : 0.0014093427453190088\n","Exploration Model Loss : 0.0006796351517550647\n","Actor Loss : 1.3733246326446533\n","Eval_AverageReturn : -129.25\n","Eval_StdReturn : 32.34095764160156\n","Eval_MaxReturn : -58.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 129.75\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -129.245285\n","best mean reward -inf\n","running time 350.306606\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -129.2452850341797\n","TimeSinceStart : 350.3066055774689\n","Exploration Critic Loss : 0.10367965698242188\n","Exploitation Critic Q Loss : 0.39736321568489075\n","Exploitation Critic V Loss : 0.0011393607128411531\n","Exploration Model Loss : 0.002728666877374053\n","Actor Loss : 1.375198245048523\n","Eval_AverageReturn : -135.375\n","Eval_StdReturn : 30.646930694580078\n","Eval_MaxReturn : -57.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 135.625\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -126.758064\n","best mean reward -inf\n","running time 416.944010\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -126.75806427001953\n","TimeSinceStart : 416.9440097808838\n","Exploration Critic Loss : 0.09147867560386658\n","Exploitation Critic Q Loss : 0.7707332372665405\n","Exploitation Critic V Loss : 0.001796026132069528\n","Exploration Model Loss : 0.0003740255779121071\n","Actor Loss : 1.32720947265625\n","Eval_AverageReturn : -124.77777862548828\n","Eval_StdReturn : 30.487804412841797\n","Eval_MaxReturn : -75.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 125.22222222222223\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -127.000000\n","best mean reward -inf\n","running time 486.158095\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -127.0\n","TimeSinceStart : 486.1580946445465\n","Exploration Critic Loss : 0.1753620207309723\n","Exploitation Critic Q Loss : 0.7111917734146118\n","Exploitation Critic V Loss : 0.004303986206650734\n","Exploration Model Loss : 0.000376613752450794\n","Actor Loss : 1.424425482749939\n","Eval_AverageReturn : -140.625\n","Eval_StdReturn : 24.803918838500977\n","Eval_MaxReturn : -75.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 140.75\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -121.851852\n","best mean reward -inf\n","running time 554.326579\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -121.85185241699219\n","TimeSinceStart : 554.3265793323517\n","Exploration Critic Loss : 0.15804365277290344\n","Exploitation Critic Q Loss : 0.8117955327033997\n","Exploitation Critic V Loss : 0.0031814626418054104\n","Exploration Model Loss : 0.0003435378894209862\n","Actor Loss : 1.2964791059494019\n","Eval_AverageReturn : -140.0\n","Eval_StdReturn : 20.51828384399414\n","Eval_MaxReturn : -88.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 140.25\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -121.188889\n","best mean reward -inf\n","running time 620.916350\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -121.18888854980469\n","TimeSinceStart : 620.9163498878479\n","Exploration Critic Loss : 0.14355480670928955\n","Exploitation Critic Q Loss : 0.7766825556755066\n","Exploitation Critic V Loss : 0.007932786829769611\n","Exploration Model Loss : 0.00022824000916443765\n","Actor Loss : 1.395094394683838\n","Eval_AverageReturn : -128.25\n","Eval_StdReturn : 29.625791549682617\n","Eval_MaxReturn : -70.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 128.75\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -106.930000\n","best mean reward -106.930000\n","running time 689.866925\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -106.93000030517578\n","Train_BestReturn : -106.93000030517578\n","TimeSinceStart : 689.8669254779816\n","Exploration Critic Loss : 0.18227332830429077\n","Exploitation Critic Q Loss : 0.887785017490387\n","Exploitation Critic V Loss : 0.006638712249696255\n","Exploration Model Loss : 0.00015215788153000176\n","Actor Loss : 1.2680366039276123\n","Eval_AverageReturn : -143.85714721679688\n","Eval_StdReturn : 15.046865463256836\n","Eval_MaxReturn : -107.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 144.0\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -91.870003\n","best mean reward -91.870003\n","running time 756.393601\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -91.87000274658203\n","Train_BestReturn : -91.87000274658203\n","TimeSinceStart : 756.3936007022858\n","Exploration Critic Loss : 0.15611058473587036\n","Exploitation Critic Q Loss : 0.16748546063899994\n","Exploitation Critic V Loss : 0.005329628475010395\n","Exploration Model Loss : 0.00014450098387897015\n","Actor Loss : 1.3718721866607666\n","Eval_AverageReturn : -112.0\n","Eval_StdReturn : 34.81060028076172\n","Eval_MaxReturn : -51.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 112.66666666666667\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -81.599998\n","best mean reward -81.599998\n","running time 826.718153\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -81.5999984741211\n","Train_BestReturn : -81.5999984741211\n","TimeSinceStart : 826.7181527614594\n","Exploration Critic Loss : 0.2873980700969696\n","Exploitation Critic Q Loss : 1.7059874534606934\n","Exploitation Critic V Loss : 0.009389901533722878\n","Exploration Model Loss : 5.769846757175401e-05\n","Actor Loss : 1.19779634475708\n","Eval_AverageReturn : -115.11111450195312\n","Eval_StdReturn : 29.064849853515625\n","Eval_MaxReturn : -75.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 115.77777777777777\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -64.349998\n","best mean reward -64.349998\n","running time 895.736804\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -64.3499984741211\n","Train_BestReturn : -64.3499984741211\n","TimeSinceStart : 895.7368035316467\n","Exploration Critic Loss : 0.10698947310447693\n","Exploitation Critic Q Loss : 0.25807657837867737\n","Exploitation Critic V Loss : 0.0075756460428237915\n","Exploration Model Loss : 5.799528298666701e-05\n","Actor Loss : 1.2330565452575684\n","Eval_AverageReturn : -109.5999984741211\n","Eval_StdReturn : 36.76737976074219\n","Eval_MaxReturn : -46.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 110.3\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -49.360001\n","best mean reward -49.360001\n","running time 969.812747\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -49.36000061035156\n","Train_BestReturn : -49.36000061035156\n","TimeSinceStart : 969.8127474784851\n","Exploration Critic Loss : 0.2363521307706833\n","Exploitation Critic Q Loss : 1.6124359369277954\n","Exploitation Critic V Loss : 0.009284072555601597\n","Exploration Model Loss : 8.391986921196803e-05\n","Actor Loss : 1.221850872039795\n","Eval_AverageReturn : -99.5999984741211\n","Eval_StdReturn : 30.80974006652832\n","Eval_MaxReturn : -61.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 100.5\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -45.160000\n","best mean reward -45.160000\n","running time 1042.222585\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -45.15999984741211\n","Train_BestReturn : -45.15999984741211\n","TimeSinceStart : 1042.2225847244263\n","Exploration Critic Loss : 0.2011176496744156\n","Exploitation Critic Q Loss : 1.585653305053711\n","Exploitation Critic V Loss : 0.009116551838815212\n","Exploration Model Loss : 2.086620224872604e-05\n","Actor Loss : 1.1575855016708374\n","Eval_AverageReturn : -85.08333587646484\n","Eval_StdReturn : 26.880781173706055\n","Eval_MaxReturn : -44.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 86.0\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -42.090000\n","best mean reward -42.090000\n","running time 1112.789948\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -42.09000015258789\n","Train_BestReturn : -42.09000015258789\n","TimeSinceStart : 1112.7899475097656\n","Exploration Critic Loss : 0.32312679290771484\n","Exploitation Critic Q Loss : 1.611437201499939\n","Exploitation Critic V Loss : 0.011606856249272823\n","Exploration Model Loss : 3.070994353038259e-05\n","Actor Loss : 1.258671522140503\n","Eval_AverageReturn : -74.78571319580078\n","Eval_StdReturn : 28.633594512939453\n","Eval_MaxReturn : -42.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 75.71428571428571\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -39.180000\n","best mean reward -39.180000\n","running time 1185.519630\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -39.18000030517578\n","Train_BestReturn : -39.18000030517578\n","TimeSinceStart : 1185.519630432129\n","Exploration Critic Loss : 0.1497398018836975\n","Exploitation Critic Q Loss : 0.24028636515140533\n","Exploitation Critic V Loss : 0.011903941631317139\n","Exploration Model Loss : 5.060660623712465e-05\n","Actor Loss : 1.175376534461975\n","Eval_AverageReturn : -77.69230651855469\n","Eval_StdReturn : 31.723285675048828\n","Eval_MaxReturn : -44.0\n","Eval_MinReturn : -148.0\n","Eval_AverageEpLen : 78.6923076923077\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -37.570000\n","best mean reward -37.570000\n","running time 1256.725255\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -37.56999969482422\n","Train_BestReturn : -37.56999969482422\n","TimeSinceStart : 1256.7252550125122\n","Exploration Critic Loss : 0.15605340898036957\n","Exploitation Critic Q Loss : 1.0256210565567017\n","Exploitation Critic V Loss : 0.013540823012590408\n","Exploration Model Loss : 6.172116991365328e-05\n","Actor Loss : 1.178812861442566\n","Eval_AverageReturn : -76.30769348144531\n","Eval_StdReturn : 20.544078826904297\n","Eval_MaxReturn : -55.0\n","Eval_MinReturn : -132.0\n","Eval_AverageEpLen : 77.3076923076923\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -36.259998\n","best mean reward -36.259998\n","running time 1329.684151\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -36.2599983215332\n","Train_BestReturn : -36.2599983215332\n","TimeSinceStart : 1329.6841509342194\n","Exploration Critic Loss : 0.11993400007486343\n","Exploitation Critic Q Loss : 0.24031184613704681\n","Exploitation Critic V Loss : 0.011303558945655823\n","Exploration Model Loss : 0.000254118291195482\n","Actor Loss : 1.214228868484497\n","Eval_AverageReturn : -67.19999694824219\n","Eval_StdReturn : 29.52558135986328\n","Eval_MaxReturn : -19.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 68.13333333333334\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -34.860001\n","best mean reward -34.860001\n","running time 1402.060746\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -34.86000061035156\n","Train_BestReturn : -34.86000061035156\n","TimeSinceStart : 1402.0607464313507\n","Exploration Critic Loss : 0.2358269989490509\n","Exploitation Critic Q Loss : 1.7689841985702515\n","Exploitation Critic V Loss : 0.013914383947849274\n","Exploration Model Loss : 0.00023311996483244002\n","Actor Loss : 1.2957406044006348\n","Eval_AverageReturn : -79.23076629638672\n","Eval_StdReturn : 39.31364059448242\n","Eval_MaxReturn : -46.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 80.07692307692308\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -33.490002\n","best mean reward -33.490002\n","running time 1473.337435\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -33.4900016784668\n","Train_BestReturn : -33.4900016784668\n","TimeSinceStart : 1473.3374347686768\n","Exploration Critic Loss : 0.11055190116167068\n","Exploitation Critic Q Loss : 0.23939791321754456\n","Exploitation Critic V Loss : 0.00896565429866314\n","Exploration Model Loss : 2.261144072690513e-05\n","Actor Loss : 1.1733112335205078\n","Eval_AverageReturn : -58.52941131591797\n","Eval_StdReturn : 28.920005798339844\n","Eval_MaxReturn : -21.0\n","Eval_MinReturn : -127.0\n","Eval_AverageEpLen : 59.529411764705884\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -33.029999\n","best mean reward -33.029999\n","running time 1546.080261\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -33.029998779296875\n","Train_BestReturn : -33.029998779296875\n","TimeSinceStart : 1546.080260515213\n","Exploration Critic Loss : 0.10842625051736832\n","Exploitation Critic Q Loss : 0.2714742422103882\n","Exploitation Critic V Loss : 0.010753480717539787\n","Exploration Model Loss : 0.00015888245252426714\n","Actor Loss : 1.1282317638397217\n","Eval_AverageReturn : -76.0\n","Eval_StdReturn : 25.395336151123047\n","Eval_MaxReturn : -51.0\n","Eval_MinReturn : -141.0\n","Eval_AverageEpLen : 77.0\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -35.299999\n","best mean reward -33.029999\n","running time 1616.635557\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -35.29999923706055\n","Train_BestReturn : -33.029998779296875\n","TimeSinceStart : 1616.6355566978455\n","Exploration Critic Loss : 0.11539221554994583\n","Exploitation Critic Q Loss : 0.28982189297676086\n","Exploitation Critic V Loss : 0.008038998581469059\n","Exploration Model Loss : 0.00019825255731120706\n","Actor Loss : 1.1225438117980957\n","Eval_AverageReturn : -80.92308044433594\n","Eval_StdReturn : 32.478302001953125\n","Eval_MaxReturn : -40.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 81.84615384615384\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -34.060001\n","best mean reward -33.029999\n","running time 1690.393301\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -34.060001373291016\n","Train_BestReturn : -33.029998779296875\n","TimeSinceStart : 1690.3933005332947\n","Exploration Critic Loss : 0.14100998640060425\n","Exploitation Critic Q Loss : 0.829323947429657\n","Exploitation Critic V Loss : 0.009212073870003223\n","Exploration Model Loss : 6.430481153074652e-05\n","Actor Loss : 1.0508441925048828\n","Eval_AverageReturn : -77.46154022216797\n","Eval_StdReturn : 34.13460922241211\n","Eval_MaxReturn : -41.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 78.38461538461539\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -35.320000\n","best mean reward -33.029999\n","running time 1761.674720\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -35.31999969482422\n","Train_BestReturn : -33.029998779296875\n","TimeSinceStart : 1761.6747195720673\n","Exploration Critic Loss : 0.10279862582683563\n","Exploitation Critic Q Loss : 0.2513560950756073\n","Exploitation Critic V Loss : 0.007596252486109734\n","Exploration Model Loss : 5.764497836935334e-05\n","Actor Loss : 1.0132172107696533\n","Eval_AverageReturn : -61.9375\n","Eval_StdReturn : 29.735225677490234\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -134.0\n","Eval_AverageEpLen : 62.9375\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -34.500000\n","best mean reward -33.029999\n","running time 1832.717119\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -34.5\n","Train_BestReturn : -33.029998779296875\n","TimeSinceStart : 1832.7171194553375\n","Exploration Critic Loss : 0.16101816296577454\n","Exploitation Critic Q Loss : 0.9281076192855835\n","Exploitation Critic V Loss : 0.010427210479974747\n","Exploration Model Loss : 0.00032309757079929113\n","Actor Loss : 1.1138397455215454\n","Eval_AverageReturn : -49.19047546386719\n","Eval_StdReturn : 19.065349578857422\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -87.0\n","Eval_AverageEpLen : 50.19047619047619\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -33.930000\n","best mean reward -33.029999\n","running time 1906.292470\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -33.93000030517578\n","Train_BestReturn : -33.029998779296875\n","TimeSinceStart : 1906.2924699783325\n","Exploration Critic Loss : 0.09129974246025085\n","Exploitation Critic Q Loss : 0.23666320741176605\n","Exploitation Critic V Loss : 0.007456421852111816\n","Exploration Model Loss : 2.017184442593134e-06\n","Actor Loss : 1.14118492603302\n","Eval_AverageReturn : -50.75\n","Eval_StdReturn : 18.862329483032227\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -89.0\n","Eval_AverageEpLen : 51.75\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -34.660000\n","best mean reward -33.029999\n","running time 1977.492692\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -34.65999984741211\n","Train_BestReturn : -33.029998779296875\n","TimeSinceStart : 1977.4926915168762\n","Exploration Critic Loss : 0.094815693795681\n","Exploitation Critic Q Loss : 0.27596616744995117\n","Exploitation Critic V Loss : 0.00808524340391159\n","Exploration Model Loss : 0.00023290683748200536\n","Actor Loss : 1.0684665441513062\n","Eval_AverageReturn : -45.3636360168457\n","Eval_StdReturn : 15.295706748962402\n","Eval_MaxReturn : -20.0\n","Eval_MinReturn : -80.0\n","Eval_AverageEpLen : 46.36363636363637\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -34.180000\n","best mean reward -33.029999\n","running time 2052.669019\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -34.18000030517578\n","Train_BestReturn : -33.029998779296875\n","TimeSinceStart : 2052.669018507004\n","Exploration Critic Loss : 0.1632489562034607\n","Exploitation Critic Q Loss : 0.8925999999046326\n","Exploitation Critic V Loss : 0.00832325778901577\n","Exploration Model Loss : 7.823118721717037e-06\n","Actor Loss : 1.025253176689148\n","Eval_AverageReturn : -52.47618865966797\n","Eval_StdReturn : 24.392854690551758\n","Eval_MaxReturn : -25.0\n","Eval_MinReturn : -140.0\n","Eval_AverageEpLen : 53.476190476190474\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -32.700001\n","best mean reward -32.700001\n","running time 2127.383105\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -32.70000076293945\n","Train_BestReturn : -32.70000076293945\n","TimeSinceStart : 2127.383105278015\n","Exploration Critic Loss : 0.17114955186843872\n","Exploitation Critic Q Loss : 1.3373968601226807\n","Exploitation Critic V Loss : 0.009889641776680946\n","Exploration Model Loss : 5.023937319492688e-07\n","Actor Loss : 1.1358070373535156\n","Eval_AverageReturn : -56.33333206176758\n","Eval_StdReturn : 22.181072235107422\n","Eval_MaxReturn : -19.0\n","Eval_MinReturn : -103.0\n","Eval_AverageEpLen : 57.333333333333336\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -32.080002\n","best mean reward -32.080002\n","running time 2199.726039\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -32.08000183105469\n","Train_BestReturn : -32.08000183105469\n","TimeSinceStart : 2199.7260394096375\n","Exploration Critic Loss : 0.07680939882993698\n","Exploitation Critic Q Loss : 0.23525889217853546\n","Exploitation Critic V Loss : 0.010247262194752693\n","Exploration Model Loss : 1.9547163901734166e-05\n","Actor Loss : 1.1253598928451538\n","Eval_AverageReturn : -52.47368240356445\n","Eval_StdReturn : 12.94266128540039\n","Eval_MaxReturn : -32.0\n","Eval_MinReturn : -73.0\n","Eval_AverageEpLen : 53.473684210526315\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -31.260000\n","best mean reward -31.260000\n","running time 2273.751576\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -31.260000228881836\n","Train_BestReturn : -31.260000228881836\n","TimeSinceStart : 2273.751576423645\n","Exploration Critic Loss : 0.0906565934419632\n","Exploitation Critic Q Loss : 0.2797238230705261\n","Exploitation Critic V Loss : 0.007040159776806831\n","Exploration Model Loss : 6.089469025027938e-05\n","Actor Loss : 1.025667667388916\n","Eval_AverageReturn : -50.95000076293945\n","Eval_StdReturn : 20.927194595336914\n","Eval_MaxReturn : -21.0\n","Eval_MinReturn : -96.0\n","Eval_AverageEpLen : 51.95\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -30.420000\n","best mean reward -30.420000\n","running time 2346.520222\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -30.420000076293945\n","Train_BestReturn : -30.420000076293945\n","TimeSinceStart : 2346.5202221870422\n","Exploration Critic Loss : 0.08653587102890015\n","Exploitation Critic Q Loss : 0.24743790924549103\n","Exploitation Critic V Loss : 0.009917019866406918\n","Exploration Model Loss : 0.00017876308993436396\n","Actor Loss : 1.083048701286316\n","Eval_AverageReturn : -45.772727966308594\n","Eval_StdReturn : 14.948188781738281\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -81.0\n","Eval_AverageEpLen : 46.77272727272727\n","Buffer size : 35001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -28.799999\n","best mean reward -28.799999\n","running time 2422.248569\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -28.799999237060547\n","Train_BestReturn : -28.799999237060547\n","TimeSinceStart : 2422.2485694885254\n","Exploration Critic Loss : 0.09998363256454468\n","Exploitation Critic Q Loss : 0.2581545114517212\n","Exploitation Critic V Loss : 0.00997050479054451\n","Exploration Model Loss : 1.6740588762331754e-05\n","Actor Loss : 1.0354887247085571\n","Eval_AverageReturn : -51.20000076293945\n","Eval_StdReturn : 14.517576217651367\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -70.0\n","Eval_AverageEpLen : 52.2\n","Buffer size : 36001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -28.760000\n","best mean reward -28.760000\n","running time 2498.729463\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -28.760000228881836\n","Train_BestReturn : -28.760000228881836\n","TimeSinceStart : 2498.729462623596\n","Exploration Critic Loss : 0.13939669728279114\n","Exploitation Critic Q Loss : 0.5951387286186218\n","Exploitation Critic V Loss : 0.011170679703354836\n","Exploration Model Loss : 9.329487227205391e-08\n","Actor Loss : 1.2294881343841553\n","Eval_AverageReturn : -59.882354736328125\n","Eval_StdReturn : 24.82854652404785\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -120.0\n","Eval_AverageEpLen : 60.88235294117647\n","Buffer size : 37001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -27.559999\n","best mean reward -27.559999\n","running time 2572.515093\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -27.559999465942383\n","Train_BestReturn : -27.559999465942383\n","TimeSinceStart : 2572.5150933265686\n","Exploration Critic Loss : 0.08802753686904907\n","Exploitation Critic Q Loss : 0.24018600583076477\n","Exploitation Critic V Loss : 0.008585013449192047\n","Exploration Model Loss : 1.0603362454730814e-07\n","Actor Loss : 1.0266892910003662\n","Eval_AverageReturn : -49.095237731933594\n","Eval_StdReturn : 21.82158088684082\n","Eval_MaxReturn : -26.0\n","Eval_MinReturn : -128.0\n","Eval_AverageEpLen : 50.095238095238095\n","Buffer size : 38001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -27.580000\n","best mean reward -27.559999\n","running time 2648.331937\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -27.579999923706055\n","Train_BestReturn : -27.559999465942383\n","TimeSinceStart : 2648.331936597824\n","Exploration Critic Loss : 0.08431056886911392\n","Exploitation Critic Q Loss : 0.2234504073858261\n","Exploitation Critic V Loss : 0.009580869227647781\n","Exploration Model Loss : 2.2035527535990695e-07\n","Actor Loss : 1.0317957401275635\n","Eval_AverageReturn : -49.650001525878906\n","Eval_StdReturn : 14.602311134338379\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -78.0\n","Eval_AverageEpLen : 50.65\n","Buffer size : 39001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -26.910000\n","best mean reward -26.910000\n","running time 2722.216191\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -26.90999984741211\n","Train_BestReturn : -26.90999984741211\n","TimeSinceStart : 2722.216191291809\n","Exploration Critic Loss : 0.06114521995186806\n","Exploitation Critic Q Loss : 0.179869145154953\n","Exploitation Critic V Loss : 0.010659917257726192\n","Exploration Model Loss : 2.678894452401437e-05\n","Actor Loss : 1.0952872037887573\n","Eval_AverageReturn : -45.8636360168457\n","Eval_StdReturn : 14.811836242675781\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -81.0\n","Eval_AverageEpLen : 46.86363636363637\n","Buffer size : 40001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -27.570000\n","best mean reward -26.910000\n","running time 2794.926466\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -27.56999969482422\n","Train_BestReturn : -26.90999984741211\n","TimeSinceStart : 2794.9264664649963\n","Exploration Critic Loss : 0.10990241169929504\n","Exploitation Critic Q Loss : 0.5180935859680176\n","Exploitation Critic V Loss : 0.009083637967705727\n","Exploration Model Loss : 4.863934009335935e-06\n","Actor Loss : 1.0186655521392822\n","Eval_AverageReturn : -44.8636360168457\n","Eval_StdReturn : 14.673090934753418\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -75.0\n","Eval_AverageEpLen : 45.86363636363637\n","Buffer size : 41001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -27.990000\n","best mean reward -26.910000\n","running time 2870.272346\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -27.989999771118164\n","Train_BestReturn : -26.90999984741211\n","TimeSinceStart : 2870.2723462581635\n","Exploration Critic Loss : 0.0818963497877121\n","Exploitation Critic Q Loss : 0.1988655924797058\n","Exploitation Critic V Loss : 0.00735766626894474\n","Exploration Model Loss : 0.00010467756510479376\n","Actor Loss : 1.0173534154891968\n","Eval_AverageReturn : -44.565216064453125\n","Eval_StdReturn : 13.940326690673828\n","Eval_MaxReturn : -21.0\n","Eval_MinReturn : -77.0\n","Eval_AverageEpLen : 45.56521739130435\n","Buffer size : 42001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -28.719999\n","best mean reward -26.910000\n","running time 2943.198936\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -28.719999313354492\n","Train_BestReturn : -26.90999984741211\n","TimeSinceStart : 2943.198935508728\n","Exploration Critic Loss : 0.12875592708587646\n","Exploitation Critic Q Loss : 0.7185356020927429\n","Exploitation Critic V Loss : 0.008672214113175869\n","Exploration Model Loss : 0.00013994259643368423\n","Actor Loss : 1.092137336730957\n","Eval_AverageReturn : -41.25\n","Eval_StdReturn : 6.977643013000488\n","Eval_MaxReturn : -26.0\n","Eval_MinReturn : -55.0\n","Eval_AverageEpLen : 42.25\n","Buffer size : 43001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -27.910000\n","best mean reward -26.910000\n","running time 3019.737486\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -27.90999984741211\n","Train_BestReturn : -26.90999984741211\n","TimeSinceStart : 3019.7374863624573\n","Exploration Critic Loss : 0.09913362562656403\n","Exploitation Critic Q Loss : 0.2585192024707794\n","Exploitation Critic V Loss : 0.007770913187414408\n","Exploration Model Loss : 4.990738307242282e-05\n","Actor Loss : 0.9596210718154907\n","Eval_AverageReturn : -42.65217208862305\n","Eval_StdReturn : 11.559331893920898\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -70.0\n","Eval_AverageEpLen : 43.65217391304348\n","Buffer size : 44001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -27.209999\n","best mean reward -26.910000\n","running time 3094.769785\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -27.209999084472656\n","Train_BestReturn : -26.90999984741211\n","TimeSinceStart : 3094.7697854042053\n","Exploration Critic Loss : 0.1160009354352951\n","Exploitation Critic Q Loss : 0.2605381906032562\n","Exploitation Critic V Loss : 0.008454851806163788\n","Exploration Model Loss : 2.5076951715163887e-05\n","Actor Loss : 1.0277783870697021\n","Eval_AverageReturn : -39.92307662963867\n","Eval_StdReturn : 10.943642616271973\n","Eval_MaxReturn : -23.0\n","Eval_MinReturn : -65.0\n","Eval_AverageEpLen : 40.92307692307692\n","Buffer size : 45001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -26.990000\n","best mean reward -26.910000\n","running time 3169.073926\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -26.989999771118164\n","Train_BestReturn : -26.90999984741211\n","TimeSinceStart : 3169.073925971985\n","Exploration Critic Loss : 0.07271333783864975\n","Exploitation Critic Q Loss : 0.1946851760149002\n","Exploitation Critic V Loss : 0.008038568310439587\n","Exploration Model Loss : 5.101664555695606e-06\n","Actor Loss : 1.0496491193771362\n","Eval_AverageReturn : -39.68000030517578\n","Eval_StdReturn : 10.062684059143066\n","Eval_MaxReturn : -20.0\n","Eval_MinReturn : -62.0\n","Eval_AverageEpLen : 40.68\n","Buffer size : 46001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -26.850000\n","best mean reward -26.850000\n","running time 3245.049451\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -26.850000381469727\n","Train_BestReturn : -26.850000381469727\n","TimeSinceStart : 3245.04945063591\n","Exploration Critic Loss : 0.0811992809176445\n","Exploitation Critic Q Loss : 0.22935792803764343\n","Exploitation Critic V Loss : 0.008604638278484344\n","Exploration Model Loss : 0.0003453322860877961\n","Actor Loss : 1.0393027067184448\n","Eval_AverageReturn : -43.21739196777344\n","Eval_StdReturn : 15.789255142211914\n","Eval_MaxReturn : -22.0\n","Eval_MinReturn : -86.0\n","Eval_AverageEpLen : 44.21739130434783\n","Buffer size : 47001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -26.809999\n","best mean reward -26.809999\n","running time 3319.788600\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -26.809999465942383\n","Train_BestReturn : -26.809999465942383\n","TimeSinceStart : 3319.7886004447937\n","Exploration Critic Loss : 0.16311343014240265\n","Exploitation Critic Q Loss : 1.1767300367355347\n","Exploitation Critic V Loss : 0.008151513524353504\n","Exploration Model Loss : 7.44919670978561e-05\n","Actor Loss : 1.1093480587005615\n","Eval_AverageReturn : -42.739131927490234\n","Eval_StdReturn : 11.913518905639648\n","Eval_MaxReturn : -21.0\n","Eval_MinReturn : -68.0\n","Eval_AverageEpLen : 43.73913043478261\n","Buffer size : 48001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -26.610001\n","best mean reward -26.610001\n","running time 3394.157735\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -26.610000610351562\n","Train_BestReturn : -26.610000610351562\n","TimeSinceStart : 3394.157734632492\n","Exploration Critic Loss : 0.09142182022333145\n","Exploitation Critic Q Loss : 0.23481065034866333\n","Exploitation Critic V Loss : 0.00948681402951479\n","Exploration Model Loss : 1.3834198398399167e-05\n","Actor Loss : 1.0644524097442627\n","Eval_AverageReturn : -49.04999923706055\n","Eval_StdReturn : 11.888965606689453\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -79.0\n","Eval_AverageEpLen : 50.05\n","Buffer size : 49001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_iql.py --env_name PointmassMedium-v0 \\\n"," --exp_name q5_iql_medium_supervised_lam10_tau0.95 --use_rnd \\\n"," --num_exploration_steps=20000 \\\n"," --awac_lambda=10 \\\n"," --iql_expectile=0.95 \\\n"," --seed 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3278182,"status":"ok","timestamp":1669204038023,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"oY4lPQ1iHWvY","outputId":"90ec8775-5872-4fb5-969a-d8cf313a6659"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_medium_supervised_lam10_tau0.1_PointmassMedium-v0_23-11-2022_10-52-41 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_medium_supervised_lam10_tau0.1_PointmassMedium-v0_23-11-2022_10-52-41\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002151\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0021514892578125\n","Eval_AverageReturn : -133.375\n","Eval_StdReturn : 32.353275299072266\n","Eval_MaxReturn : -54.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 133.625\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 6.789409\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 6.789409399032593\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 13.874829\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 13.874829292297363\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 81.918855\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 81.91885495185852\n","Exploration Critic Loss : 0.062114302068948746\n","Exploitation Critic Q Loss : 0.06881146132946014\n","Exploitation Critic V Loss : 0.00012216140748932958\n","Exploration Model Loss : 0.029426220804452896\n","Actor Loss : 1.576534390449524\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 161.389482\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 161.38948225975037\n","Exploration Critic Loss : 0.14266109466552734\n","Exploitation Critic Q Loss : 0.31861117482185364\n","Exploitation Critic V Loss : 0.00020295522699598223\n","Exploration Model Loss : 0.0019684755243360996\n","Actor Loss : 1.5796008110046387\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 228.444242\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 228.444242477417\n","Exploration Critic Loss : 0.2987457811832428\n","Exploitation Critic Q Loss : 0.5806710720062256\n","Exploitation Critic V Loss : 0.00045535736717283726\n","Exploration Model Loss : 0.0010891688289120793\n","Actor Loss : 1.580136775970459\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 294.288364\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 294.2883641719818\n","Exploration Critic Loss : 0.35487034916877747\n","Exploitation Critic Q Loss : 0.010878289118409157\n","Exploitation Critic V Loss : 0.0006252884631976485\n","Exploration Model Loss : 0.0011604840401560068\n","Actor Loss : 1.5579674243927002\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 360.427873\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 360.42787289619446\n","Exploration Critic Loss : 0.6595577001571655\n","Exploitation Critic Q Loss : 0.45192039012908936\n","Exploitation Critic V Loss : 0.001259601442143321\n","Exploration Model Loss : 0.0009554352145642042\n","Actor Loss : 1.5552443265914917\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 425.473449\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 425.4734492301941\n","Exploration Critic Loss : 0.6476245522499084\n","Exploitation Critic Q Loss : 1.0562564134597778\n","Exploitation Critic V Loss : 0.00268068490549922\n","Exploration Model Loss : 0.0008096704259514809\n","Actor Loss : 1.5306673049926758\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 494.297780\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 494.2977797985077\n","Exploration Critic Loss : 0.608967661857605\n","Exploitation Critic Q Loss : 0.6830105185508728\n","Exploitation Critic V Loss : 0.001418069121427834\n","Exploration Model Loss : 0.000541555171366781\n","Actor Loss : 1.49627685546875\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 561.470000\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 561.4700002670288\n","Exploration Critic Loss : 0.43682029843330383\n","Exploitation Critic Q Loss : 1.4905929565429688\n","Exploitation Critic V Loss : 0.002987263724207878\n","Exploration Model Loss : 0.00039049334009177983\n","Actor Loss : 1.4735395908355713\n","Eval_AverageReturn : -149.0\n","Eval_StdReturn : 2.4494898319244385\n","Eval_MaxReturn : -143.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 149.14285714285714\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 630.637849\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 630.6378486156464\n","Exploration Critic Loss : 0.23266951739788055\n","Exploitation Critic Q Loss : 1.65518319606781\n","Exploitation Critic V Loss : 0.0009861878352239728\n","Exploration Model Loss : 0.00023306740331463516\n","Actor Loss : 1.431338906288147\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 697.006683\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 697.0066831111908\n","Exploration Critic Loss : 0.1465805619955063\n","Exploitation Critic Q Loss : 0.9464753270149231\n","Exploitation Critic V Loss : 0.002334273885935545\n","Exploration Model Loss : 0.0002337977202842012\n","Actor Loss : 1.422332525253296\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 764.749892\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 764.7498917579651\n","Exploration Critic Loss : 0.15024961531162262\n","Exploitation Critic Q Loss : 1.8864821195602417\n","Exploitation Critic V Loss : 0.0020297083538025618\n","Exploration Model Loss : 0.00031747412867844105\n","Actor Loss : 1.377964973449707\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 831.809532\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 831.8095319271088\n","Exploration Critic Loss : 0.13502237200737\n","Exploitation Critic Q Loss : 0.9855027794837952\n","Exploitation Critic V Loss : 0.002207847312092781\n","Exploration Model Loss : 0.0011253963457420468\n","Actor Loss : 1.4516688585281372\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 903.522877\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 903.5228772163391\n","Exploration Critic Loss : 0.08504975587129593\n","Exploitation Critic Q Loss : 3.1959962844848633\n","Exploitation Critic V Loss : 0.006031418219208717\n","Exploration Model Loss : 0.00021796405781060457\n","Actor Loss : 1.4190974235534668\n","Eval_AverageReturn : -146.42857360839844\n","Eval_StdReturn : 8.748177528381348\n","Eval_MaxReturn : -125.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 146.57142857142858\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -149.770004\n","best mean reward -149.770004\n","running time 971.031625\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -149.77000427246094\n","Train_BestReturn : -149.77000427246094\n","TimeSinceStart : 971.0316245555878\n","Exploration Critic Loss : 0.07070266455411911\n","Exploitation Critic Q Loss : 4.460536479949951\n","Exploitation Critic V Loss : 0.0024195995647460222\n","Exploration Model Loss : 6.503234908450395e-05\n","Actor Loss : 1.4430201053619385\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -148.960007\n","best mean reward -148.960007\n","running time 1037.708442\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -148.9600067138672\n","Train_BestReturn : -148.9600067138672\n","TimeSinceStart : 1037.708441734314\n","Exploration Critic Loss : 0.06283831596374512\n","Exploitation Critic Q Loss : 4.359580993652344\n","Exploitation Critic V Loss : 0.0035126912407577038\n","Exploration Model Loss : 2.220074020442553e-05\n","Actor Loss : 1.4590920209884644\n","Eval_AverageReturn : -141.25\n","Eval_StdReturn : 23.15032386779785\n","Eval_MaxReturn : -80.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 141.375\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -148.929993\n","best mean reward -148.929993\n","running time 1108.134564\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -148.92999267578125\n","Train_BestReturn : -148.92999267578125\n","TimeSinceStart : 1108.1345641613007\n","Exploration Critic Loss : 0.027778947725892067\n","Exploitation Critic Q Loss : 0.90150386095047\n","Exploitation Critic V Loss : 0.0024809835013002157\n","Exploration Model Loss : 2.0788467736565508e-05\n","Actor Loss : 1.3935707807540894\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -148.929993\n","best mean reward -148.929993\n","running time 1174.678454\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -148.92999267578125\n","Train_BestReturn : -148.92999267578125\n","TimeSinceStart : 1174.678454399109\n","Exploration Critic Loss : 0.13432663679122925\n","Exploitation Critic Q Loss : 4.728353023529053\n","Exploitation Critic V Loss : 0.006307528354227543\n","Exploration Model Loss : 0.00012049159704474732\n","Actor Loss : 1.3663283586502075\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -148.839996\n","best mean reward -148.839996\n","running time 1242.010723\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -148.83999633789062\n","Train_BestReturn : -148.83999633789062\n","TimeSinceStart : 1242.0107233524323\n","Exploration Critic Loss : 0.05373546853661537\n","Exploitation Critic Q Loss : 1.2051727771759033\n","Exploitation Critic V Loss : 0.0026172325015068054\n","Exploration Model Loss : 4.2582996684359387e-05\n","Actor Loss : 1.3697872161865234\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -146.649994\n","best mean reward -146.649994\n","running time 1309.552878\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -146.64999389648438\n","Train_BestReturn : -146.64999389648438\n","TimeSinceStart : 1309.5528781414032\n","Exploration Critic Loss : 0.09948708117008209\n","Exploitation Critic Q Loss : 2.434394598007202\n","Exploitation Critic V Loss : 0.003438136773183942\n","Exploration Model Loss : 5.162639354239218e-05\n","Actor Loss : 1.3817073106765747\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -146.270004\n","best mean reward -146.270004\n","running time 1376.859355\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -146.27000427246094\n","Train_BestReturn : -146.27000427246094\n","TimeSinceStart : 1376.859355211258\n","Exploration Critic Loss : 0.17807108163833618\n","Exploitation Critic Q Loss : 6.248114585876465\n","Exploitation Critic V Loss : 0.0040743970312178135\n","Exploration Model Loss : 2.7571222744882107e-05\n","Actor Loss : 1.383837103843689\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -145.500000\n","best mean reward -145.500000\n","running time 1445.030874\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -145.5\n","Train_BestReturn : -145.5\n","TimeSinceStart : 1445.030874490738\n","Exploration Critic Loss : 0.03654119372367859\n","Exploitation Critic Q Loss : 0.09761689603328705\n","Exploitation Critic V Loss : 0.007487393915653229\n","Exploration Model Loss : 6.663720341748558e-06\n","Actor Loss : 1.388550043106079\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -145.229996\n","best mean reward -145.229996\n","running time 1511.568222\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -145.22999572753906\n","Train_BestReturn : -145.22999572753906\n","TimeSinceStart : 1511.5682220458984\n","Exploration Critic Loss : 0.08478118479251862\n","Exploitation Critic Q Loss : 2.6015286445617676\n","Exploitation Critic V Loss : 0.018247999250888824\n","Exploration Model Loss : 6.883117748657241e-05\n","Actor Loss : 1.3905237913131714\n","Eval_AverageReturn : -146.14285278320312\n","Eval_StdReturn : 9.448031425476074\n","Eval_MaxReturn : -123.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 146.28571428571428\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -142.619995\n","best mean reward -142.619995\n","running time 1581.182728\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -142.6199951171875\n","Train_BestReturn : -142.6199951171875\n","TimeSinceStart : 1581.1827278137207\n","Exploration Critic Loss : 0.0766214057803154\n","Exploitation Critic Q Loss : 2.4452638626098633\n","Exploitation Critic V Loss : 0.013517927378416061\n","Exploration Model Loss : 2.380160913162399e-05\n","Actor Loss : 1.3530633449554443\n","Eval_AverageReturn : -137.5\n","Eval_StdReturn : 18.350749969482422\n","Eval_MaxReturn : -102.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 137.875\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -142.570007\n","best mean reward -142.570007\n","running time 1647.356532\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -142.57000732421875\n","Train_BestReturn : -142.57000732421875\n","TimeSinceStart : 1647.3565316200256\n","Exploration Critic Loss : 0.020342346280813217\n","Exploitation Critic Q Loss : 0.07689773291349411\n","Exploitation Critic V Loss : 0.009844827465713024\n","Exploration Model Loss : 0.00029238639399409294\n","Actor Loss : 1.3873448371887207\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -142.570007\n","best mean reward -142.570007\n","running time 1717.267093\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -142.57000732421875\n","Train_BestReturn : -142.57000732421875\n","TimeSinceStart : 1717.2670934200287\n","Exploration Critic Loss : 0.12841661274433136\n","Exploitation Critic Q Loss : 3.6222434043884277\n","Exploitation Critic V Loss : 0.008496757596731186\n","Exploration Model Loss : 6.667675188509747e-05\n","Actor Loss : 1.3502633571624756\n","Eval_AverageReturn : -146.0\n","Eval_StdReturn : 9.797959327697754\n","Eval_MaxReturn : -122.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 146.14285714285714\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -141.660004\n","best mean reward -141.660004\n","running time 1783.946015\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -141.66000366210938\n","Train_BestReturn : -141.66000366210938\n","TimeSinceStart : 1783.9460146427155\n","Exploration Critic Loss : 0.14056865870952606\n","Exploitation Critic Q Loss : 3.9538302421569824\n","Exploitation Critic V Loss : 0.014019235037267208\n","Exploration Model Loss : 6.8967474362580106e-06\n","Actor Loss : 1.4003283977508545\n","Eval_AverageReturn : -148.85714721679688\n","Eval_StdReturn : 2.799417018890381\n","Eval_MaxReturn : -142.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 149.0\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -140.179993\n","best mean reward -140.179993\n","running time 1850.032852\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -140.17999267578125\n","Train_BestReturn : -140.17999267578125\n","TimeSinceStart : 1850.0328524112701\n","Exploration Critic Loss : 0.014133276417851448\n","Exploitation Critic Q Loss : 0.07260950654745102\n","Exploitation Critic V Loss : 0.009947400540113449\n","Exploration Model Loss : 7.547753193648532e-05\n","Actor Loss : 1.3605520725250244\n","Eval_AverageReturn : -136.875\n","Eval_StdReturn : 34.725486755371094\n","Eval_MaxReturn : -45.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 137.0\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -139.979996\n","best mean reward -139.979996\n","running time 1918.604250\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -139.97999572753906\n","Train_BestReturn : -139.97999572753906\n","TimeSinceStart : 1918.6042504310608\n","Exploration Critic Loss : 0.05133821815252304\n","Exploitation Critic Q Loss : 1.4440230131149292\n","Exploitation Critic V Loss : 0.0045182183384895325\n","Exploration Model Loss : 9.346067236037925e-05\n","Actor Loss : 1.3339293003082275\n","Eval_AverageReturn : -148.42857360839844\n","Eval_StdReturn : 3.849198341369629\n","Eval_MaxReturn : -139.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.57142857142858\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -140.449997\n","best mean reward -139.979996\n","running time 1985.794519\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -140.4499969482422\n","Train_BestReturn : -139.97999572753906\n","TimeSinceStart : 1985.7945187091827\n","Exploration Critic Loss : 0.007974717766046524\n","Exploitation Critic Q Loss : 0.2719907760620117\n","Exploitation Critic V Loss : 0.02697933278977871\n","Exploration Model Loss : 6.810475315432996e-05\n","Actor Loss : 1.3869539499282837\n","Eval_AverageReturn : -142.0\n","Eval_StdReturn : 21.1660099029541\n","Eval_MaxReturn : -86.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 142.125\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -139.759995\n","best mean reward -139.759995\n","running time 2055.759077\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -139.75999450683594\n","Train_BestReturn : -139.75999450683594\n","TimeSinceStart : 2055.7590765953064\n","Exploration Critic Loss : 0.14486490190029144\n","Exploitation Critic Q Loss : 3.546103000640869\n","Exploitation Critic V Loss : 0.020526057109236717\n","Exploration Model Loss : 8.397278179472778e-06\n","Actor Loss : 1.2842720746994019\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -139.759995\n","best mean reward -139.759995\n","running time 2122.053768\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -139.75999450683594\n","Train_BestReturn : -139.75999450683594\n","TimeSinceStart : 2122.053768157959\n","Exploration Critic Loss : 0.16586558520793915\n","Exploitation Critic Q Loss : 4.241186141967773\n","Exploitation Critic V Loss : 0.006662040948867798\n","Exploration Model Loss : 2.5622304633543536e-07\n","Actor Loss : 1.373211145401001\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -138.289993\n","best mean reward -138.289993\n","running time 2193.636429\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -138.2899932861328\n","Train_BestReturn : -138.2899932861328\n","TimeSinceStart : 2193.6364285945892\n","Exploration Critic Loss : 0.21711194515228271\n","Exploitation Critic Q Loss : 5.292774200439453\n","Exploitation Critic V Loss : 0.006508517544716597\n","Exploration Model Loss : 0.00019448623061180115\n","Actor Loss : 1.3621189594268799\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -138.259995\n","best mean reward -138.259995\n","running time 2261.184516\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -138.25999450683594\n","Train_BestReturn : -138.25999450683594\n","TimeSinceStart : 2261.1845161914825\n","Exploration Critic Loss : 0.1051490306854248\n","Exploitation Critic Q Loss : 2.6701982021331787\n","Exploitation Critic V Loss : 0.0075852833688259125\n","Exploration Model Loss : 2.3093541585694766e-06\n","Actor Loss : 1.4095879793167114\n","Eval_AverageReturn : -134.875\n","Eval_StdReturn : 26.2080020904541\n","Eval_MaxReturn : -88.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 135.125\n","Buffer size : 35001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -138.520004\n","best mean reward -138.259995\n","running time 2327.254605\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -138.52000427246094\n","Train_BestReturn : -138.25999450683594\n","TimeSinceStart : 2327.2546050548553\n","Exploration Critic Loss : 0.06275137513875961\n","Exploitation Critic Q Loss : 1.4974970817565918\n","Exploitation Critic V Loss : 0.011925344355404377\n","Exploration Model Loss : 7.36274205337395e-06\n","Actor Loss : 1.3385529518127441\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 36001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -138.029999\n","best mean reward -138.029999\n","running time 2394.663930\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -138.02999877929688\n","Train_BestReturn : -138.02999877929688\n","TimeSinceStart : 2394.6639301776886\n","Exploration Critic Loss : 0.04851812124252319\n","Exploitation Critic Q Loss : 0.7732470631599426\n","Exploitation Critic V Loss : 0.007285897620022297\n","Exploration Model Loss : 6.929625669727102e-05\n","Actor Loss : 1.3466342687606812\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 37001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -138.210007\n","best mean reward -138.029999\n","running time 2459.877581\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -138.2100067138672\n","Train_BestReturn : -138.02999877929688\n","TimeSinceStart : 2459.877580881119\n","Exploration Critic Loss : 0.14763645827770233\n","Exploitation Critic Q Loss : 3.5749576091766357\n","Exploitation Critic V Loss : 0.0148890670388937\n","Exploration Model Loss : 1.2864674658885633e-07\n","Actor Loss : 1.3019943237304688\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 38001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -140.259995\n","best mean reward -138.029999\n","running time 2528.261675\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -140.25999450683594\n","Train_BestReturn : -138.02999877929688\n","TimeSinceStart : 2528.2616753578186\n","Exploration Critic Loss : 0.11790613830089569\n","Exploitation Critic Q Loss : 2.688558340072632\n","Exploitation Critic V Loss : 0.005887399893254042\n","Exploration Model Loss : 3.8163963722581684e-07\n","Actor Loss : 1.37541925907135\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 39001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -140.309998\n","best mean reward -138.029999\n","running time 2595.152140\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -140.30999755859375\n","Train_BestReturn : -138.02999877929688\n","TimeSinceStart : 2595.1521401405334\n","Exploration Critic Loss : 0.0639534443616867\n","Exploitation Critic Q Loss : 1.4150019884109497\n","Exploitation Critic V Loss : 0.021008804440498352\n","Exploration Model Loss : 0.00015663840167690068\n","Actor Loss : 1.3467624187469482\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 40001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -140.309998\n","best mean reward -138.029999\n","running time 2664.172308\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -140.30999755859375\n","Train_BestReturn : -138.02999877929688\n","TimeSinceStart : 2664.1723082065582\n","Exploration Critic Loss : 0.06127811223268509\n","Exploitation Critic Q Loss : 1.328395128250122\n","Exploitation Critic V Loss : 0.004437533672899008\n","Exploration Model Loss : 7.908660336397588e-05\n","Actor Loss : 1.3578479290008545\n","Eval_AverageReturn : -141.375\n","Eval_StdReturn : 17.83912467956543\n","Eval_MaxReturn : -96.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 141.625\n","Buffer size : 41001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -141.220001\n","best mean reward -138.029999\n","running time 2730.829620\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -141.22000122070312\n","Train_BestReturn : -138.02999877929688\n","TimeSinceStart : 2730.8296201229095\n","Exploration Critic Loss : 0.12069694697856903\n","Exploitation Critic Q Loss : 2.6675610542297363\n","Exploitation Critic V Loss : 0.01436804048717022\n","Exploration Model Loss : 0.0004697191761806607\n","Actor Loss : 1.3829364776611328\n","Eval_AverageReturn : -140.875\n","Eval_StdReturn : 24.142480850219727\n","Eval_MaxReturn : -77.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 141.0\n","Buffer size : 42001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -141.860001\n","best mean reward -138.029999\n","running time 2798.603506\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -141.86000061035156\n","Train_BestReturn : -138.02999877929688\n","TimeSinceStart : 2798.6035058498383\n","Exploration Critic Loss : 0.13038094341754913\n","Exploitation Critic Q Loss : 2.7330594062805176\n","Exploitation Critic V Loss : 0.005109055433422327\n","Exploration Model Loss : 0.0002923952997662127\n","Actor Loss : 1.401801347732544\n","Eval_AverageReturn : -145.57142639160156\n","Eval_StdReturn : 10.847740173339844\n","Eval_MaxReturn : -119.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 145.71428571428572\n","Buffer size : 43001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -143.130005\n","best mean reward -138.029999\n","running time 2867.192856\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -143.1300048828125\n","Train_BestReturn : -138.02999877929688\n","TimeSinceStart : 2867.1928560733795\n","Exploration Critic Loss : 0.0679917186498642\n","Exploitation Critic Q Loss : 1.4636214971542358\n","Exploitation Critic V Loss : 0.011534026823937893\n","Exploration Model Loss : 3.9386151911458e-05\n","Actor Loss : 1.3365545272827148\n","Eval_AverageReturn : -145.57142639160156\n","Eval_StdReturn : 10.847740173339844\n","Eval_MaxReturn : -119.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 145.71428571428572\n","Buffer size : 44001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -143.020004\n","best mean reward -138.029999\n","running time 2934.346195\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -143.02000427246094\n","Train_BestReturn : -138.02999877929688\n","TimeSinceStart : 2934.3461952209473\n","Exploration Critic Loss : 0.06597097218036652\n","Exploitation Critic Q Loss : 1.3275554180145264\n","Exploitation Critic V Loss : 0.005851703695952892\n","Exploration Model Loss : 3.5613396676126285e-07\n","Actor Loss : 1.369293212890625\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 45001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -143.679993\n","best mean reward -138.029999\n","running time 3003.727340\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -143.67999267578125\n","Train_BestReturn : -138.02999877929688\n","TimeSinceStart : 3003.727340221405\n","Exploration Critic Loss : 0.008531980216503143\n","Exploitation Critic Q Loss : 0.09075190126895905\n","Exploitation Critic V Loss : 0.00939114298671484\n","Exploration Model Loss : 3.56047735294851e-07\n","Actor Loss : 1.422778606414795\n","Eval_AverageReturn : -147.14285278320312\n","Eval_StdReturn : 6.998542308807373\n","Eval_MaxReturn : -130.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 147.28571428571428\n","Buffer size : 46001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -143.660004\n","best mean reward -138.029999\n","running time 3070.267769\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -143.66000366210938\n","Train_BestReturn : -138.02999877929688\n","TimeSinceStart : 3070.2677693367004\n","Exploration Critic Loss : 0.011488579213619232\n","Exploitation Critic Q Loss : 0.05731593072414398\n","Exploitation Critic V Loss : 0.011793268844485283\n","Exploration Model Loss : 0.0006861628498882055\n","Actor Loss : 1.4043426513671875\n","Eval_AverageReturn : -144.85714721679688\n","Eval_StdReturn : 12.597375869750977\n","Eval_MaxReturn : -114.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 145.0\n","Buffer size : 47001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -144.160004\n","best mean reward -138.029999\n","running time 3139.604307\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -144.16000366210938\n","Train_BestReturn : -138.02999877929688\n","TimeSinceStart : 3139.6043066978455\n","Exploration Critic Loss : 0.07360316067934036\n","Exploitation Critic Q Loss : 1.4252495765686035\n","Exploitation Critic V Loss : 0.007169683463871479\n","Exploration Model Loss : 1.1851373528770637e-05\n","Actor Loss : 1.3269623517990112\n","Eval_AverageReturn : -146.2857208251953\n","Eval_StdReturn : 9.098105430603027\n","Eval_MaxReturn : -124.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 146.42857142857142\n","Buffer size : 48001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -145.919998\n","best mean reward -138.029999\n","running time 3206.310835\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -145.9199981689453\n","Train_BestReturn : -138.02999877929688\n","TimeSinceStart : 3206.310834646225\n","Exploration Critic Loss : 0.12326608598232269\n","Exploitation Critic Q Loss : 2.740584373474121\n","Exploitation Critic V Loss : 0.007252560928463936\n","Exploration Model Loss : 0.00015888948109932244\n","Actor Loss : 1.390759825706482\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 49001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_iql.py --env_name PointmassMedium-v0 \\\n"," --exp_name q5_iql_medium_supervised_lam10_tau0.7 --use_rnd \\\n"," --num_exploration_steps=20000 \\\n"," --awac_lambda=10 \\\n"," --iql_expectile=0.7 \\\n"," --seed 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3516827,"status":"ok","timestamp":1669244031098,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"UWi-hr8SHdNI","outputId":"8a429f5f-c13c-4453-c10a-db7c286a045b"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_medium_unsupervised_lam2_tau0.5_PointmassMedium-v0_23-11-2022_21-55-17 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_medium_unsupervised_lam2_tau0.5_PointmassMedium-v0_23-11-2022_21-55-17\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.005455\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.00545501708984375\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 6.943417\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 6.943417310714722\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 16.068787\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 16.068787097930908\n","Eval_AverageReturn : -141.875\n","Eval_StdReturn : 21.496728897094727\n","Eval_MaxReturn : -85.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 142.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 86.651615\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 86.65161466598511\n","Exploration Critic Loss : 0.01280331052839756\n","Exploitation Critic Q Loss : 0.11396346241235733\n","Exploitation Critic V Loss : 0.00018655031453818083\n","Exploration Model Loss : 0.01053706556558609\n","Actor Loss : 1.5752607583999634\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 158.964291\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 158.96429085731506\n","Exploration Critic Loss : 0.019155586138367653\n","Exploitation Critic Q Loss : 0.18951252102851868\n","Exploitation Critic V Loss : 0.0002776064211502671\n","Exploration Model Loss : 0.0007280183490365744\n","Actor Loss : 1.531134843826294\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 235.965444\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 235.96544432640076\n","Exploration Critic Loss : 0.055427685379981995\n","Exploitation Critic Q Loss : 0.1866845339536667\n","Exploitation Critic V Loss : 0.0004411553090903908\n","Exploration Model Loss : 0.0030939565040171146\n","Actor Loss : 1.520902156829834\n","Eval_AverageReturn : -145.0\n","Eval_StdReturn : 12.247448921203613\n","Eval_MaxReturn : -115.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 145.14285714285714\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 307.205708\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 307.20570826530457\n","Exploration Critic Loss : 0.10130085051059723\n","Exploitation Critic Q Loss : 0.6304720044136047\n","Exploitation Critic V Loss : 0.0010267276084050536\n","Exploration Model Loss : 0.0010223635472357273\n","Actor Loss : 1.518628716468811\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 387.779935\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 387.77993512153625\n","Exploration Critic Loss : 0.11706779152154922\n","Exploitation Critic Q Loss : 0.40068522095680237\n","Exploitation Critic V Loss : 0.001398835564032197\n","Exploration Model Loss : 0.0008404991822317243\n","Actor Loss : 1.456583857536316\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 461.482777\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 461.48277735710144\n","Exploration Critic Loss : 0.11992067843675613\n","Exploitation Critic Q Loss : 0.012986812740564346\n","Exploitation Critic V Loss : 0.0022379097063094378\n","Exploration Model Loss : 0.0006151189445517957\n","Actor Loss : 1.4479111433029175\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 534.935940\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 534.9359397888184\n","Exploration Critic Loss : 0.178705632686615\n","Exploitation Critic Q Loss : 1.1806633472442627\n","Exploitation Critic V Loss : 0.0017595284152776003\n","Exploration Model Loss : 0.0005919918767176569\n","Actor Loss : 1.4895610809326172\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 605.998282\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 605.9982821941376\n","Exploration Critic Loss : 0.13061703741550446\n","Exploitation Critic Q Loss : 0.6545654535293579\n","Exploitation Critic V Loss : 0.0034333092626184225\n","Exploration Model Loss : 0.0006272849277593195\n","Actor Loss : 1.502587914466858\n","Eval_AverageReturn : -142.5\n","Eval_StdReturn : 19.8431339263916\n","Eval_MaxReturn : -90.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 142.625\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -149.616440\n","best mean reward -inf\n","running time 677.690518\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -149.61643981933594\n","TimeSinceStart : 677.6905179023743\n","Exploration Critic Loss : 0.10201595723628998\n","Exploitation Critic Q Loss : 0.7319396734237671\n","Exploitation Critic V Loss : 0.003696649568155408\n","Exploration Model Loss : 0.003966126590967178\n","Actor Loss : 1.4445644617080688\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -149.649994\n","best mean reward -inf\n","running time 753.335770\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -149.64999389648438\n","TimeSinceStart : 753.335770368576\n","Exploration Critic Loss : 0.10060843825340271\n","Exploitation Critic Q Loss : 2.3843533992767334\n","Exploitation Critic V Loss : 0.00743022421374917\n","Exploration Model Loss : 0.00018528057262301445\n","Actor Loss : 1.4261692762374878\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -149.674423\n","best mean reward -inf\n","running time 826.776441\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -149.67442321777344\n","TimeSinceStart : 826.7764413356781\n","Exploration Critic Loss : 0.11524542421102524\n","Exploitation Critic Q Loss : 3.4277775287628174\n","Exploitation Critic V Loss : 0.002960277022793889\n","Exploration Model Loss : 0.00011791236465796828\n","Actor Loss : 1.3986669778823853\n","Eval_AverageReturn : -148.57142639160156\n","Eval_StdReturn : 3.4992711544036865\n","Eval_MaxReturn : -140.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.71428571428572\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -149.698929\n","best mean reward -inf\n","running time 898.188071\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -149.6989288330078\n","TimeSinceStart : 898.1880712509155\n","Exploration Critic Loss : 0.1011556088924408\n","Exploitation Critic Q Loss : 3.482868194580078\n","Exploitation Critic V Loss : 0.006110656075179577\n","Exploration Model Loss : 7.942388037918136e-05\n","Actor Loss : 1.432574987411499\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -149.720001\n","best mean reward -inf\n","running time 974.015796\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -149.72000122070312\n","TimeSinceStart : 974.0157959461212\n","Exploration Critic Loss : 0.05155140161514282\n","Exploitation Critic Q Loss : 1.7790297269821167\n","Exploitation Critic V Loss : 0.006306112743914127\n","Exploration Model Loss : 3.1989344279281795e-05\n","Actor Loss : 1.423585295677185\n","Eval_AverageReturn : -149.85714721679688\n","Eval_StdReturn : 0.3499270975589752\n","Eval_MaxReturn : -149.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -149.720001\n","best mean reward -149.720001\n","running time 1051.901143\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -149.72000122070312\n","Train_BestReturn : -149.72000122070312\n","TimeSinceStart : 1051.9011433124542\n","Exploration Critic Loss : 0.025098824873566628\n","Exploitation Critic Q Loss : 0.8245059847831726\n","Exploitation Critic V Loss : 0.0029853361193090677\n","Exploration Model Loss : 2.286790004291106e-05\n","Actor Loss : 1.3775126934051514\n","Eval_AverageReturn : -147.42857360839844\n","Eval_StdReturn : 6.298687934875488\n","Eval_MaxReturn : -132.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 147.57142857142858\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -149.720001\n","best mean reward -149.720001\n","running time 1122.980457\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -149.72000122070312\n","Train_BestReturn : -149.72000122070312\n","TimeSinceStart : 1122.9804573059082\n","Exploration Critic Loss : 0.09266956895589828\n","Exploitation Critic Q Loss : 2.5339791774749756\n","Exploitation Critic V Loss : 0.008819332346320152\n","Exploration Model Loss : 1.4643373106082436e-05\n","Actor Loss : 1.3973406553268433\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -149.720001\n","best mean reward -149.720001\n","running time 1197.179183\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -149.72000122070312\n","Train_BestReturn : -149.72000122070312\n","TimeSinceStart : 1197.1791825294495\n","Exploration Critic Loss : 0.06942145526409149\n","Exploitation Critic Q Loss : 1.61411714553833\n","Exploitation Critic V Loss : 0.00247622886672616\n","Exploration Model Loss : 1.570690619701054e-05\n","Actor Loss : 1.2699518203735352\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -149.720001\n","best mean reward -149.720001\n","running time 1266.517432\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -149.72000122070312\n","Train_BestReturn : -149.72000122070312\n","TimeSinceStart : 1266.517431974411\n","Exploration Critic Loss : 0.05223315581679344\n","Exploitation Critic Q Loss : 2.7934348583221436\n","Exploitation Critic V Loss : 0.005265101324766874\n","Exploration Model Loss : 0.0001323287870036438\n","Actor Loss : 1.3982117176055908\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -149.669998\n","best mean reward -149.669998\n","running time 1340.192420\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -149.6699981689453\n","Train_BestReturn : -149.6699981689453\n","TimeSinceStart : 1340.192420244217\n","Exploration Critic Loss : 0.045052316039800644\n","Exploitation Critic Q Loss : 2.2009263038635254\n","Exploitation Critic V Loss : 0.005152719095349312\n","Exploration Model Loss : 6.967299850657582e-05\n","Actor Loss : 1.4368538856506348\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -149.669998\n","best mean reward -149.669998\n","running time 1416.563157\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -149.6699981689453\n","Train_BestReturn : -149.6699981689453\n","TimeSinceStart : 1416.563157081604\n","Exploration Critic Loss : 0.15895067155361176\n","Exploitation Critic Q Loss : 0.04524464160203934\n","Exploitation Critic V Loss : 0.0072614820674061775\n","Exploration Model Loss : 0.00027948227943852544\n","Actor Loss : 1.4079833030700684\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -149.669998\n","best mean reward -149.669998\n","running time 1488.276165\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -149.6699981689453\n","Train_BestReturn : -149.6699981689453\n","TimeSinceStart : 1488.2761652469635\n","Exploration Critic Loss : 0.15799225866794586\n","Exploitation Critic Q Loss : 4.187605381011963\n","Exploitation Critic V Loss : 0.011064836755394936\n","Exploration Model Loss : 0.00012255234469193965\n","Actor Loss : 1.4296385049819946\n","Eval_AverageReturn : -141.375\n","Eval_StdReturn : 22.819604873657227\n","Eval_MaxReturn : -81.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 141.5\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -149.669998\n","best mean reward -149.669998\n","running time 1564.394036\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -149.6699981689453\n","Train_BestReturn : -149.6699981689453\n","TimeSinceStart : 1564.3940362930298\n","Exploration Critic Loss : 0.09320402145385742\n","Exploitation Critic Q Loss : 0.9120962023735046\n","Exploitation Critic V Loss : 0.0070533910766243935\n","Exploration Model Loss : 8.81630476214923e-05\n","Actor Loss : 1.3629961013793945\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -149.089996\n","best mean reward -149.089996\n","running time 1636.257383\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -149.08999633789062\n","Train_BestReturn : -149.08999633789062\n","TimeSinceStart : 1636.2573833465576\n","Exploration Critic Loss : 0.20041382312774658\n","Exploitation Critic Q Loss : 2.0271942615509033\n","Exploitation Critic V Loss : 0.008533523418009281\n","Exploration Model Loss : 6.503117765532807e-05\n","Actor Loss : 1.3807001113891602\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -148.320007\n","best mean reward -148.320007\n","running time 1711.185159\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -148.32000732421875\n","Train_BestReturn : -148.32000732421875\n","TimeSinceStart : 1711.1851587295532\n","Exploration Critic Loss : 0.21176722645759583\n","Exploitation Critic Q Loss : 3.7897918224334717\n","Exploitation Critic V Loss : 0.007504550274461508\n","Exploration Model Loss : 4.632184572983533e-05\n","Actor Loss : 1.3619413375854492\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -148.029999\n","best mean reward -148.029999\n","running time 1784.012449\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -148.02999877929688\n","Train_BestReturn : -148.02999877929688\n","TimeSinceStart : 1784.0124490261078\n","Exploration Critic Loss : 0.3062543272972107\n","Exploitation Critic Q Loss : 4.190723896026611\n","Exploitation Critic V Loss : 0.008782495744526386\n","Exploration Model Loss : 1.330695431533968e-05\n","Actor Loss : 1.3973910808563232\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -148.029999\n","best mean reward -148.029999\n","running time 1854.127749\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -148.02999877929688\n","Train_BestReturn : -148.02999877929688\n","TimeSinceStart : 1854.1277494430542\n","Exploration Critic Loss : 0.3636760711669922\n","Exploitation Critic Q Loss : 4.4778852462768555\n","Exploitation Critic V Loss : 0.012799630872905254\n","Exploration Model Loss : 0.00024517832207493484\n","Actor Loss : 1.3875348567962646\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -148.029999\n","best mean reward -148.029999\n","running time 1932.554219\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -148.02999877929688\n","Train_BestReturn : -148.02999877929688\n","TimeSinceStart : 1932.5542194843292\n","Exploration Critic Loss : 0.28200221061706543\n","Exploitation Critic Q Loss : 1.9106292724609375\n","Exploitation Critic V Loss : 0.006655045319348574\n","Exploration Model Loss : 1.644252915866673e-05\n","Actor Loss : 1.41448175907135\n","Eval_AverageReturn : -144.0\n","Eval_StdReturn : 14.696938514709473\n","Eval_MaxReturn : -108.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 144.14285714285714\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -148.029999\n","best mean reward -148.029999\n","running time 2006.135136\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -148.02999877929688\n","Train_BestReturn : -148.02999877929688\n","TimeSinceStart : 2006.135136127472\n","Exploration Critic Loss : 0.4927755296230316\n","Exploitation Critic Q Loss : 2.9989259243011475\n","Exploitation Critic V Loss : 0.007186460308730602\n","Exploration Model Loss : 3.6116334740654565e-06\n","Actor Loss : 1.3661307096481323\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -148.029999\n","best mean reward -148.029999\n","running time 2081.697602\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -148.02999877929688\n","Train_BestReturn : -148.02999877929688\n","TimeSinceStart : 2081.697601556778\n","Exploration Critic Loss : 0.3458697199821472\n","Exploitation Critic Q Loss : 0.0453992523252964\n","Exploitation Critic V Loss : 0.003107979893684387\n","Exploration Model Loss : 5.8099998568650335e-05\n","Actor Loss : 1.293759822845459\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -148.029999\n","best mean reward -148.029999\n","running time 2156.381849\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -148.02999877929688\n","Train_BestReturn : -148.02999877929688\n","TimeSinceStart : 2156.3818485736847\n","Exploration Critic Loss : 0.6105601787567139\n","Exploitation Critic Q Loss : 1.8518822193145752\n","Exploitation Critic V Loss : 0.006729205138981342\n","Exploration Model Loss : 0.0002813133760355413\n","Actor Loss : 1.3468148708343506\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -147.559998\n","best mean reward -147.559998\n","running time 2230.610521\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -147.55999755859375\n","Train_BestReturn : -147.55999755859375\n","TimeSinceStart : 2230.6105206012726\n","Exploration Critic Loss : 0.7677465677261353\n","Exploitation Critic Q Loss : 2.142854928970337\n","Exploitation Critic V Loss : 0.006737202405929565\n","Exploration Model Loss : 4.886397277914512e-07\n","Actor Loss : 1.3286755084991455\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -146.509995\n","best mean reward -146.509995\n","running time 2298.191521\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -146.50999450683594\n","Train_BestReturn : -146.50999450683594\n","TimeSinceStart : 2298.191520690918\n","Exploration Critic Loss : 0.44586315751075745\n","Exploitation Critic Q Loss : 0.050798624753952026\n","Exploitation Critic V Loss : 0.004159415140748024\n","Exploration Model Loss : 8.775568858254701e-07\n","Actor Loss : 1.3719024658203125\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -146.509995\n","best mean reward -146.509995\n","running time 2373.463160\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -146.50999450683594\n","Train_BestReturn : -146.50999450683594\n","TimeSinceStart : 2373.463160276413\n","Exploration Critic Loss : 1.314728021621704\n","Exploitation Critic Q Loss : 2.148467540740967\n","Exploitation Critic V Loss : 0.004530682694166899\n","Exploration Model Loss : 1.609033643035218e-05\n","Actor Loss : 1.2472933530807495\n","Eval_AverageReturn : -148.7142791748047\n","Eval_StdReturn : 3.149343729019165\n","Eval_MaxReturn : -141.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.85714285714286\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -146.559998\n","best mean reward -146.509995\n","running time 2444.583535\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -146.55999755859375\n","Train_BestReturn : -146.50999450683594\n","TimeSinceStart : 2444.5835354328156\n","Exploration Critic Loss : 0.9696799516677856\n","Exploitation Critic Q Loss : 0.9614124298095703\n","Exploitation Critic V Loss : 0.004000662360340357\n","Exploration Model Loss : 5.5461925512645394e-05\n","Actor Loss : 1.2856578826904297\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 35001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -145.910004\n","best mean reward -145.910004\n","running time 2517.982099\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -145.91000366210938\n","Train_BestReturn : -145.91000366210938\n","TimeSinceStart : 2517.9820988178253\n","Exploration Critic Loss : 1.2957594394683838\n","Exploitation Critic Q Loss : 0.957254946231842\n","Exploitation Critic V Loss : 0.007752629928290844\n","Exploration Model Loss : 3.786849993048236e-05\n","Actor Loss : 1.3304694890975952\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 36001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -145.509995\n","best mean reward -145.509995\n","running time 2592.286601\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -145.50999450683594\n","Train_BestReturn : -145.50999450683594\n","TimeSinceStart : 2592.286600828171\n","Exploration Critic Loss : 1.3722028732299805\n","Exploitation Critic Q Loss : 1.037552833557129\n","Exploitation Critic V Loss : 0.00665745884180069\n","Exploration Model Loss : 2.1386039605886253e-08\n","Actor Loss : 1.353912353515625\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 37001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -143.089996\n","best mean reward -143.089996\n","running time 2659.954959\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -143.08999633789062\n","Train_BestReturn : -143.08999633789062\n","TimeSinceStart : 2659.9549593925476\n","Exploration Critic Loss : 1.409045934677124\n","Exploitation Critic Q Loss : 4.41153621673584\n","Exploitation Critic V Loss : 0.010021526366472244\n","Exploration Model Loss : 9.175546011874758e-08\n","Actor Loss : 1.36855149269104\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 38001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -139.440002\n","best mean reward -139.440002\n","running time 2733.225144\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -139.44000244140625\n","Train_BestReturn : -139.44000244140625\n","TimeSinceStart : 2733.2251436710358\n","Exploration Critic Loss : 1.7482664585113525\n","Exploitation Critic Q Loss : 1.0013339519500732\n","Exploitation Critic V Loss : 0.006004699040204287\n","Exploration Model Loss : 3.542679769452661e-05\n","Actor Loss : 1.3851326704025269\n","Eval_AverageReturn : -136.75\n","Eval_StdReturn : 35.05620574951172\n","Eval_MaxReturn : -44.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 136.875\n","Buffer size : 39001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -139.000000\n","best mean reward -139.000000\n","running time 2801.414603\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -139.0\n","Train_BestReturn : -139.0\n","TimeSinceStart : 2801.4146032333374\n","Exploration Critic Loss : 2.529099225997925\n","Exploitation Critic Q Loss : 1.003379225730896\n","Exploitation Critic V Loss : 0.0049797287210822105\n","Exploration Model Loss : 6.722621037624776e-05\n","Actor Loss : 1.3758184909820557\n","Eval_AverageReturn : -146.42857360839844\n","Eval_StdReturn : 8.748177528381348\n","Eval_MaxReturn : -125.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 146.57142857142858\n","Buffer size : 40001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -134.289993\n","best mean reward -134.289993\n","running time 2875.098850\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -134.2899932861328\n","Train_BestReturn : -134.2899932861328\n","TimeSinceStart : 2875.098850250244\n","Exploration Critic Loss : 2.547874689102173\n","Exploitation Critic Q Loss : 1.0360887050628662\n","Exploitation Critic V Loss : 0.01202000118792057\n","Exploration Model Loss : 1.515294934506528e-05\n","Actor Loss : 1.391381859779358\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 41001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -132.130005\n","best mean reward -132.130005\n","running time 2941.143611\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -132.1300048828125\n","Train_BestReturn : -132.1300048828125\n","TimeSinceStart : 2941.143611431122\n","Exploration Critic Loss : 2.5810725688934326\n","Exploitation Critic Q Loss : 2.025754451751709\n","Exploitation Critic V Loss : 0.012063342146575451\n","Exploration Model Loss : 1.7982891620249575e-07\n","Actor Loss : 1.3263648748397827\n","Eval_AverageReturn : -149.0\n","Eval_StdReturn : 2.4494898319244385\n","Eval_MaxReturn : -143.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 149.14285714285714\n","Buffer size : 42001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -128.669998\n","best mean reward -128.669998\n","running time 3014.965258\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -128.6699981689453\n","Train_BestReturn : -128.6699981689453\n","TimeSinceStart : 3014.965258359909\n","Exploration Critic Loss : 2.6064279079437256\n","Exploitation Critic Q Loss : 0.175863578915596\n","Exploitation Critic V Loss : 0.017615733668208122\n","Exploration Model Loss : 3.909816314262571e-06\n","Actor Loss : 1.3831048011779785\n","Eval_AverageReturn : -148.57142639160156\n","Eval_StdReturn : 3.4992713928222656\n","Eval_MaxReturn : -140.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.71428571428572\n","Buffer size : 43001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -126.500000\n","best mean reward -126.500000\n","running time 3086.579260\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -126.5\n","Train_BestReturn : -126.5\n","TimeSinceStart : 3086.5792603492737\n","Exploration Critic Loss : 3.6993324756622314\n","Exploitation Critic Q Loss : 3.6383371353149414\n","Exploitation Critic V Loss : 0.014644507318735123\n","Exploration Model Loss : 6.953512183827115e-06\n","Actor Loss : 1.3635296821594238\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 44001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -124.709999\n","best mean reward -124.709999\n","running time 3155.132411\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -124.70999908447266\n","Train_BestReturn : -124.70999908447266\n","TimeSinceStart : 3155.1324105262756\n","Exploration Critic Loss : 2.468074083328247\n","Exploitation Critic Q Loss : 1.5479872226715088\n","Exploitation Critic V Loss : 0.01732877641916275\n","Exploration Model Loss : 2.426317280423973e-07\n","Actor Loss : 1.3178778886795044\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 45001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -119.129997\n","best mean reward -119.129997\n","running time 3227.018629\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -119.12999725341797\n","Train_BestReturn : -119.12999725341797\n","TimeSinceStart : 3227.0186293125153\n","Exploration Critic Loss : 2.784605026245117\n","Exploitation Critic Q Loss : 1.3749831914901733\n","Exploitation Critic V Loss : 0.018453465774655342\n","Exploration Model Loss : 0.0009890388464555144\n","Actor Loss : 1.3330849409103394\n","Eval_AverageReturn : -125.375\n","Eval_StdReturn : 35.66138458251953\n","Eval_MaxReturn : -44.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 125.875\n","Buffer size : 46001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -116.470001\n","best mean reward -116.470001\n","running time 3295.632567\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -116.47000122070312\n","Train_BestReturn : -116.47000122070312\n","TimeSinceStart : 3295.632567167282\n","Exploration Critic Loss : 4.407616138458252\n","Exploitation Critic Q Loss : 0.17248192429542542\n","Exploitation Critic V Loss : 0.017038583755493164\n","Exploration Model Loss : 0.0002509354962967336\n","Actor Loss : 1.2810518741607666\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 47001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -114.379997\n","best mean reward -114.379997\n","running time 3370.446449\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -114.37999725341797\n","Train_BestReturn : -114.37999725341797\n","TimeSinceStart : 3370.4464490413666\n","Exploration Critic Loss : 3.6077632904052734\n","Exploitation Critic Q Loss : 1.2869873046875\n","Exploitation Critic V Loss : 0.01172246877104044\n","Exploration Model Loss : 0.00020727128139697015\n","Actor Loss : 1.3809008598327637\n","Eval_AverageReturn : -142.625\n","Eval_StdReturn : 19.51241683959961\n","Eval_MaxReturn : -91.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 142.75\n","Buffer size : 48001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -113.959999\n","best mean reward -113.959999\n","running time 3440.897245\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -113.95999908447266\n","Train_BestReturn : -113.95999908447266\n","TimeSinceStart : 3440.8972454071045\n","Exploration Critic Loss : 4.901241302490234\n","Exploitation Critic Q Loss : 1.5179036855697632\n","Exploitation Critic V Loss : 0.03715336322784424\n","Exploration Model Loss : 0.0001871230051619932\n","Actor Loss : 1.2415961027145386\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 49001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_iql.py --env_name PointmassMedium-v0 \\\n"," --exp_name q5_iql_medium_unsupervised_lam2_tau0.5 --use_rnd \\\n"," --unsupervised_exploration \\\n"," --num_exploration_steps=20000 \\\n"," --awac_lambda=2 \\\n"," --iql_expectile=0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3320613,"status":"ok","timestamp":1669247351700,"user":{"displayName":"Yorick Chern","userId":"04667733807414025524"},"user_tz":480},"id":"UKOyhL41IE5j","outputId":"7903ad3d-92b0-4ce3-d5bf-0a704748c957"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_medium_unsupervised_lam2_tau0.7_PointmassMedium-v0_23-11-2022_22-53-54 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_medium_unsupervised_lam2_tau0.7_PointmassMedium-v0_23-11-2022_22-53-54\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002340\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0023398399353027344\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 6.656572\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 6.656571626663208\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -138.857147\n","best mean reward -inf\n","running time 13.817400\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -138.85714721679688\n","TimeSinceStart : 13.817400455474854\n","Eval_AverageReturn : -141.875\n","Eval_StdReturn : 21.496728897094727\n","Eval_MaxReturn : -85.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 142.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -142.571426\n","best mean reward -inf\n","running time 82.209941\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -142.57142639160156\n","TimeSinceStart : 82.20994138717651\n","Exploration Critic Loss : 0.025194207206368446\n","Exploitation Critic Q Loss : 0.028585240244865417\n","Exploitation Critic V Loss : 0.00024292265879921615\n","Exploration Model Loss : 0.9609844088554382\n","Actor Loss : 1.5589414834976196\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -144.222229\n","best mean reward -inf\n","running time 151.277170\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -144.22222900390625\n","TimeSinceStart : 151.27716970443726\n","Exploration Critic Loss : 0.003245329950004816\n","Exploitation Critic Q Loss : 0.09472431242465973\n","Exploitation Critic V Loss : 0.0004104320250917226\n","Exploration Model Loss : 0.000664175720885396\n","Actor Loss : 1.5375137329101562\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -145.411758\n","best mean reward -inf\n","running time 222.273305\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -145.41175842285156\n","TimeSinceStart : 222.27330541610718\n","Exploration Critic Loss : 0.007363101001828909\n","Exploitation Critic Q Loss : 0.35255277156829834\n","Exploitation Critic V Loss : 0.0014273255364969373\n","Exploration Model Loss : 0.0010146412532776594\n","Actor Loss : 1.4227631092071533\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -146.195129\n","best mean reward -inf\n","running time 294.245426\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -146.19512939453125\n","TimeSinceStart : 294.24542570114136\n","Exploration Critic Loss : 0.012858881615102291\n","Exploitation Critic Q Loss : 0.3123416006565094\n","Exploitation Critic V Loss : 0.0029558613896369934\n","Exploration Model Loss : 0.0009055746486410499\n","Actor Loss : 1.4879090785980225\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -146.680847\n","best mean reward -inf\n","running time 367.547810\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -146.68084716796875\n","TimeSinceStart : 367.54780983924866\n","Exploration Critic Loss : 0.019588831812143326\n","Exploitation Critic Q Loss : 0.8152370452880859\n","Exploitation Critic V Loss : 0.003657618537545204\n","Exploration Model Loss : 0.0007747440249659121\n","Actor Loss : 1.4574013948440552\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -147.111115\n","best mean reward -inf\n","running time 436.533545\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -147.11111450195312\n","TimeSinceStart : 436.53354501724243\n","Exploration Critic Loss : 0.022206291556358337\n","Exploitation Critic Q Loss : 1.0051507949829102\n","Exploitation Critic V Loss : 0.002170642837882042\n","Exploration Model Loss : 0.0004361080063972622\n","Actor Loss : 1.3661763668060303\n","Eval_AverageReturn : -148.57142639160156\n","Eval_StdReturn : 3.4992711544036865\n","Eval_MaxReturn : -140.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.71428571428572\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -147.442627\n","best mean reward -inf\n","running time 507.377939\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -147.442626953125\n","TimeSinceStart : 507.3779389858246\n","Exploration Critic Loss : 0.025096219033002853\n","Exploitation Critic Q Loss : 1.1022958755493164\n","Exploitation Critic V Loss : 0.002721776720136404\n","Exploration Model Loss : 0.0003520140890032053\n","Actor Loss : 1.3483202457427979\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -147.671646\n","best mean reward -inf\n","running time 573.266257\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -147.67164611816406\n","TimeSinceStart : 573.2662568092346\n","Exploration Critic Loss : 0.05784371495246887\n","Exploitation Critic Q Loss : 0.017642125487327576\n","Exploitation Critic V Loss : 0.2822534739971161\n","Exploration Model Loss : 0.00029465474653989077\n","Actor Loss : 1.335922122001648\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -147.891891\n","best mean reward -inf\n","running time 640.320990\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -147.8918914794922\n","TimeSinceStart : 640.3209898471832\n","Exploration Critic Loss : 0.021168651059269905\n","Exploitation Critic Q Loss : 1.279414415359497\n","Exploitation Critic V Loss : 0.004686064552515745\n","Exploration Model Loss : 0.00021521950839087367\n","Actor Loss : 1.2899894714355469\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -147.123459\n","best mean reward -inf\n","running time 707.414556\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -147.1234588623047\n","TimeSinceStart : 707.4145557880402\n","Exploration Critic Loss : 0.016822049394249916\n","Exploitation Critic Q Loss : 1.3839302062988281\n","Exploitation Critic V Loss : 0.0035986381117254496\n","Exploration Model Loss : 0.00013515408500097692\n","Actor Loss : 1.2880706787109375\n","Eval_AverageReturn : -143.42857360839844\n","Eval_StdReturn : 16.09664535522461\n","Eval_MaxReturn : -104.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 143.57142857142858\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -146.579544\n","best mean reward -inf\n","running time 774.841206\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -146.5795440673828\n","TimeSinceStart : 774.8412063121796\n","Exploration Critic Loss : 0.014652781188488007\n","Exploitation Critic Q Loss : 0.8386338353157043\n","Exploitation Critic V Loss : 0.004796301014721394\n","Exploration Model Loss : 9.686702105682343e-05\n","Actor Loss : 1.3535711765289307\n","Eval_AverageReturn : -142.125\n","Eval_StdReturn : 20.835290908813477\n","Eval_MaxReturn : -87.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 142.25\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -146.747375\n","best mean reward -inf\n","running time 844.243863\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -146.74737548828125\n","TimeSinceStart : 844.2438633441925\n","Exploration Critic Loss : 0.023096855729818344\n","Exploitation Critic Q Loss : 0.07345757633447647\n","Exploitation Critic V Loss : 0.0034209545701742172\n","Exploration Model Loss : 6.324896094156429e-05\n","Actor Loss : 1.3473550081253052\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -145.529999\n","best mean reward -145.529999\n","running time 908.130908\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -145.52999877929688\n","Train_BestReturn : -145.52999877929688\n","TimeSinceStart : 908.130907535553\n","Exploration Critic Loss : 0.08115153014659882\n","Exploitation Critic Q Loss : 0.2631608545780182\n","Exploitation Critic V Loss : 0.08443200588226318\n","Exploration Model Loss : 4.19390999013558e-05\n","Actor Loss : 1.2658511400222778\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -144.449997\n","best mean reward -144.449997\n","running time 975.599375\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -144.4499969482422\n","Train_BestReturn : -144.4499969482422\n","TimeSinceStart : 975.5993750095367\n","Exploration Critic Loss : 0.020043395459651947\n","Exploitation Critic Q Loss : 0.8157821297645569\n","Exploitation Critic V Loss : 0.01040962990373373\n","Exploration Model Loss : 2.155181937268935e-05\n","Actor Loss : 1.2735601663589478\n","Eval_AverageReturn : -143.0\n","Eval_StdReturn : 11.771636962890625\n","Eval_MaxReturn : -118.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 143.28571428571428\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -143.300003\n","best mean reward -143.300003\n","running time 1040.912321\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -143.3000030517578\n","Train_BestReturn : -143.3000030517578\n","TimeSinceStart : 1040.9123210906982\n","Exploration Critic Loss : 0.07797640562057495\n","Exploitation Critic Q Loss : 0.04550093784928322\n","Exploitation Critic V Loss : 0.012669220566749573\n","Exploration Model Loss : 1.7772816136130132e-05\n","Actor Loss : 1.2919243574142456\n","Eval_AverageReturn : -147.85714721679688\n","Eval_StdReturn : 5.248906135559082\n","Eval_MaxReturn : -135.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.0\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -135.639999\n","best mean reward -135.639999\n","running time 1109.726917\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -135.63999938964844\n","Train_BestReturn : -135.63999938964844\n","TimeSinceStart : 1109.7269167900085\n","Exploration Critic Loss : 0.38707998394966125\n","Exploitation Critic Q Loss : 2.9859440326690674\n","Exploitation Critic V Loss : 0.017599521204829216\n","Exploration Model Loss : 1.023304503178224e-05\n","Actor Loss : 1.3178826570510864\n","Eval_AverageReturn : -128.625\n","Eval_StdReturn : 37.63288879394531\n","Eval_MaxReturn : -51.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 128.875\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -127.730003\n","best mean reward -127.730003\n","running time 1173.636800\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -127.7300033569336\n","Train_BestReturn : -127.7300033569336\n","TimeSinceStart : 1173.636799812317\n","Exploration Critic Loss : 0.24989120662212372\n","Exploitation Critic Q Loss : 1.0140442848205566\n","Exploitation Critic V Loss : 0.014095902442932129\n","Exploration Model Loss : 2.4126025891746394e-05\n","Actor Loss : 1.2661211490631104\n","Eval_AverageReturn : -121.22222137451172\n","Eval_StdReturn : 34.774757385253906\n","Eval_MaxReturn : -64.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 121.66666666666667\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -125.370003\n","best mean reward -125.370003\n","running time 1240.527939\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -125.37000274658203\n","Train_BestReturn : -125.37000274658203\n","TimeSinceStart : 1240.527939081192\n","Exploration Critic Loss : 0.387884259223938\n","Exploitation Critic Q Loss : 0.2785418629646301\n","Exploitation Critic V Loss : 0.023557022213935852\n","Exploration Model Loss : 2.7079044230049476e-05\n","Actor Loss : 1.2975648641586304\n","Eval_AverageReturn : -148.85714721679688\n","Eval_StdReturn : 2.799417018890381\n","Eval_MaxReturn : -142.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 149.0\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -119.339996\n","best mean reward -119.339996\n","running time 1311.090455\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -119.33999633789062\n","Train_BestReturn : -119.33999633789062\n","TimeSinceStart : 1311.0904548168182\n","Exploration Critic Loss : 0.565311074256897\n","Exploitation Critic Q Loss : 1.2964370250701904\n","Exploitation Critic V Loss : 0.03596718981862068\n","Exploration Model Loss : 2.752727596089244e-05\n","Actor Loss : 1.356858730316162\n","Eval_AverageReturn : -141.375\n","Eval_StdReturn : 12.236599922180176\n","Eval_MaxReturn : -116.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 141.75\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -109.230003\n","best mean reward -109.230003\n","running time 1378.960130\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -109.2300033569336\n","Train_BestReturn : -109.2300033569336\n","TimeSinceStart : 1378.9601302146912\n","Exploration Critic Loss : 0.2503311038017273\n","Exploitation Critic Q Loss : 1.126345157623291\n","Exploitation Critic V Loss : 0.04150661826133728\n","Exploration Model Loss : 0.00022207105939742178\n","Actor Loss : 1.3620753288269043\n","Eval_AverageReturn : -138.375\n","Eval_StdReturn : 18.02732276916504\n","Eval_MaxReturn : -102.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 138.75\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -102.339996\n","best mean reward -102.339996\n","running time 1448.428451\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -102.33999633789062\n","Train_BestReturn : -102.33999633789062\n","TimeSinceStart : 1448.4284512996674\n","Exploration Critic Loss : 0.24504129588603973\n","Exploitation Critic Q Loss : 0.1687634140253067\n","Exploitation Critic V Loss : 0.052915602922439575\n","Exploration Model Loss : 3.220201324438676e-05\n","Actor Loss : 1.3573083877563477\n","Eval_AverageReturn : -128.875\n","Eval_StdReturn : 24.323020935058594\n","Eval_MaxReturn : -81.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 129.375\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -91.110001\n","best mean reward -91.110001\n","running time 1513.769204\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -91.11000061035156\n","Train_BestReturn : -91.11000061035156\n","TimeSinceStart : 1513.769203901291\n","Exploration Critic Loss : 0.5277758240699768\n","Exploitation Critic Q Loss : 0.317228764295578\n","Exploitation Critic V Loss : 0.038334716111421585\n","Exploration Model Loss : 0.00010186194413108751\n","Actor Loss : 1.3062143325805664\n","Eval_AverageReturn : -140.0\n","Eval_StdReturn : 17.471405029296875\n","Eval_MaxReturn : -99.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 140.375\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -80.750000\n","best mean reward -80.750000\n","running time 1586.063691\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -80.75\n","Train_BestReturn : -80.75\n","TimeSinceStart : 1586.0636913776398\n","Exploration Critic Loss : 0.4481002688407898\n","Exploitation Critic Q Loss : 1.282612919807434\n","Exploitation Critic V Loss : 0.044929683208465576\n","Exploration Model Loss : 3.755304351216182e-05\n","Actor Loss : 1.3690242767333984\n","Eval_AverageReturn : -126.0\n","Eval_StdReturn : 35.017852783203125\n","Eval_MaxReturn : -57.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 126.5\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -74.330002\n","best mean reward -74.330002\n","running time 1651.018030\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -74.33000183105469\n","Train_BestReturn : -74.33000183105469\n","TimeSinceStart : 1651.0180304050446\n","Exploration Critic Loss : 0.7213586568832397\n","Exploitation Critic Q Loss : 4.9342427253723145\n","Exploitation Critic V Loss : 0.060972824692726135\n","Exploration Model Loss : 9.989943646360189e-05\n","Actor Loss : 1.2837342023849487\n","Eval_AverageReturn : -125.55555725097656\n","Eval_StdReturn : 28.41013526916504\n","Eval_MaxReturn : -83.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 126.0\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -69.620003\n","best mean reward -69.620003\n","running time 1721.163991\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -69.62000274658203\n","Train_BestReturn : -69.62000274658203\n","TimeSinceStart : 1721.1639912128448\n","Exploration Critic Loss : 0.7899667024612427\n","Exploitation Critic Q Loss : 0.8766980767250061\n","Exploitation Critic V Loss : 0.05863262340426445\n","Exploration Model Loss : 0.0001641026756260544\n","Actor Loss : 1.3888156414031982\n","Eval_AverageReturn : -105.0\n","Eval_StdReturn : 35.22499084472656\n","Eval_MaxReturn : -44.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 105.7\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -63.410000\n","best mean reward -63.410000\n","running time 1786.628342\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -63.40999984741211\n","Train_BestReturn : -63.40999984741211\n","TimeSinceStart : 1786.6283421516418\n","Exploration Critic Loss : 0.6097495555877686\n","Exploitation Critic Q Loss : 0.3091845214366913\n","Exploitation Critic V Loss : 0.06502987444400787\n","Exploration Model Loss : 0.0001559805532451719\n","Actor Loss : 1.2686184644699097\n","Eval_AverageReturn : -99.4000015258789\n","Eval_StdReturn : 44.726280212402344\n","Eval_MaxReturn : -29.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 100.2\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -58.340000\n","best mean reward -58.340000\n","running time 1858.104463\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -58.34000015258789\n","Train_BestReturn : -58.34000015258789\n","TimeSinceStart : 1858.1044628620148\n","Exploration Critic Loss : 0.43939894437789917\n","Exploitation Critic Q Loss : 0.856950044631958\n","Exploitation Critic V Loss : 0.04740964621305466\n","Exploration Model Loss : 1.0612576261337381e-05\n","Actor Loss : 1.27944016456604\n","Eval_AverageReturn : -116.11111450195312\n","Eval_StdReturn : 25.623098373413086\n","Eval_MaxReturn : -59.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 117.0\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -54.520000\n","best mean reward -54.520000\n","running time 1925.555052\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -54.52000045776367\n","Train_BestReturn : -54.52000045776367\n","TimeSinceStart : 1925.555052280426\n","Exploration Critic Loss : 0.36772334575653076\n","Exploitation Critic Q Loss : 1.3188213109970093\n","Exploitation Critic V Loss : 0.05453008785843849\n","Exploration Model Loss : 2.3732794943498448e-05\n","Actor Loss : 1.2691960334777832\n","Eval_AverageReturn : -103.69999694824219\n","Eval_StdReturn : 31.432626724243164\n","Eval_MaxReturn : -69.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 104.5\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -50.650002\n","best mean reward -50.650002\n","running time 1993.387247\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -50.650001525878906\n","Train_BestReturn : -50.650001525878906\n","TimeSinceStart : 1993.3872468471527\n","Exploration Critic Loss : 0.4911614656448364\n","Exploitation Critic Q Loss : 1.3524738550186157\n","Exploitation Critic V Loss : 0.05217422544956207\n","Exploration Model Loss : 0.0001350182865280658\n","Actor Loss : 1.2880384922027588\n","Eval_AverageReturn : -117.77777862548828\n","Eval_StdReturn : 29.525047302246094\n","Eval_MaxReturn : -67.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 118.55555555555556\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -45.910000\n","best mean reward -45.910000\n","running time 2065.327359\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -45.90999984741211\n","Train_BestReturn : -45.90999984741211\n","TimeSinceStart : 2065.3273587226868\n","Exploration Critic Loss : 0.2316935807466507\n","Exploitation Critic Q Loss : 1.2284153699874878\n","Exploitation Critic V Loss : 0.08431309461593628\n","Exploration Model Loss : 7.646283961548761e-07\n","Actor Loss : 1.2892756462097168\n","Eval_AverageReturn : -82.66666412353516\n","Eval_StdReturn : 23.026554107666016\n","Eval_MaxReturn : -48.0\n","Eval_MinReturn : -118.0\n","Eval_AverageEpLen : 83.66666666666667\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -43.580002\n","best mean reward -43.580002\n","running time 2132.387943\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -43.58000183105469\n","Train_BestReturn : -43.58000183105469\n","TimeSinceStart : 2132.387942790985\n","Exploration Critic Loss : 0.23763920366764069\n","Exploitation Critic Q Loss : 0.7960196733474731\n","Exploitation Critic V Loss : 0.0695900097489357\n","Exploration Model Loss : 2.923416468547657e-05\n","Actor Loss : 1.3476024866104126\n","Eval_AverageReturn : -92.90908813476562\n","Eval_StdReturn : 33.813541412353516\n","Eval_MaxReturn : -41.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 93.72727272727273\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -40.619999\n","best mean reward -40.619999\n","running time 2204.746297\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -40.619998931884766\n","Train_BestReturn : -40.619998931884766\n","TimeSinceStart : 2204.7462968826294\n","Exploration Critic Loss : 0.14351226389408112\n","Exploitation Critic Q Loss : 1.6499263048171997\n","Exploitation Critic V Loss : 0.04220767319202423\n","Exploration Model Loss : 1.3860066246706992e-05\n","Actor Loss : 1.1817395687103271\n","Eval_AverageReturn : -95.7272720336914\n","Eval_StdReturn : 37.045650482177734\n","Eval_MaxReturn : -39.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 96.45454545454545\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -39.169998\n","best mean reward -39.169998\n","running time 2272.369491\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -39.16999816894531\n","Train_BestReturn : -39.16999816894531\n","TimeSinceStart : 2272.36949133873\n","Exploration Critic Loss : 0.22613322734832764\n","Exploitation Critic Q Loss : 1.2945481538772583\n","Exploitation Critic V Loss : 0.059111032634973526\n","Exploration Model Loss : 6.021563967806287e-05\n","Actor Loss : 1.2748006582260132\n","Eval_AverageReturn : -92.7272720336914\n","Eval_StdReturn : 35.65514373779297\n","Eval_MaxReturn : -46.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 93.54545454545455\n","Buffer size : 35001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -37.630001\n","best mean reward -37.630001\n","running time 2342.299798\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -37.630001068115234\n","Train_BestReturn : -37.630001068115234\n","TimeSinceStart : 2342.2997982501984\n","Exploration Critic Loss : 0.030588626861572266\n","Exploitation Critic Q Loss : 0.22458603978157043\n","Exploitation Critic V Loss : 0.07288402318954468\n","Exploration Model Loss : 0.00036694901064038277\n","Actor Loss : 1.2815420627593994\n","Eval_AverageReturn : -85.83333587646484\n","Eval_StdReturn : 34.87796401977539\n","Eval_MaxReturn : -44.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 86.75\n","Buffer size : 36001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -36.720001\n","best mean reward -36.720001\n","running time 2410.396382\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -36.720001220703125\n","Train_BestReturn : -36.720001220703125\n","TimeSinceStart : 2410.396381855011\n","Exploration Critic Loss : 0.10984129458665848\n","Exploitation Critic Q Loss : 0.2882938086986542\n","Exploitation Critic V Loss : 0.08330212533473969\n","Exploration Model Loss : 4.044945853820536e-06\n","Actor Loss : 1.2543323040008545\n","Eval_AverageReturn : -83.0\n","Eval_StdReturn : 27.604347229003906\n","Eval_MaxReturn : -50.0\n","Eval_MinReturn : -138.0\n","Eval_AverageEpLen : 84.0\n","Buffer size : 37001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -35.820000\n","best mean reward -35.820000\n","running time 2479.997697\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -35.81999969482422\n","Train_BestReturn : -35.81999969482422\n","TimeSinceStart : 2479.997697353363\n","Exploration Critic Loss : 0.09826532006263733\n","Exploitation Critic Q Loss : 0.23951661586761475\n","Exploitation Critic V Loss : 0.06216522306203842\n","Exploration Model Loss : 3.6314158933237195e-05\n","Actor Loss : 1.25627601146698\n","Eval_AverageReturn : -86.5\n","Eval_StdReturn : 45.739295959472656\n","Eval_MaxReturn : -35.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 87.33333333333333\n","Buffer size : 38001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -35.669998\n","best mean reward -35.669998\n","running time 2549.198028\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -35.66999816894531\n","Train_BestReturn : -35.66999816894531\n","TimeSinceStart : 2549.1980283260345\n","Exploration Critic Loss : 0.05393324792385101\n","Exploitation Critic Q Loss : 0.3387705981731415\n","Exploitation Critic V Loss : 0.07452332228422165\n","Exploration Model Loss : 9.967189953385969e-07\n","Actor Loss : 1.1964631080627441\n","Eval_AverageReturn : -93.18181610107422\n","Eval_StdReturn : 29.743810653686523\n","Eval_MaxReturn : -44.0\n","Eval_MinReturn : -144.0\n","Eval_AverageEpLen : 94.18181818181819\n","Buffer size : 39001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -34.900002\n","best mean reward -34.900002\n","running time 2618.709021\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -34.900001525878906\n","Train_BestReturn : -34.900001525878906\n","TimeSinceStart : 2618.7090213298798\n","Exploration Critic Loss : 0.08410704135894775\n","Exploitation Critic Q Loss : 0.2432066947221756\n","Exploitation Critic V Loss : 0.05982732027769089\n","Exploration Model Loss : 0.00027199872420169413\n","Actor Loss : 1.2352843284606934\n","Eval_AverageReturn : -68.19999694824219\n","Eval_StdReturn : 23.908716201782227\n","Eval_MaxReturn : -28.0\n","Eval_MinReturn : -126.0\n","Eval_AverageEpLen : 69.2\n","Buffer size : 40001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -36.259998\n","best mean reward -34.900002\n","running time 2688.446674\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -36.2599983215332\n","Train_BestReturn : -34.900001525878906\n","TimeSinceStart : 2688.4466741085052\n","Exploration Critic Loss : 0.04377288371324539\n","Exploitation Critic Q Loss : 0.2670412063598633\n","Exploitation Critic V Loss : 0.05876262113451958\n","Exploration Model Loss : 0.0007184313144534826\n","Actor Loss : 1.2385895252227783\n","Eval_AverageReturn : -76.71428680419922\n","Eval_StdReturn : 33.96726989746094\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 77.64285714285714\n","Buffer size : 41001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -36.939999\n","best mean reward -34.900002\n","running time 2758.433038\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -36.939998626708984\n","Train_BestReturn : -34.900001525878906\n","TimeSinceStart : 2758.4330384731293\n","Exploration Critic Loss : 0.017199261114001274\n","Exploitation Critic Q Loss : 0.34453141689300537\n","Exploitation Critic V Loss : 0.06957638263702393\n","Exploration Model Loss : 1.2111895557609387e-05\n","Actor Loss : 1.2010749578475952\n","Eval_AverageReturn : -67.93333435058594\n","Eval_StdReturn : 30.306140899658203\n","Eval_MaxReturn : -26.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 68.86666666666666\n","Buffer size : 42001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -36.900002\n","best mean reward -34.900002\n","running time 2825.525490\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -36.900001525878906\n","Train_BestReturn : -34.900001525878906\n","TimeSinceStart : 2825.5254900455475\n","Exploration Critic Loss : 0.026120902970433235\n","Exploitation Critic Q Loss : 1.2364692687988281\n","Exploitation Critic V Loss : 0.06432123482227325\n","Exploration Model Loss : 3.1299459806177765e-05\n","Actor Loss : 1.2447764873504639\n","Eval_AverageReturn : -85.25\n","Eval_StdReturn : 34.19825744628906\n","Eval_MaxReturn : -37.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 86.16666666666667\n","Buffer size : 43001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -36.299999\n","best mean reward -34.900002\n","running time 2895.261297\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -36.29999923706055\n","Train_BestReturn : -34.900001525878906\n","TimeSinceStart : 2895.2612965106964\n","Exploration Critic Loss : 0.1132306307554245\n","Exploitation Critic Q Loss : 0.41072794795036316\n","Exploitation Critic V Loss : 0.08633014559745789\n","Exploration Model Loss : 2.2604701825912343e-06\n","Actor Loss : 1.2333983182907104\n","Eval_AverageReturn : -66.86666870117188\n","Eval_StdReturn : 36.76205825805664\n","Eval_MaxReturn : -20.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 67.8\n","Buffer size : 44001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -33.939999\n","best mean reward -33.939999\n","running time 2966.110379\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -33.939998626708984\n","Train_BestReturn : -33.939998626708984\n","TimeSinceStart : 2966.1103794574738\n","Exploration Critic Loss : 0.0747288167476654\n","Exploitation Critic Q Loss : 0.58073490858078\n","Exploitation Critic V Loss : 0.07636652141809464\n","Exploration Model Loss : 0.0002750351559370756\n","Actor Loss : 1.2233833074569702\n","Eval_AverageReturn : -76.46154022216797\n","Eval_StdReturn : 35.979942321777344\n","Eval_MaxReturn : -40.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 77.38461538461539\n","Buffer size : 45001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -35.730000\n","best mean reward -33.939999\n","running time 3036.525015\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -35.72999954223633\n","Train_BestReturn : -33.939998626708984\n","TimeSinceStart : 3036.5250146389008\n","Exploration Critic Loss : 0.08483538031578064\n","Exploitation Critic Q Loss : 1.540951132774353\n","Exploitation Critic V Loss : 0.06674868613481522\n","Exploration Model Loss : 3.30581606249325e-05\n","Actor Loss : 1.1609137058258057\n","Eval_AverageReturn : -58.05882263183594\n","Eval_StdReturn : 17.668418884277344\n","Eval_MaxReturn : -26.0\n","Eval_MinReturn : -92.0\n","Eval_AverageEpLen : 59.05882352941177\n","Buffer size : 46001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -36.169998\n","best mean reward -33.939999\n","running time 3107.547555\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -36.16999816894531\n","Train_BestReturn : -33.939998626708984\n","TimeSinceStart : 3107.547554731369\n","Exploration Critic Loss : 0.04388665780425072\n","Exploitation Critic Q Loss : 0.2783287763595581\n","Exploitation Critic V Loss : 0.06729146838188171\n","Exploration Model Loss : 4.982397149433382e-05\n","Actor Loss : 1.285591959953308\n","Eval_AverageReturn : -65.6875\n","Eval_StdReturn : 18.583322525024414\n","Eval_MaxReturn : -38.0\n","Eval_MinReturn : -110.0\n","Eval_AverageEpLen : 66.6875\n","Buffer size : 47001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -36.160000\n","best mean reward -33.939999\n","running time 3179.214364\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -36.15999984741211\n","Train_BestReturn : -33.939998626708984\n","TimeSinceStart : 3179.2143642902374\n","Exploration Critic Loss : 0.0395054966211319\n","Exploitation Critic Q Loss : 0.33137935400009155\n","Exploitation Critic V Loss : 0.07161040604114532\n","Exploration Model Loss : 1.3818213119520806e-05\n","Actor Loss : 1.2323585748672485\n","Eval_AverageReturn : -102.80000305175781\n","Eval_StdReturn : 35.695377349853516\n","Eval_MaxReturn : -37.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 103.6\n","Buffer size : 48001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -37.860001\n","best mean reward -33.939999\n","running time 3249.274431\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -37.86000061035156\n","Train_BestReturn : -33.939998626708984\n","TimeSinceStart : 3249.2744307518005\n","Exploration Critic Loss : 0.05417736992239952\n","Exploitation Critic Q Loss : 0.36916637420654297\n","Exploitation Critic V Loss : 0.09879098832607269\n","Exploration Model Loss : 0.00012636688188649714\n","Actor Loss : 1.2632369995117188\n","Eval_AverageReturn : -57.72222137451172\n","Eval_StdReturn : 19.63501739501953\n","Eval_MaxReturn : -32.0\n","Eval_MinReturn : -99.0\n","Eval_AverageEpLen : 58.72222222222222\n","Buffer size : 49001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_iql.py --env_name PointmassMedium-v0 \\\n"," --exp_name q5_iql_medium_unsupervised_lam2_tau0.7 --use_rnd \\\n"," --unsupervised_exploration \\\n"," --num_exploration_steps=20000 \\\n"," --awac_lambda=2 \\\n"," --iql_expectile=0.7"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"8yMzohICIH-7","outputId":"b4ed0217-fecb-4570-8ea0-53f8cec43662"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","LOGGING TO:  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_medium_unsupervised_lam2_tau0.99_PointmassMedium-v0_23-11-2022_23-49-13 \n","\n","\n","\n","########################\n","logging outputs to  /content/gdrive/MyDrive/cs285_f2022/homework_fall2022/hw5/cs285/scripts/../../data/hw5_expl_q5_iql_medium_unsupervised_lam2_tau0.99_PointmassMedium-v0_23-11-2022_23-49-13\n","########################\n","GPU not detected. Defaulting to CPU.\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002767\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0027666091918945312\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 6.537652\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 6.537651538848877\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 1001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 13.452398\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 13.45239782333374\n","Eval_AverageReturn : -141.875\n","Eval_StdReturn : 21.496728897094727\n","Eval_MaxReturn : -85.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 142.0\n","Buffer size : 2001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 82.815195\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 82.815194606781\n","Exploration Critic Loss : 0.008061796426773071\n","Exploitation Critic Q Loss : 0.04702512174844742\n","Exploitation Critic V Loss : 1.6633604900562204e-05\n","Exploration Model Loss : 0.003148488001897931\n","Actor Loss : 1.5945600271224976\n","Eval_AverageReturn : -133.625\n","Eval_StdReturn : 24.020498275756836\n","Eval_MaxReturn : -80.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 134.0\n","Buffer size : 3001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 147.419086\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 147.41908597946167\n","Exploration Critic Loss : 0.119669608771801\n","Exploitation Critic Q Loss : 0.17133651673793793\n","Exploitation Critic V Loss : 1.778389014361892e-05\n","Exploration Model Loss : 0.0006710989982821047\n","Actor Loss : 1.5133963823318481\n","Eval_AverageReturn : -141.0\n","Eval_StdReturn : 15.787652969360352\n","Eval_MaxReturn : -109.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 141.25\n","Buffer size : 4001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 214.827581\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 214.8275809288025\n","Exploration Critic Loss : 0.007087394595146179\n","Exploitation Critic Q Loss : 0.0022629895247519016\n","Exploitation Critic V Loss : 3.448488132562488e-05\n","Exploration Model Loss : 0.0006996949086897075\n","Actor Loss : 1.5033226013183594\n","Eval_AverageReturn : -143.7142791748047\n","Eval_StdReturn : 15.396791458129883\n","Eval_MaxReturn : -106.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 143.85714285714286\n","Buffer size : 5001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 279.967623\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 279.96762323379517\n","Exploration Critic Loss : 0.18937112390995026\n","Exploitation Critic Q Loss : 0.5540990233421326\n","Exploitation Critic V Loss : 0.0004080040380358696\n","Exploration Model Loss : 0.0005577077972702682\n","Actor Loss : 1.4929876327514648\n","Eval_AverageReturn : -134.25\n","Eval_StdReturn : 21.223512649536133\n","Eval_MaxReturn : -94.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 134.625\n","Buffer size : 6001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 347.189151\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 347.1891508102417\n","Exploration Critic Loss : 0.04313363879919052\n","Exploitation Critic Q Loss : 0.7202271223068237\n","Exploitation Critic V Loss : 4.814852945855819e-05\n","Exploration Model Loss : 0.00038869178388267756\n","Actor Loss : 1.4756054878234863\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 7001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 413.029822\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 413.0298216342926\n","Exploration Critic Loss : 0.027908314019441605\n","Exploitation Critic Q Loss : 0.01532647293061018\n","Exploitation Critic V Loss : 0.00018560036551207304\n","Exploration Model Loss : 0.0003438945859670639\n","Actor Loss : 1.4628982543945312\n","Eval_AverageReturn : -141.0\n","Eval_StdReturn : 23.8117618560791\n","Eval_MaxReturn : -78.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 141.125\n","Buffer size : 8001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 476.549144\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 476.54914355278015\n","Exploration Critic Loss : 0.06869222223758698\n","Exploitation Critic Q Loss : 0.873153030872345\n","Exploitation Critic V Loss : 0.0005006660358048975\n","Exploration Model Loss : 0.000349294365150854\n","Actor Loss : 1.3729296922683716\n","Eval_AverageReturn : -146.57142639160156\n","Eval_StdReturn : 6.586938381195068\n","Eval_MaxReturn : -131.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 146.85714285714286\n","Buffer size : 9001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -150.000000\n","best mean reward -inf\n","running time 544.551332\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -150.0\n","TimeSinceStart : 544.5513324737549\n","Exploration Critic Loss : 0.07314170897006989\n","Exploitation Critic Q Loss : 1.0507358312606812\n","Exploitation Critic V Loss : 0.00044975834316574037\n","Exploration Model Loss : 0.00022324806195683777\n","Actor Loss : 1.4301005601882935\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 10001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -149.479446\n","best mean reward -inf\n","running time 608.171215\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -149.4794464111328\n","TimeSinceStart : 608.171215057373\n","Exploration Critic Loss : 0.09929152578115463\n","Exploitation Critic Q Loss : 0.5058687329292297\n","Exploitation Critic V Loss : 0.0002433557529002428\n","Exploration Model Loss : 0.00011825214460259303\n","Actor Loss : 1.3360259532928467\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 11001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -148.524994\n","best mean reward -inf\n","running time 677.479641\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -148.52499389648438\n","TimeSinceStart : 677.4796414375305\n","Exploration Critic Loss : 0.12184851616621017\n","Exploitation Critic Q Loss : 0.5541608929634094\n","Exploitation Critic V Loss : 0.00041189268813468516\n","Exploration Model Loss : 0.0001448963157599792\n","Actor Loss : 1.374288558959961\n","Eval_AverageReturn : -148.7142791748047\n","Eval_StdReturn : 3.149343729019165\n","Eval_MaxReturn : -141.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 148.85714285714286\n","Buffer size : 12001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -148.206894\n","best mean reward -inf\n","running time 741.193806\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -148.20689392089844\n","TimeSinceStart : 741.1938064098358\n","Exploration Critic Loss : 0.1827079951763153\n","Exploitation Critic Q Loss : 1.806174874305725\n","Exploitation Critic V Loss : 0.0004439449985511601\n","Exploration Model Loss : 8.15341918496415e-05\n","Actor Loss : 1.3422317504882812\n","Eval_AverageReturn : -139.875\n","Eval_StdReturn : 20.065128326416016\n","Eval_MaxReturn : -90.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 140.125\n","Buffer size : 13001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -148.138290\n","best mean reward -inf\n","running time 807.652592\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -148.13829040527344\n","TimeSinceStart : 807.6525917053223\n","Exploration Critic Loss : 0.08689846843481064\n","Exploitation Critic Q Loss : 0.033340636640787125\n","Exploitation Critic V Loss : 0.0007161112152971327\n","Exploration Model Loss : 4.977155913366005e-05\n","Actor Loss : 1.3688758611679077\n","Eval_AverageReturn : -150.0\n","Eval_StdReturn : 0.0\n","Eval_MaxReturn : -150.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 150.0\n","Buffer size : 14001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -147.750000\n","best mean reward -147.750000\n","running time 871.980195\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -147.75\n","Train_BestReturn : -147.75\n","TimeSinceStart : 871.9801950454712\n","Exploration Critic Loss : 0.1406993716955185\n","Exploitation Critic Q Loss : 0.6117752194404602\n","Exploitation Critic V Loss : 0.00023855340259615332\n","Exploration Model Loss : 3.715336788445711e-05\n","Actor Loss : 1.3287206888198853\n","Eval_AverageReturn : -147.14285278320312\n","Eval_StdReturn : 6.998542785644531\n","Eval_MaxReturn : -130.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 147.28571428571428\n","Buffer size : 15001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -144.470001\n","best mean reward -144.470001\n","running time 938.706574\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -144.47000122070312\n","Train_BestReturn : -144.47000122070312\n","TimeSinceStart : 938.7065744400024\n","Exploration Critic Loss : 0.12967923283576965\n","Exploitation Critic Q Loss : 1.6171033382415771\n","Exploitation Critic V Loss : 0.00026436251937411726\n","Exploration Model Loss : 2.8223641493241303e-05\n","Actor Loss : 1.3396965265274048\n","Eval_AverageReturn : -144.2857208251953\n","Eval_StdReturn : 13.99708366394043\n","Eval_MaxReturn : -110.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 144.42857142857142\n","Buffer size : 16001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -140.779999\n","best mean reward -140.779999\n","running time 1004.522420\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -140.77999877929688\n","Train_BestReturn : -140.77999877929688\n","TimeSinceStart : 1004.522420167923\n","Exploration Critic Loss : 0.10931579023599625\n","Exploitation Critic Q Loss : 1.6024184226989746\n","Exploitation Critic V Loss : 0.0003733455669134855\n","Exploration Model Loss : 8.911764780350495e-06\n","Actor Loss : 1.3136857748031616\n","Eval_AverageReturn : -139.125\n","Eval_StdReturn : 20.496570587158203\n","Eval_MaxReturn : -87.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 139.5\n","Buffer size : 17001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -135.610001\n","best mean reward -135.610001\n","running time 1068.103034\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -135.61000061035156\n","Train_BestReturn : -135.61000061035156\n","TimeSinceStart : 1068.1030344963074\n","Exploration Critic Loss : 0.3819902837276459\n","Exploitation Critic Q Loss : 1.3173354864120483\n","Exploitation Critic V Loss : 0.00028535546152852476\n","Exploration Model Loss : 1.3541106454795226e-05\n","Actor Loss : 1.2844913005828857\n","Eval_AverageReturn : -142.125\n","Eval_StdReturn : 20.835290908813477\n","Eval_MaxReturn : -87.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 142.25\n","Buffer size : 18001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -126.309998\n","best mean reward -126.309998\n","running time 1138.570555\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -126.30999755859375\n","Train_BestReturn : -126.30999755859375\n","TimeSinceStart : 1138.5705552101135\n","Exploration Critic Loss : 0.4274183213710785\n","Exploitation Critic Q Loss : 1.0588549375534058\n","Exploitation Critic V Loss : 0.0009860338177531958\n","Exploration Model Loss : 0.00012008166231680661\n","Actor Loss : 1.318055272102356\n","Eval_AverageReturn : -139.25\n","Eval_StdReturn : 28.44182586669922\n","Eval_MaxReturn : -64.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 139.375\n","Buffer size : 19001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -117.809998\n","best mean reward -117.809998\n","running time 1205.055779\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -117.80999755859375\n","Train_BestReturn : -117.80999755859375\n","TimeSinceStart : 1205.055778503418\n","Exploration Critic Loss : 0.27610084414482117\n","Exploitation Critic Q Loss : 0.34110206365585327\n","Exploitation Critic V Loss : 0.000848504074383527\n","Exploration Model Loss : 9.012453665491194e-05\n","Actor Loss : 1.3266551494598389\n","Eval_AverageReturn : -130.875\n","Eval_StdReturn : 34.78303146362305\n","Eval_MaxReturn : -43.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 131.25\n","Buffer size : 20001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -104.839996\n","best mean reward -104.839996\n","running time 1274.994451\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -104.83999633789062\n","Train_BestReturn : -104.83999633789062\n","TimeSinceStart : 1274.9944512844086\n","Exploration Critic Loss : 0.5222543478012085\n","Exploitation Critic Q Loss : 1.2782338857650757\n","Exploitation Critic V Loss : 0.0014486152213066816\n","Exploration Model Loss : 6.396272510755807e-05\n","Actor Loss : 1.3532952070236206\n","Eval_AverageReturn : -124.75\n","Eval_StdReturn : 24.29377555847168\n","Eval_MaxReturn : -89.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 125.375\n","Buffer size : 21001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -97.019997\n","best mean reward -97.019997\n","running time 1339.745620\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -97.0199966430664\n","Train_BestReturn : -97.0199966430664\n","TimeSinceStart : 1339.7456195354462\n","Exploration Critic Loss : 0.4560662508010864\n","Exploitation Critic Q Loss : 0.8584185838699341\n","Exploitation Critic V Loss : 0.002169384155422449\n","Exploration Model Loss : 0.00018819680553860962\n","Actor Loss : 1.4065111875534058\n","Eval_AverageReturn : -134.25\n","Eval_StdReturn : 16.88009262084961\n","Eval_MaxReturn : -107.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 134.875\n","Buffer size : 22001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -91.000000\n","best mean reward -91.000000\n","running time 1407.882376\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -91.0\n","Train_BestReturn : -91.0\n","TimeSinceStart : 1407.8823761940002\n","Exploration Critic Loss : 0.46255651116371155\n","Exploitation Critic Q Loss : 0.9570553302764893\n","Exploitation Critic V Loss : 0.002459682757034898\n","Exploration Model Loss : 0.00028302049031481147\n","Actor Loss : 1.2933087348937988\n","Eval_AverageReturn : -126.5\n","Eval_StdReturn : 31.256999969482422\n","Eval_MaxReturn : -73.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 126.875\n","Buffer size : 23001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -82.089996\n","best mean reward -82.089996\n","running time 1474.125572\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -82.08999633789062\n","Train_BestReturn : -82.08999633789062\n","TimeSinceStart : 1474.1255719661713\n","Exploration Critic Loss : 0.39423874020576477\n","Exploitation Critic Q Loss : 0.2570796608924866\n","Exploitation Critic V Loss : 0.004393741954118013\n","Exploration Model Loss : 4.9090398533735424e-05\n","Actor Loss : 1.3556352853775024\n","Eval_AverageReturn : -114.66666412353516\n","Eval_StdReturn : 30.38274574279785\n","Eval_MaxReturn : -76.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 115.33333333333333\n","Buffer size : 24001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -74.709999\n","best mean reward -74.709999\n","running time 1542.314146\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -74.70999908447266\n","Train_BestReturn : -74.70999908447266\n","TimeSinceStart : 1542.3141462802887\n","Exploration Critic Loss : 0.33180609345436096\n","Exploitation Critic Q Loss : 0.2855696380138397\n","Exploitation Critic V Loss : 0.0044262465089559555\n","Exploration Model Loss : 2.051195770036429e-05\n","Actor Loss : 1.2067676782608032\n","Eval_AverageReturn : -127.75\n","Eval_StdReturn : 28.19906997680664\n","Eval_MaxReturn : -66.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 128.25\n","Buffer size : 25001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -64.019997\n","best mean reward -64.019997\n","running time 1609.313419\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -64.0199966430664\n","Train_BestReturn : -64.0199966430664\n","TimeSinceStart : 1609.3134188652039\n","Exploration Critic Loss : 0.24900926649570465\n","Exploitation Critic Q Loss : 1.5874282121658325\n","Exploitation Critic V Loss : 0.004078785888850689\n","Exploration Model Loss : 0.00015109464584384114\n","Actor Loss : 1.2256897687911987\n","Eval_AverageReturn : -135.125\n","Eval_StdReturn : 19.808063507080078\n","Eval_MaxReturn : -94.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 135.625\n","Buffer size : 26001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -58.169998\n","best mean reward -58.169998\n","running time 1674.878944\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -58.16999816894531\n","Train_BestReturn : -58.16999816894531\n","TimeSinceStart : 1674.8789443969727\n","Exploration Critic Loss : 0.25435590744018555\n","Exploitation Critic Q Loss : 0.41979748010635376\n","Exploitation Critic V Loss : 0.004662557505071163\n","Exploration Model Loss : 0.0006316996878013015\n","Actor Loss : 1.2506831884384155\n","Eval_AverageReturn : -99.19999694824219\n","Eval_StdReturn : 28.49842071533203\n","Eval_MaxReturn : -58.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 100.1\n","Buffer size : 27001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -49.240002\n","best mean reward -49.240002\n","running time 1746.657658\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -49.2400016784668\n","Train_BestReturn : -49.2400016784668\n","TimeSinceStart : 1746.657657623291\n","Exploration Critic Loss : 0.13671933114528656\n","Exploitation Critic Q Loss : 0.2791925370693207\n","Exploitation Critic V Loss : 0.00554313650354743\n","Exploration Model Loss : 4.534760591923259e-05\n","Actor Loss : 1.2686189413070679\n","Eval_AverageReturn : -118.22222137451172\n","Eval_StdReturn : 25.951995849609375\n","Eval_MaxReturn : -83.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 118.88888888888889\n","Buffer size : 28001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -42.950001\n","best mean reward -42.950001\n","running time 1814.065993\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -42.95000076293945\n","Train_BestReturn : -42.95000076293945\n","TimeSinceStart : 1814.065993309021\n","Exploration Critic Loss : 0.2609601616859436\n","Exploitation Critic Q Loss : 1.466210961341858\n","Exploitation Critic V Loss : 0.004750364925712347\n","Exploration Model Loss : 8.015021739993244e-05\n","Actor Loss : 1.275490164756775\n","Eval_AverageReturn : -92.09091186523438\n","Eval_StdReturn : 24.68512535095215\n","Eval_MaxReturn : -60.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 93.0\n","Buffer size : 29001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -40.950001\n","best mean reward -40.950001\n","running time 1886.447774\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -40.95000076293945\n","Train_BestReturn : -40.95000076293945\n","TimeSinceStart : 1886.4477744102478\n","Exploration Critic Loss : 0.09192954748868942\n","Exploitation Critic Q Loss : 0.20398201048374176\n","Exploitation Critic V Loss : 0.0038146364968270063\n","Exploration Model Loss : 1.2368152965791523e-05\n","Actor Loss : 1.3081644773483276\n","Eval_AverageReturn : -82.15384674072266\n","Eval_StdReturn : 36.24668884277344\n","Eval_MaxReturn : -37.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 83.07692307692308\n","Buffer size : 30001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -41.180000\n","best mean reward -40.950001\n","running time 1953.959327\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -41.18000030517578\n","Train_BestReturn : -40.95000076293945\n","TimeSinceStart : 1953.9593269824982\n","Exploration Critic Loss : 0.11402757465839386\n","Exploitation Critic Q Loss : 0.17604538798332214\n","Exploitation Critic V Loss : 0.003074894892051816\n","Exploration Model Loss : 0.00025027853553183377\n","Actor Loss : 1.2355732917785645\n","Eval_AverageReturn : -90.0\n","Eval_StdReturn : 23.292606353759766\n","Eval_MaxReturn : -47.0\n","Eval_MinReturn : -114.0\n","Eval_AverageEpLen : 91.0\n","Buffer size : 31001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -38.020000\n","best mean reward -38.020000\n","running time 2024.613088\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -38.02000045776367\n","Train_BestReturn : -38.02000045776367\n","TimeSinceStart : 2024.613088130951\n","Exploration Critic Loss : 0.15596085786819458\n","Exploitation Critic Q Loss : 0.19260220229625702\n","Exploitation Critic V Loss : 0.0025614374317228794\n","Exploration Model Loss : 8.41302826302126e-05\n","Actor Loss : 1.2151402235031128\n","Eval_AverageReturn : -95.36363983154297\n","Eval_StdReturn : 27.180448532104492\n","Eval_MaxReturn : -58.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 96.27272727272727\n","Buffer size : 32001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -36.779999\n","best mean reward -36.779999\n","running time 2096.448330\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -36.779998779296875\n","Train_BestReturn : -36.779998779296875\n","TimeSinceStart : 2096.4483304023743\n","Exploration Critic Loss : 0.08793617784976959\n","Exploitation Critic Q Loss : 0.39298900961875916\n","Exploitation Critic V Loss : 0.0026225284673273563\n","Exploration Model Loss : 4.7870333219179884e-05\n","Actor Loss : 1.2075978517532349\n","Eval_AverageReturn : -91.81818389892578\n","Eval_StdReturn : 16.606779098510742\n","Eval_MaxReturn : -64.0\n","Eval_MinReturn : -121.0\n","Eval_AverageEpLen : 92.81818181818181\n","Buffer size : 33001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -34.320000\n","best mean reward -34.320000\n","running time 2171.969513\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -34.31999969482422\n","Train_BestReturn : -34.31999969482422\n","TimeSinceStart : 2171.969512939453\n","Exploration Critic Loss : 0.13326622545719147\n","Exploitation Critic Q Loss : 1.4420543909072876\n","Exploitation Critic V Loss : 0.001941445516422391\n","Exploration Model Loss : 2.017255695818676e-07\n","Actor Loss : 1.196664571762085\n","Eval_AverageReturn : -67.80000305175781\n","Eval_StdReturn : 22.32845687866211\n","Eval_MaxReturn : -37.0\n","Eval_MinReturn : -113.0\n","Eval_AverageEpLen : 68.8\n","Buffer size : 34001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -32.950001\n","best mean reward -32.950001\n","running time 2247.303644\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -32.95000076293945\n","Train_BestReturn : -32.95000076293945\n","TimeSinceStart : 2247.303644180298\n","Exploration Critic Loss : 0.02255498431622982\n","Exploitation Critic Q Loss : 0.1246618703007698\n","Exploitation Critic V Loss : 0.0019544006790965796\n","Exploration Model Loss : 6.655463380411675e-08\n","Actor Loss : 1.2172143459320068\n","Eval_AverageReturn : -79.0\n","Eval_StdReturn : 26.715431213378906\n","Eval_MaxReturn : -33.0\n","Eval_MinReturn : -134.0\n","Eval_AverageEpLen : 80.0\n","Buffer size : 35001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -32.240002\n","best mean reward -32.240002\n","running time 2325.870363\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -32.2400016784668\n","Train_BestReturn : -32.2400016784668\n","TimeSinceStart : 2325.870363473892\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 0.8044898509979248\n","Exploitation Critic V Loss : 0.0018920355942100286\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.2722070217132568\n","Eval_AverageReturn : -72.06666564941406\n","Eval_StdReturn : 28.64371109008789\n","Eval_MaxReturn : -32.0\n","Eval_MinReturn : -127.0\n","Eval_AverageEpLen : 73.06666666666666\n","Buffer size : 36001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -32.240002\n","best mean reward -32.240002\n","running time 2399.775987\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -32.2400016784668\n","Train_BestReturn : -32.2400016784668\n","TimeSinceStart : 2399.7759866714478\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 1.074549674987793\n","Exploitation Critic V Loss : 0.0016222273698076606\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.2013423442840576\n","Eval_AverageReturn : -80.38461303710938\n","Eval_StdReturn : 29.203630447387695\n","Eval_MaxReturn : -41.0\n","Eval_MinReturn : -130.0\n","Eval_AverageEpLen : 81.38461538461539\n","Buffer size : 37001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -33.509998\n","best mean reward -32.240002\n","running time 2480.054421\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -33.5099983215332\n","Train_BestReturn : -32.2400016784668\n","TimeSinceStart : 2480.0544214248657\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 0.8364858627319336\n","Exploitation Critic V Loss : 0.0026662652380764484\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.2632341384887695\n","Eval_AverageReturn : -79.92308044433594\n","Eval_StdReturn : 35.62725830078125\n","Eval_MaxReturn : -45.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 80.76923076923077\n","Buffer size : 38001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -33.320000\n","best mean reward -32.240002\n","running time 2557.542590\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -33.31999969482422\n","Train_BestReturn : -32.2400016784668\n","TimeSinceStart : 2557.5425896644592\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 0.5455543994903564\n","Exploitation Critic V Loss : 0.0023882295936346054\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.2409675121307373\n","Eval_AverageReturn : -86.41666412353516\n","Eval_StdReturn : 29.587324142456055\n","Eval_MaxReturn : -40.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 87.33333333333333\n","Buffer size : 39001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -33.160000\n","best mean reward -32.240002\n","running time 2636.721819\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -33.15999984741211\n","Train_BestReturn : -32.2400016784668\n","TimeSinceStart : 2636.721819162369\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 0.7225198745727539\n","Exploitation Critic V Loss : 0.0022205563727766275\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.2811864614486694\n","Eval_AverageReturn : -63.0625\n","Eval_StdReturn : 21.46354103088379\n","Eval_MaxReturn : -37.0\n","Eval_MinReturn : -115.0\n","Eval_AverageEpLen : 64.0625\n","Buffer size : 40001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -31.830000\n","best mean reward -31.830000\n","running time 2714.713821\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -31.829999923706055\n","Train_BestReturn : -31.829999923706055\n","TimeSinceStart : 2714.713821411133\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 0.18704330921173096\n","Exploitation Critic V Loss : 0.002784136449918151\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.2326265573501587\n","Eval_AverageReturn : -61.764705657958984\n","Eval_StdReturn : 23.21391487121582\n","Eval_MaxReturn : -27.0\n","Eval_MinReturn : -118.0\n","Eval_AverageEpLen : 62.76470588235294\n","Buffer size : 41001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -31.360001\n","best mean reward -31.360001\n","running time 2794.271829\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -31.360000610351562\n","Train_BestReturn : -31.360000610351562\n","TimeSinceStart : 2794.271828889847\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 0.3714243173599243\n","Exploitation Critic V Loss : 0.0020694907288998365\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.2609893083572388\n","Eval_AverageReturn : -67.93333435058594\n","Eval_StdReturn : 31.88409423828125\n","Eval_MaxReturn : -24.0\n","Eval_MinReturn : -114.0\n","Eval_AverageEpLen : 68.93333333333334\n","Buffer size : 42001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -29.389999\n","best mean reward -29.389999\n","running time 2870.141502\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -29.389999389648438\n","Train_BestReturn : -29.389999389648438\n","TimeSinceStart : 2870.1415021419525\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 0.6371628046035767\n","Exploitation Critic V Loss : 0.002394831506535411\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.252324104309082\n","Eval_AverageReturn : -68.19999694824219\n","Eval_StdReturn : 22.18467903137207\n","Eval_MaxReturn : -44.0\n","Eval_MinReturn : -135.0\n","Eval_AverageEpLen : 69.2\n","Buffer size : 43001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -28.639999\n","best mean reward -28.639999\n","running time 2949.844845\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -28.639999389648438\n","Train_BestReturn : -28.639999389648438\n","TimeSinceStart : 2949.844845056534\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 0.7408257126808167\n","Exploitation Critic V Loss : 0.0019807135686278343\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.1977890729904175\n","Eval_AverageReturn : -70.19999694824219\n","Eval_StdReturn : 17.47455406188965\n","Eval_MaxReturn : -43.0\n","Eval_MinReturn : -103.0\n","Eval_AverageEpLen : 71.2\n","Buffer size : 44001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -28.379999\n","best mean reward -28.379999\n","running time 3026.476344\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -28.3799991607666\n","Train_BestReturn : -28.3799991607666\n","TimeSinceStart : 3026.476343870163\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 0.40298834443092346\n","Exploitation Critic V Loss : 0.0023058876395225525\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.1407767534255981\n","Eval_AverageReturn : -72.14286041259766\n","Eval_StdReturn : 21.85947608947754\n","Eval_MaxReturn : -41.0\n","Eval_MinReturn : -114.0\n","Eval_AverageEpLen : 73.14285714285714\n","Buffer size : 45001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -28.879999\n","best mean reward -28.379999\n","running time 3107.503028\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -28.8799991607666\n","Train_BestReturn : -28.3799991607666\n","TimeSinceStart : 3107.5030283927917\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 0.7314457893371582\n","Exploitation Critic V Loss : 0.003284401958808303\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.2341476678848267\n","Eval_AverageReturn : -58.55555725097656\n","Eval_StdReturn : 21.992984771728516\n","Eval_MaxReturn : -20.0\n","Eval_MinReturn : -104.0\n","Eval_AverageEpLen : 59.55555555555556\n","Buffer size : 46001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -29.059999\n","best mean reward -28.379999\n","running time 3186.131454\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -29.059999465942383\n","Train_BestReturn : -28.3799991607666\n","TimeSinceStart : 3186.1314544677734\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 0.3881935477256775\n","Exploitation Critic V Loss : 0.002129314001649618\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.1670925617218018\n","Eval_AverageReturn : -62.47058868408203\n","Eval_StdReturn : 27.06662368774414\n","Eval_MaxReturn : -32.0\n","Eval_MinReturn : -150.0\n","Eval_AverageEpLen : 63.411764705882355\n","Buffer size : 47001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -28.330000\n","best mean reward -28.330000\n","running time 3264.488780\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -28.329999923706055\n","Train_BestReturn : -28.329999923706055\n","TimeSinceStart : 3264.4887795448303\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 0.865087628364563\n","Exploitation Critic V Loss : 0.002669562818482518\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.1471134424209595\n","Eval_AverageReturn : -51.45000076293945\n","Eval_StdReturn : 19.187170028686523\n","Eval_MaxReturn : -18.0\n","Eval_MinReturn : -94.0\n","Eval_AverageEpLen : 52.45\n","Buffer size : 48001\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -27.620001\n","best mean reward -27.620001\n","running time 3342.514873\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -27.6200008392334\n","Train_BestReturn : -27.6200008392334\n","TimeSinceStart : 3342.51487326622\n","Exploration Critic Loss : nan\n","NaN or Inf found in input tensor.\n","Exploitation Critic Q Loss : 0.25584256649017334\n","Exploitation Critic V Loss : 0.0026888789143413305\n","Exploration Model Loss : nan\n","NaN or Inf found in input tensor.\n","Actor Loss : 1.1733325719833374\n","Eval_AverageReturn : -57.33333206176758\n","Eval_StdReturn : 14.28674602508545\n","Eval_MaxReturn : -31.0\n","Eval_MinReturn : -82.0\n","Eval_AverageEpLen : 58.333333333333336\n","Buffer size : 49001\n","Done logging...\n","\n","\n"]}],"source":["!python cs285/scripts/run_hw5_iql.py --env_name PointmassMedium-v0 \\\n"," --exp_name q5_iql_medium_unsupervised_lam2_tau0.99 --use_rnd \\\n"," --unsupervised_exploration \\\n"," --num_exploration_steps=20000 \\\n"," --awac_lambda=2 \\\n"," --iql_expectile=0.99"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-_WyMbz4IK0J"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["xV8QEMwcOUbq","0G8Fc7EhOXS1","PhI0pXVCCl6J","g8uWfqd7Jdeb","YvnQflic32Xc","Vak_qLvvwW1e"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.5.2"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}